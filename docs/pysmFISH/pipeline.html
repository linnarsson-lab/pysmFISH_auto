<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysmFISH.pipeline API documentation</title>
<meta name="description" content="Module containing the classes used to create pipelines â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysmFISH.pipeline</code></h1>
</header>
<section id="section-intro">
<p>Module containing the classes used to create pipelines.</p>
<p>The master class Pipeline contains all the option to run
standard eel and smfish experiments. Use the methods as step for
building the pipeline. The methods do not require input<br>
however the underline functions do and must be included as
attributes. This may not be the best solution but allows
flexibility and easy to use. Make sure to look at the docstrings
when chaining different modules.</p>
<p>Assert will be used to make sure that the attributes created by
a step and required from another are present</p>
<p>If additional functionality are
required subclass Pipeline and add/replace the different functionality</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Module containing the classes used to create pipelines.

The master class Pipeline contains all the option to run
standard eel and smfish experiments. Use the methods as step for
building the pipeline. The methods do not require input  
however the underline functions do and must be included as 
attributes. This may not be the best solution but allows
flexibility and easy to use. Make sure to look at the docstrings
when chaining different modules.

Assert will be used to make sure that the attributes created by
a step and required from another are present

If additional functionality are
required subclass Pipeline and add/replace the different functionality
&#34;&#34;&#34;
from typing import *

import os
import dask
import sys
import gc
import yaml
import pandas as pd
import numpy as np

from pathlib import Path
from datetime import datetime

from dask.distributed import Client
from dask.base import tokenize
from dask import dataframe as dd
from dask import delayed

# Import from pysmFISH package
import pysmFISH

from pysmFISH import logger_utils
from pysmFISH import utils
from pysmFISH import configuration_files
from pysmFISH import io
from pysmFISH import data_organization
from pysmFISH import microscopy_file_parsers
from pysmFISH import processing_cluster_setup
from pysmFISH import data_models
from pysmFISH import preprocessing
from pysmFISH import fovs_registration
from pysmFISH import barcodes_analysis
from pysmFISH import fov_processing
from pysmFISH import stitching
from pysmFISH import qc_utils
from pysmFISH import data_organization
from pysmFISH import processing_cluster_setup



# utils.nice_deltastring for calculate the time
class Pipeline():
    &#34;&#34;&#34;
        General pipeline class used for running barcoded eel or serial smFISH experiments.
        The modules are used as steps to build a pipeline

        The exposed attributes allow to modify the properties of the runnning pipline
        on the fly


        Args:
            pipeline_run_name (str): Name of the running pipeline made of current datetime and
                    experiment name (ex. &#39;210507_13_25_21_LBEXP20210226_EEL_HE_2100um&#39;)
            experiment_fpath (str): Path to the experiment folder
            run_type (str): Define if it is a &#39;new&#39; or &#39;re-run&#39; (default: new)
            parsing_type (str): Define the parsing type. Can be: 
                            original/no_parsing/reparsing_from_processing_folder/reparsing_from_storage
                            (default: original)

        __Optional KWargs__:  
            raw_data_folder_storage_path (str): Path to the cold storage hard drive (default: /fish/rawdata)  
            parsed_image_tag (str): Tag to identify the zarr file with parsed images (default: img_data)  
            preprocessed_image_tag (str): Tag to identify the zarr file with preprocessed images 
                                        (default: preprocessed_img_data)  
            dataset_folder_storage_path (str): Path to the location where the dataset are stored (default: /fish/fish_datasets)  
            results_folder_storage_path (str): Path to the location where the dataset are stored (default: /fish/fish_results)
            save_intermediate_steps (bool): Determine if the processed images will be saved (default: True)
            dataset_path (str): Path to an existing dataset that will be used in the processing
            chunk_size (int): Number of FOV to process in parallel
            same_dot_radius_duplicate_dots (float): Searching distance that define two dots as identical
                                    (default: 5)
            stitching_selected (str): Define the stitched counts on which the overlapping dotes will be removed 
                                    (default: microscope_stitched) 
            hamming_distance (int): Value to select the barcodes that are passing the 
                                    screening (&lt; hamming_distance). (default: 3)


            processing_engine (str): Define the name of the system that will run the processing. Can be local/htcondor
                                    (default htcondor). If engine == local the parameters that define the cluster
                                    will be ignored
            cores (int): Number of cores/job to use in htcondor or in the local processing (default 20). In the
                            the unmanaged cluster correspond to the nummber of core for each process (nprocs)
            memory (str): Total memory for all the cores in condor (default 200GB) or per core in local setup
                        or per process (nprocs) in the unmanaged cluster
            disk (str): Size of the spillover disk for dask (default 0.1GB)
            local_directory (str): Directory where to spill over on the node (default /tmp)
            logs_directory: (str): Directory where to store dask and htcondor logs
            adaptive: (bool): Decide if the cluster can increase/decrease the number of worker accroding to
                                the processig required. (default True)
            maximum_jobs (int): Max number of jobs to run in htcondor
            scheduler_port (int): define the dask scheduler port. Used for the unmanaged cluster (default 23875) 
            dashboard_port (int): define the dask dashboard port: Used for the unmanaged cluser (default 8787)
            scheduler_address (str): Address of the dask scheduler. Used for the unmanaged cluser. 
                                &#39;localhost&#39; if running of the main node (default &#39;localhost)
            workers_addresses_list (list[str]): Addresses of the workers (default [monod10,monod11,monod12,monod33])
            nprocs (int): number of processes for each workers (unmanaged cluster) (default 40 for single node monod)
            nthreads (int): number threads/process (default 1)
            save_bits_int: (bool): Save the intensity of the bits and the flipping direction
            start_from_preprocessed_imgs (bool): Run the processing starting from the counting
                using preprocessed images. default: False 
            resume: (bool): Restart the processsing. Determine automatically which files are already processed by checking
                            the *_*decoded_* files in the results folder
            reuse_cluster (str): Connect the pipeline to a previously created cluster (default False). Can be: &#39;connect_to_client&#39; ,&#39;connect_to_scheduler&#39;
            active_cluster (dask_cluster): Already active cluster to reconnect to when you want to reuse a cluster
                                            (default None)
            active_client (dask_client): Already active client to reconnect to when you want to reuse a cluster
                                            (default None)
            active_scheduler_address (str): Running cluster to connect when you want reuse a cluster

        Attributes:
            storage_experiment_fpath: Path to folder in the storage HD where to store (or are stored) the raw data for
                                        the current experiment
            parsed_raw_data_fpath: Path to the zarr file containing the raw data
            analysis_parameters: Parameters used for running the analysis
            metadata: Dictionary with the parameters that characterize the current experiment
            preprocessed_zarr_fpath: Path to the zarr file containing the preprocessed images
            cluster: dask cluster running locally or on htcondor
            client: dask client used to manage the cluster
            data: Dataset that define the experiment
            grpd_fovs: Dataset grouped-by field of view number
            reference_round: Round used as starting point for the decoding of the codebooks
            tiles_org: Describe the organization of the tiles in the dataset
            tile_corners_coords_pxl: Coords of the tiles acquired by the microscope in the reference round
            running_functions: Dictionary of functions used to define the type of filtering used for the 
                                fish and reference channels

    &#34;&#34;&#34;

    def __init__(self, pipeline_run_name:str, experiment_fpath:str,
                run_type:str = &#39;new&#39;, parsing_type:str = &#39;original&#39;, **kwarg):

        self.pipeline_run_name = pipeline_run_name
        self.experiment_fpath = Path(experiment_fpath)
        self.run_type = run_type
        self.parsing_type = parsing_type

        # Collect some of the parameters. If missing a predefined value is assigned
        self.raw_data_folder_storage_path = kwarg.pop(&#39;raw_data_folder_storage_path&#39;, &#39;/fish/rawdata&#39;)
        self.parsed_image_tag = kwarg.pop(&#39;parsed_image_tag&#39;,&#39;img_data&#39;)
        self.preprocessed_image_tag = kwarg.pop(&#39;filtered_image_tag&#39;,&#39;preprocessed_img_data&#39;)
        self.dataset_folder_storage_path = kwarg.pop(&#39;dataset_folder_storage_path&#39;,&#39;/fish/fish_datasets&#39;)
        self.results_folder_storage_path = kwarg.pop(&#39;results_folder_storage_path&#39;,&#39;/fish/fish_results&#39;)
        self.save_intermediate_steps = kwarg.pop(&#39;save_intermediate_steps&#39;,True)
        self.dataset_path = kwarg.pop(&#39;dataset_path&#39;,&#39;&#39;)
        self.store_dataset = kwarg.pop(&#39;store_dataset&#39;,True)
        self.chunk_size = kwarg.pop(&#39;chunk_size&#39;,20)
        self.same_dot_radius_duplicate_dots = kwarg.pop(&#39;same_dot_radius_duplicate_dots&#39;,5)
        self.stitching_selected = kwarg.pop(&#39;stitching_selected&#39;,&#39;microscope_stitched&#39;)
        self.hamming_distance = kwarg.pop(&#39;hamming_distance&#39;,3)
        self.save_bits_int = kwarg.pop(&#39;save_bits_int&#39;,True)
        self.adaptive = kwarg.pop(&#39;adaptive&#39;,True)
        self.maximum_jobs = kwarg.pop(&#39;maximum_jobs&#39;,15)
        self.scheduler_port = kwarg.pop(&#39;scheduler_port&#39;,8786)
        self.dashboard_port = kwarg.pop(&#39;dashboard_port&#39;,8787)
        self.nprocs = kwarg.pop(&#39;nprocs&#39;,40)
        self.nthreads = kwarg.pop(&#39;nthreads&#39;,1)
        self.scheduler_address = kwarg.pop(&#39;scheduler_address&#39;,&#39;localhost&#39;)
        self.workers_addresses_list = kwarg.pop(&#39;workers_addresses_list&#39;,[&#39;monod10&#39;,&#39;monod11&#39;,&#39;monod12&#39;,&#39;monod33&#39;])
        self.reuse_cluster = kwarg.pop(&#39;reuse_cluster&#39;,False)
        self.active_client = kwarg.pop(&#39;active_client&#39;,None)
        self.active_cluster = kwarg.pop(&#39;active_cluster&#39;,None)
        self.active_scheduler_address = kwarg.pop(&#39;active_scheduler_address&#39;,None)
        
        self.start_from_preprocessed_imgs = kwarg.pop(&#39;maximum_jobs&#39;,False)
        self.resume = kwarg.pop(&#39;resume&#39;,False)
        self.processing_engine = kwarg.pop(&#39;processing_engine&#39;,&#39;htcondor&#39;)

        # Parameters for processing in htcondor
        self.processing_env_config = {}
        self.processing_env_config[&#39;processing_engine&#39;] = self.processing_engine
        self.processing_env_config[&#39;cores&#39;] = kwarg.pop(&#39;cores&#39;,20)
        self.processing_env_config[&#39;memory&#39;] = kwarg.pop(&#39;memory&#39;,&#39;200GB&#39;)
        self.processing_env_config[&#39;disk&#39;] = kwarg.pop(&#39;disk&#39;,&#39;0.1GB&#39;)
        self.processing_env_config[&#39;local_directory&#39;] = kwarg.pop(&#39;local_directory&#39;,&#39;/tmp&#39;)
        self.processing_env_config[&#39;logs_directory&#39;] = (self.experiment_fpath / &#39;logs&#39;).as_posix()
        self.processing_env_config[&#39;adaptive&#39;] = self.adaptive
        self.processing_env_config[&#39;maximum_jobs&#39;] = self.maximum_jobs
        self.processing_env_config[&#39;scheduler_port&#39;] = self.scheduler_port
        self.processing_env_config[&#39;dashboard_port&#39;] = self.dashboard_port
        self.processing_env_config[&#39;scheduler_address&#39;] = self.scheduler_address
        self.processing_env_config[&#39;workers_addresses_list&#39;] = self.workers_addresses_list
        self.processing_env_config[&#39;nprocs&#39;] = self.nprocs
        self.processing_env_config[&#39;nthreads&#39;] = self.nthreads


        # Define the experiment folder location in the storage HD
        self.storage_experiment_fpath = (Path(self.raw_data_folder_storage_path) / self.experiment_fpath.stem).as_posix()
        self.parsed_raw_data_fpath = self.experiment_fpath / (self.experiment_fpath.stem + &#39;_&#39; + self.parsed_image_tag + &#39;.zarr&#39;) 
    

    # -----------------------------------
    # PROCESSING STEPS
    # ------------------------------------

    def save_git_commit(self):
        hash_str =  utils.get_git_hash()
        processing_info = {}
        processing_info[&#39;git_commit_hash&#39;] = hash_str
        processing_info_fpath = self.experiment_fpath / &#39;results&#39; /&#39;git_info.yaml&#39;
        with open(processing_info_fpath, &#39;w&#39;) as new_config:
                    yaml.safe_dump(processing_info, new_config,default_flow_style=False,sort_keys=False)

    def create_folders_step(self):
        &#34;&#34;&#34;
            Create the folder structure used for the data processing. If the folder
            is already present it won&#39;t overwrite it

            Folder structure
                - codebook: contain the codebooks used for the analysis
                - original_robofish_logs: contains all the original robofish logs.
                - extra_files: contains the extra files acquired during imaging.
                - extra_processing_data: contains extra files used in the analysis
                    like the dark images for flat field correction.
                - pipeline_config: contains all the configuration files.
                - raw_data: contains the renamed .nd2 files and the corresponding .pkl metadata 
                                        files
                - output_figures: contains the reports and visualizations
                - notebooks: contain potential notebooks used for processing the data
                - probes: contains the fasta file with the probes used in the experiment
                - fresh_tissue: contain the images and the process data obtained from 
                                                        imaging the fresh tissue before eel processing
                - logs: contains the dask and htcondor logs
                - microscope_tiles_coords: contain the coords of the FOVs according to the 
                                                                                                microscope stage.
                - results: contains all the processing results            
        &#34;&#34;&#34;

        utils.create_folder_structure(self.experiment_fpath, self.run_type)

    
    def prepare_processing_dataset_step(self):
        &#34;&#34;&#34;
            If a path to an existing dataset is entered it will be loaded otherwise
            it will create a new dataset that will have all the info that characterize the
            experiment.

            Args:
                zarr_file_path (Path): Path of the file used to build the dataset

        &#34;&#34;&#34;
        self.data = data_models.Dataset()
        if self.dataset_path:
            try:
                self.data.load_dataset(self.dataset_path)
            except:
                self.logger.error(f&#34;can&#39;t load the dataset from {self.dataset_path}&#34;)
                sys.exit(f&#34;can&#39;t load the dataset from {self.dataset_path}&#34;)
        else:
            try:
                self.data.create_full_dataset_from_zmetadata(self.parsed_raw_data_fpath)
            except:
                self.logger.error(f&#34;can&#39;t create dataset from {self.parsed_raw_data_fpath}&#34;)
                sys.exit(f&#34;can&#39;t create dataset from {self.parsed_raw_data_fpath}&#34;)
        
        self.metadata = self.data.collect_metadata(self.data.dataset)
        self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)
    
    
    def create_analysis_config_file_from_dataset_step(self):
        &#34;&#34;&#34;
            Load or create the yaml file with all the parameters for running the analysis. It will first
            load the analysis_config.yaml file present in the pipeline_config folder. If not it 
            will create one using the master template stored in the config_db directory

            The following attributes created by another step must be accessible:
            - metadata

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot load/create the analysis config because missing metadata attr&#39;)
        self.analysis_parameters = configuration_files.create_analysis_config_file_from_dataset(self.experiment_fpath, self.metadata)
        

    def processing_cluster_init_step(self):
        &#34;&#34;&#34;Create new cluster and client or reuse a cluster and client previously created
           Can connect direclty to the client/cluster using the &#39;connect_to_client&#39; flag or 
           to the scheduler with the &#39;connect_to_scheduler&#39;.
        &#34;&#34;&#34;

        # Start processing environment
        if self.reuse_cluster == &#39;connect_to_client&#39;:
            self.cluster = self.active_cluster
            self.client = self.active_client
            self.client.run(gc.collect)
            self.client.run(utils.trim_memory())
        elif self.reuse_cluster == &#39;connect_to_scheduler&#39;:
            self.client = Client(self.active_scheduler_address)
            self.client.run(gc.collect)
            self.client.run(utils.trim_memory())
        else:
            self.cluster = processing_cluster_setup.start_processing_env(self.processing_env_config)
            self.client = Client(self.cluster,asynchronous=True)
            # self.logger.debug(f&#39;Dask dashboard info {self.client.scheduler_info()}&#39;)


    def nikon_nd2_parsing_graph_step(self):
        &#34;&#34;&#34;
            Run the parsing according to what is specified by the parsing_type
            argument.

        &#34;&#34;&#34;
        microscopy_file_parsers.nikon_nd2_parsing_graph(self.experiment_fpath,
                                    self.parsing_type,self.parsed_image_tag, 
                                    self.storage_experiment_fpath,
                                    self.client)
    


    def determine_tiles_organization(self):
        &#34;&#34;&#34;
            Determine the organization of the field of views in the dataset

            The following attributes created by another step must be accessible:
            - analysis_parameters
            - dataset
            - metadata

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot determine tiles organization because missing metadata attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot determine tiles organization because missing dataset attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot determine tiles organization because missing analysis_parameters attr&#39;)
        self.reference_round = self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
        self.tiles_org = stitching.organize_square_tiles(self.experiment_fpath,
                                    self.data.dataset,self.metadata,
                                    self.reference_round)
        self.tiles_org.run_tiles_organization()
        self.tile_corners_coords_pxl = self.tiles_org.tile_corners_coords_pxl


    def determine_tiles_organization_before_room_reorganisation(self):
        &#34;&#34;&#34;
            Determine the organization of the field of views in the dataset

            The following attributes created by another step must be accessible:
            - analysis_parameters
            - dataset
            - metadata

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot determine tiles organization because missing metadata attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot determine tiles organization because missing dataset attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot determine tiles organization because missing analysis_parameters attr&#39;)
        self.reference_round = self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
        self.tiles_org = stitching.organize_square_tiles_old_room(self.experiment_fpath,
                                    self.data.dataset,self.metadata,
                                    self.reference_round)
        self.tiles_org.run_tiles_organization()
        self.tile_corners_coords_pxl = self.tiles_org.tile_corners_coords_pxl




    def create_running_functions_step(self):
        &#34;&#34;&#34;
            Create the dictionary with the function names used to run a specific pipeline as defined
            in metadata[&#39;pipeline&#39;]

            The following attributes created by another step must be accessible:
            - metadata

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot create running functions because missing metadata attr&#39;)
        self.running_functions = configuration_files.create_function_runner(self.experiment_fpath,self.metadata)



    def processing_barcoded_eel_step(self):
        &#34;&#34;&#34;
            Create and run a dask delayed task graph used to process barcoded eel experiments
            It runs:
            (1) Image preprocessing
            (2) Dot calling
            (3) Field of view registration
            (4) Barcode decoding
            (5) Registration to the microscope coords
            (6) Consolidate the processed images zarr file metadata
            (7) Create a simple output for quick visualization
            
            The following attributes created by another step must be accessible:
            - metadata
            - analysis_parameters
            - running_functions
            - grpd_fovs
            - client

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
        assert self.running_functions, self.logger.error(f&#39;cannot process eel fovs because missing running_functions attr&#39;)
        assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
        assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
        assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
    
        fov_processing.processing_barcoded_eel_fov_graph(self.experiment_fpath,self.analysis_parameters,
                                    self.running_functions, self.tiles_org,self.metadata,
                                    self.grpd_fovs,self.save_intermediate_steps, 
                                    self.preprocessed_image_tag,self.client,self.chunk_size,self.save_bits_int,
                                    self.start_from_preprocessed_imgs)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;microscope_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;decoded_fov&#39;,
                                file_tag=&#39;microscope_stitched&#39;)
        # ----------------------------------------------------------------  



    def rerun_decoding_step(self):
        &#34;&#34;&#34;
            Create and run a dask delayed task graph used to process barcoded eel experiments
            It runs:
            (2) Barcode decoding
            (3) Registration to the microscope coords
            
            The following attributes created by another step must be accessible:
            - metadata
            - analysis_parameters
            - grpd_fovs
            - client

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
        assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
        assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
        assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
    

        fov_processing.processing_barcoded_eel_fov_graph_from_decoding(self.experiment_fpath,self.analysis_parameters,
                                    self.tiles_org,self.metadata,
                                    self.grpd_fovs, self.client, self.chunk_size)



    def rerun_from_registration_step(self):
        &#34;&#34;&#34;
            Create and run a dask delayed task graph from the registration step.
            Requires the raw_counts files
            
            It runs:
            (1) Field of view registration
            (2) Barcode decoding
            (3) Registration to the microscope coords
            
            The following attributes created by another step must be accessible:
            - metadata
            - analysis_parameters
            - running_functions
            - grpd_fovs
            - client
            - tiles_org

        &#34;&#34;&#34;

        assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
        assert self.running_functions, self.logger.error(f&#39;cannot process eel fovs because missing running_functions attr&#39;)
        assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
        assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
        assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
        
        fov_processing.processing_barcoded_eel_fov_starting_from_registration_graph(self.experiment_fpath,
                                    self.analysis_parameters,
                                    self.running_functions, 
                                    self.tiles_org,
                                    self.metadata,
                                    self.grpd_fovs,
                                    self.preprocessed_image_tag, 
                                    self.client, 
                                    self.chunk_size, 
                                    self.save_bits_int)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;microscope_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;decoded_fov&#39;,
                                file_tag=&#39;microscope_stitched&#39;)
        # ----------------------------------------------------------------  

    def processing_serial_fish_step(self):
        &#34;&#34;&#34;
            Create and run a dask delayed task graph used to process serial smFISH experiments
            It runs:
            (1) Image preprocessing
            (2) Dot calling
            (3) Field of view registration
            (4) Registration to the microscope coords
            (5) Consolidate the processed images zarr file metadata
            (6) Create a simple output for quick visualization
            
            The following attributes created by another step must be accessible:
            - metadata
            - analysis_parameters
            - running_functions
            - grpd_fovs
            - client

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot process smFISH fovs because missing metadata attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process smFISH fovs because missing analysis_parameters attr&#39;)
        assert self.running_functions, self.logger.error(f&#39;cannot process smFISH fovs because missing running_functions attr&#39;)
        assert self.grpd_fovs, self.logger.error(f&#39;cannot process smFISH fovs because missing grpd_fovs attr&#39;)
        assert self.client, self.logger.error(f&#39;cannot process smFISH fovs because missing client attr&#39;)
        assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
    
        
        fov_processing.processing_serial_fish_fov_graph(self.experiment_fpath,self.analysis_parameters,
                                    self.running_functions, self.tiles_org,self.metadata,
                                    self.grpd_fovs,self.save_intermediate_steps, 
                                    self.preprocessed_image_tag,self.client,self.chunk_size)

        
        # Removed the dots on the microscope stitched
        self.stitching_selected = &#39;microscope_stitched&#39;

        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)
 
        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;microscope_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;microscope_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_microscope_stitched&#39;)
        # ---------------------------------------------------------------- 

    
    def microscope_stitched_remove_dots_eel_graph_step(self):
        &#34;&#34;&#34;
            Function to remove the duplicated 
            barcodes present in the overlapping regions of the tiles after stitching
            using the microscope coords

            Args:
            ----
            hamming_distance (int): Value to select the barcodes that are passing the 
                screening (&lt; hamming_distance). Default = 3
            same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
                Default = 10
            stitching_selected (str): barcodes coords set where the duplicated dots will be
                removed

            The following attributes created by another step must be accessible:
            - dataset
            - tiles_org
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
        
        assert (isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles) | 
                isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles_old_room)), \
                            self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
        
        
        # Removed the dots on the microscope stitched
        self.stitching_selected = &#39;microscope_stitched&#39;

        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)
 
        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;microscope_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;microscope_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_microscope_stitched&#39;)
        # ---------------------------------------------------------------- 

    def stitch_and_remove_dots_eel_graph_step(self):

        &#34;&#34;&#34;
            Function to stitch the different fovs and remove the duplicated 
            barcodes present in the overlapping regions of the tiles

            Args:
            ----
            hamming_distance (int): Value to select the barcodes that are passing the 
                screening (&lt; hamming_distance). Default = 3
            same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
                Default = 10
            stitching_selected (str): barcodes coords set where the duplicated dots will be
                removed

            The following attributes created by another step must be accessible:
            - dataset
            - tiles_org
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
        assert isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles), \
                            self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
        
        
        self.adjusted_coords = stitching.stitching_graph(self.experiment_fpath,self.metadata[&#39;stitching_channel&#39;],
                                                                    self.tiles_org, self.metadata,
                                                                    self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;], 
                                                                    self.client)
        
        
        # Recalculate the overlapping regions after stitching
        self.tiles_org.tile_corners_coords_pxl = self.adjusted_coords
        self.tiles_org.determine_overlapping_regions()
        
        # Removed the dots on the global stitched
        self.stitching_selected = &#39;global_stitched&#39;
        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected,
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_global_stitched&#39;)
        # ----------------------------------------------------------------  

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_removed&#39;,
                                file_tag=&#39;removed_global_stitched&#39;)
        # ---------------------------------------------------------------- 

    def stitch_and_remove_dots_eel_graph_old_room_step(self):

        &#34;&#34;&#34;
            Function to stitch the different fovs and remove the duplicated 
            barcodes present in the overlapping regions of the tiles

            Args:
            ----
            hamming_distance (int): Value to select the barcodes that are passing the 
                screening (&lt; hamming_distance). Default = 3
            same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
                Default = 10
            stitching_selected (str): barcodes coords set where the duplicated dots will be
                removed

            The following attributes created by another step must be accessible:
            - dataset
            - tiles_org
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
        assert isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles), \
                            self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
        
        
        self.adjusted_coords = stitching.stitching_graph(self.experiment_fpath,self.metadata[&#39;stitching_channel&#39;],
                                                                    self.tiles_org, self.metadata,
                                                                    self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;], 
                                                                    self.client)
        
        
        # Recalculate the overlapping regions after stitching
        self.tiles_org.tile_corners_coords_pxl = self.adjusted_coords
        self.tiles_org.determine_overlapping_regions()
        
        # Removed the dots on the global stitched
        self.stitching_selected = &#39;global_stitched&#39;
        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected,
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_global_stitched&#39;)
        # ----------------------------------------------------------------  

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_removed&#39;,
                                file_tag=&#39;removed_global_stitched&#39;)
        # ---------------------------------------------------------------- 

    
    
    
    def stitch_and_remove_dots_eel_graph_step(self):

        &#34;&#34;&#34;
            Function to stitch the different fovs and remove the duplicated 
            barcodes present in the overlapping regions of the tiles

            Args:
            ----
            hamming_distance (int): Value to select the barcodes that are passing the 
                screening (&lt; hamming_distance). Default = 3
            same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
                Default = 10
            stitching_selected (str): barcodes coords set where the duplicated dots will be
                removed

            The following attributes created by another step must be accessible:
            - dataset
            - tiles_org
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
        assert isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles), \
                            self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
        
        
        self.adjusted_coords = stitching.stitching_graph_serial_nuclei(self.experiment_fpath,self.metadata[&#39;stitching_channel&#39;],
                                                                    self.tiles_org, self.metadata,
                                                                    self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;], 
                                                                    self.client)
        
        
        # Recalculate the overlapping regions after stitching
        self.tiles_org.tile_corners_coords_pxl = self.adjusted_coords
        self.tiles_org.determine_overlapping_regions()
        
        # Removed the dots on the global stitched
        self.stitching_selected = &#39;global_stitched&#39;
        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected,
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_global_stitched&#39;)
        # ----------------------------------------------------------------  

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_removed&#39;,
                                file_tag=&#39;removed_global_stitched&#39;)
        # ---------------------------------------------------------------- 
    
    

    
    def processing_fresh_tissue_step(self,parsing=True,
                                    tag_ref_beads=&#39;_ChannelEuropium_Cy3_&#39;,
                                    tag_nuclei=&#39;_ChannelCy3_&#39;):
        &#34;&#34;&#34;
            This function create and run a processing graph that parse and filter the nuclei staining in fresh tissue
            and parse, filter and counts the Europium beads used for the registration with the smFISH images at high
            power.

            Args:
            ----
            tag_ref_beads (str): The tag reference of the .nd2 file containing the raw beads images. Default: &#39;_ChannelEuropium_Cy3_&#39;
            tag_ref_nuclei (str): The tag reference of the .nd2 file containing the raw images of nuclei. Default: &#39;_ChannelCy3_&#39;

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot process fresh tissue because missing client attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process fresh tissue because missing analysis_parameters attr&#39;)
        assert self.running_functions, self.logger.error(f&#39;cannot process fresh tissue because missing running_functions attr&#39;)
        fov_processing.process_fresh_sample_graph(self.experiment_fpath,self.running_functions,
                                                self.analysis_parameters, self.client,
                                                self.chunk_size,
                                                tag_ref_beads= tag_ref_beads,
                                                tag_nuclei= tag_nuclei,
                                                eel_metadata= self.metadata,
                                                parsing= parsing,
                                                save_steps_output=self.save_intermediate_steps)


        stitching.stitched_beads_on_nuclei_fresh_tissue(self.experiment_fpath,
                                      self.client,
                                      nuclei_tag=tag_nuclei,
                                      beads_tag = tag_ref_beads,
                                      round_num = 1,
                                      overlapping_percentage=5,
                                      machine=self.metadata[&#39;machine&#39;])


    # --------------------------------
    # QC STEPS (some other included in the graph function)
    # --------------------------------


    def QC_check_experiment_yaml_file_step(self):
        &#34;&#34;&#34;
            This QC function check in the fields required in the ExperimentName_config.yaml file
            are present and have the expected values. This file is important for the parsing of the
            data.
            The following keys are required:
            &#39;Stitching_type&#39;, &#39;Experiment_type&#39;, &#39;Barcode_length&#39;, &#39;Barcode&#39;, &#39;Codebook&#39;, &#39;Machine&#39;, &#39;Operator&#39;,
            &#39;Overlapping_percentage&#39;,&#39;Probes_FASTA_name&#39;,&#39;Species&#39;,&#39;Start_date&#39;,&#39;Strain&#39;,&#39;Tissue&#39;,&#39;Pipeline&#39;
        &#34;&#34;&#34;

        qc_utils.QC_check_experiment_yaml_file(self.experiment_fpath)



    def QC_registration_error_step(self):
        &#34;&#34;&#34;
            Visualise the error in the registration. It plots the error for each fov and the number of matching
            beads identified after the registration. The FOV number, the round number with the lowest number
            of matching beads and the lowest number of matching beads.

            The following attributes created by another step must be accessible:
            - analysis_parameters
            - tile_corners_coords_pxl
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot run QC on registration because missing client attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot run QC on registration because missing analysis_parameters attr&#39;)
        assert self.metadata, self.logger.error(f&#39;cannot process smFISH fovs because missing metadata attr&#39;)
        assert isinstance(self.tile_corners_coords_pxl, np.ndarray), self.logger.error(f&#39;cannot run QC on registration because missing tile_corners_coords_pxl attr&#39;)
        
        qc_reg = qc_utils.QC_registration_error(self.client, self.experiment_fpath, 
                    self.analysis_parameters, self.metadata, self.tile_corners_coords_pxl)

        qc_reg.run_qc()


    # --------------------------------
    # DATA REORGANIZATION
    # --------------------------------
    
    def transfer_data_after_processing(self):
        &#34;&#34;&#34;
            Function use to clear space in the processing folder. 
            - Tranfer parsed images zarr / filtered images zarr / dataset
            - Remove the log and tmp folders
            - Transfer the remaining data to cold storage
            Before transfering the data it is necessary to close the cluster and the client
            otherwise you won&#39;t be able to remove the logs folder.
        &#34;&#34;&#34;

        data_organization.reorganize_processing_dir(self.experiment_fpath,
                            self.storage_fpath,
                            self.store_dataset,
                            self.dataset_folder_storage_path,
                            self.results_folder_storage_path)



    
    # --------------------------------
    # RUNNING OPTIONS
    # --------------------------------
    


    def run_setup(self):
        start = datetime.now()

        self.create_folders_step()

        self.logger = logger_utils.json_logger((self.experiment_fpath / &#39;logs&#39;),&#39;pipeline_run&#39;) 
        self.logger.info(f&#34;Start parsing&#34;)

        self.save_git_commit()
        self.logger.info(f&#39;Saved current git commit version&#39;)

        if self.run_type == &#39;original&#39;:
            self.QC_check_experiment_yaml_file_step()
            self.logger.info(f&#39;Checked config file&#39;)
        
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Setup completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

    def run_cluster_activation(self):
        start = datetime.now()
        self.processing_cluster_init_step()
        self.logger.info(f&#39;Started dask processing cluster&#39;)
        self.logger.info(f&#34;client dashboard {self.client.scheduler_info()[&#39;services&#39;][&#39;dashboard&#39;]}&#34;)
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Cluester activation completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
    
    def run_parsing(self):
        start = datetime.now()
        # Run parsing only if required
        self.logger.info(f&#39;Parsing started&#39;)
        if self.parsing_type != &#39;no_parsing&#39;:
            self.nikon_nd2_parsing_graph_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Parsing completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        

    
    def run_parsing_only(self):
        &#34;&#34;&#34;
            Pipeline running the data organization and the parsing
            of the .nd2 files of the entire experiment
        &#34;&#34;&#34;

        start = datetime.now()

        self.create_folders_step()

        self.logger = logger_utils.json_logger((self.experiment_fpath / &#39;logs&#39;),&#39;pipeline_run&#39;) 
        self.logger.info(f&#34;Start parsing&#34;)

        self.save_git_commit()
        self.logger.info(f&#39;Saved current git commit version&#39;)

        self.QC_check_experiment_yaml_file_step()
        self.logger.info(f&#39;Checked config file&#39;)

        self.processing_cluster_init_step()
        self.logger.info(f&#39;Started dask processing cluster&#39;)
        self.logger.info(f&#34;client dashboard {self.client.scheduler_info()[&#39;services&#39;][&#39;dashboard&#39;]}&#34;)

        # Run parsing only if required
        self.logger.info(f&#39;Parsing started&#39;)
        if self.parsing_type != &#39;no_parsing&#39;:
            self.nikon_nd2_parsing_graph_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Parsing completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        
        step_start = datetime.now()
        self.logger.info(f&#39;Started creation of the dataset&#39;)
        self.prepare_processing_dataset_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
                Dataset creation completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
    
    def run_required_steps(self):
        &#34;&#34;&#34;
            Short pipeline used to make sure that the basic required
            step are run and will be included in more complex pipelines

        &#34;&#34;&#34;
        start = datetime.now()
        step_start = datetime.now()
        self.logger.info(f&#39;Started creation of the dataset&#39;)
        self.prepare_processing_dataset_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
                Dataset creation completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        self.create_analysis_config_file_from_dataset_step()
        self.logger.info(f&#39;Created analysis_config.yaml file&#39;)
        self.determine_tiles_organization()
        self.logger.info(f&#39;Determined the tiles organization&#39;)
        self.create_running_functions_step()
        self.logger.info(f&#39;Created the running function dictionary&#39;)
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
                Required steps completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        self.logger.info(f&#34;&#34;)
    
    def run_full(self):
        &#34;&#34;&#34;
            Full run from raw images from nikon or parsed images
        &#34;&#34;&#34;

        start = datetime.now()
        self.run_setup()
        self.run_cluster_activation()
        self.run_parsing()
        self.run_required_steps()    
        
        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*barcodes_max_array*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


        if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
            step_start = datetime.now()
            self.processing_barcoded_eel_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            self.QC_registration_error_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            self.microscope_stitched_remove_dots_eel_graph_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            
            step_start = datetime.now()
            try: 
                self.stitch_and_remove_dots_eel_graph_step()
            except:
                self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
                pass
            else:
                self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.processing_fresh_tissue_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Processing fresh tissue completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        elif self.metadata[&#39;experiment_type&#39;] == &#39;smfish-serial&#39;:
            step_start = datetime.now()
            self.processing_serial_fish_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    serial smfish fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        else:
            self.logger.error(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)
            sys.exit(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)


        step_start = datetime.now()
        # self.transfer_data_after_processing()
        # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
        #             data transfer after processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

        
        self.client.close()
        self.cluster.close()
        if self.processing_engine == &#39;unmanaged_cluster&#39;:
            processing_cluster_setup.kill_process()

    

    def test_run_after_editing(self):
        &#34;&#34;&#34;
            Full run from raw images from nikon or parsed images
        &#34;&#34;&#34;

        start = datetime.now()    
        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


        if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
            step_start = datetime.now()
            self.processing_barcoded_eel_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            # step_start = datetime.now()
            # self.QC_registration_error_step()
            # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
            #         QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

        # self.client.close()
        # self.cluster.close()


    def test_run_short(self):
        start = datetime.now()
        self.run_parsing_only()
        self.run_required_steps()
        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


        if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
            step_start = datetime.now()
            self.processing_barcoded_eel_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.QC_registration_error_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        
        self.client.close()
        self.cluster.close()

    def test_run_decoding(self):
        &#34;&#34;&#34;
            Run analysis starting from the raw data files.
            Requires raw files 
        &#34;&#34;&#34;

        raw_files_path = list((self.experiment_fpath / &#39;results&#39;).glob(&#39;*_decoded_fov_*&#39;))

        start = datetime.now()
        self.run_parsing_only()
        self.run_required_steps()
        if raw_files_path:
            
            
            step_start = datetime.now()  
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.rerun_decoding_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        else:
            self.logger.error(f&#34;{self.experiment_fpath.stem} missing files with raw counts&#34;)
            raise FileNotFoundError
        
        self.client.close()
        self.cluster.close()


    def test_run_from_registration(self):
        &#34;&#34;&#34;
            Run analysis starting from the raw data files.
            Requires raw files 
        &#34;&#34;&#34;

        raw_files_path = list((self.experiment_fpath / &#39;results&#39;).glob(&#39;*_raw_counts_*&#39;))
        start = datetime.now()
        # self.run_parsing_only()
        # self.run_required_steps()
        
        if raw_files_path:

            if self.resume:
                already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
                already_done_fovs = []
                for fname in already_processed:
                    fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                    already_done_fovs.append(fov_num)
                not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
                self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
                self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)
            
            step_start = datetime.now()  
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.rerun_from_registration_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.QC_registration_error_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.microscope_stitched_remove_dots_eel_graph_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            try: 
                self.stitch_and_remove_dots_eel_graph_step()
            except:
                self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
                pass
            else:
                self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        else:
            self.logger.error(f&#34;{self.experiment_fpath.stem} missing files with raw counts&#34;)
            raise FileNotFoundError
        
        self.client.close()
        self.cluster.close()

    def test_run_Lars_mouse_atlas(self):
        &#34;&#34;&#34;
        The data were acquired before the rearrangement of the room and 
        the fresh tissue data combined the DAPI/Europium in one single 
        image. 
        TODO: The fresh tissue processing need to be specifically implemented
            for this experiment

        &#34;&#34;&#34;

        start = datetime.now()
        self.run_setup()
        self.run_cluster_activation()
        self.run_parsing()
        self.run_required_steps()   
        self.determine_tiles_organization_before_room_reorganisation() 
        
        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*barcodes_max_array*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)

        # Adjust the dataset because changes in the yaml files
        self.data.dataset.loc[self.data.dataset.channel == &#39;Cy5&#39;,&#39;processing_type&#39;] = &#39;fish&#39;
        self.data.dataset.loc[self.data.dataset.channel == &#39;Europium&#39;,&#39;processing_type&#39;] = &#39;both-beads&#39;


        if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
            step_start = datetime.now()
            self.processing_barcoded_eel_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            self.QC_registration_error_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            self.microscope_stitched_remove_dots_eel_graph_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            
            step_start = datetime.now()
            try: 
                self.stitch_and_remove_dots_eel_graph_step()
            except:
                self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
                pass
            else:
                self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        else:
            self.logger.error(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)
            sys.exit(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)


        step_start = datetime.now()
        # self.transfer_data_after_processing()
        # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
        #             data transfer after processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

        
        self.client.close()
        self.cluster.close()
        if self.processing_engine == &#39;unmanaged_cluster&#39;:
            processing_cluster_setup.kill_process()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pysmFISH.pipeline.Pipeline"><code class="flex name class">
<span>class <span class="ident">Pipeline</span></span>
<span>(</span><span>pipeline_run_name:Â str, experiment_fpath:Â str, run_type:Â strÂ =Â 'new', parsing_type:Â strÂ =Â 'original', **kwarg)</span>
</code></dt>
<dd>
<div class="desc"><p>General pipeline class used for running barcoded eel or serial smFISH experiments.
The modules are used as steps to build a pipeline</p>
<p>The exposed attributes allow to modify the properties of the runnning pipline
on the fly</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pipeline_run_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the running pipeline made of current datetime and
experiment name (ex. '210507_13_25_21_LBEXP20210226_EEL_HE_2100um')</dd>
<dt><strong><code>experiment_fpath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the experiment folder</dd>
<dt><strong><code>run_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Define if it is a 'new' or 're-run' (default: new)</dd>
<dt><strong><code>parsing_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Define the parsing type. Can be:
original/no_parsing/reparsing_from_processing_folder/reparsing_from_storage
(default: original)</dd>
</dl>
<p><strong>Optional KWargs</strong>:<br>
raw_data_folder_storage_path (str): Path to the cold storage hard drive (default: /fish/rawdata)<br>
parsed_image_tag (str): Tag to identify the zarr file with parsed images (default: img_data)<br>
preprocessed_image_tag (str): Tag to identify the zarr file with preprocessed images
(default: preprocessed_img_data)<br>
dataset_folder_storage_path (str): Path to the location where the dataset are stored (default: /fish/fish_datasets)<br>
results_folder_storage_path (str): Path to the location where the dataset are stored (default: /fish/fish_results)
save_intermediate_steps (bool): Determine if the processed images will be saved (default: True)
dataset_path (str): Path to an existing dataset that will be used in the processing
chunk_size (int): Number of FOV to process in parallel
same_dot_radius_duplicate_dots (float): Searching distance that define two dots as identical
(default: 5)
stitching_selected (str): Define the stitched counts on which the overlapping dotes will be removed
(default: microscope_stitched)
hamming_distance (int): Value to select the barcodes that are passing the
screening (&lt; hamming_distance). (default: 3)</p>
<pre><code>processing_engine (str): Define the name of the system that will run the processing. Can be local/htcondor
                        (default htcondor). If engine == local the parameters that define the cluster
                        will be ignored
cores (int): Number of cores/job to use in htcondor or in the local processing (default 20). In the
                the unmanaged cluster correspond to the nummber of core for each process (nprocs)
memory (str): Total memory for all the cores in condor (default 200GB) or per core in local setup
            or per process (nprocs) in the unmanaged cluster
disk (str): Size of the spillover disk for dask (default 0.1GB)
local_directory (str): Directory where to spill over on the node (default /tmp)
logs_directory: (str): Directory where to store dask and htcondor logs
adaptive: (bool): Decide if the cluster can increase/decrease the number of worker accroding to
                    the processig required. (default True)
maximum_jobs (int): Max number of jobs to run in htcondor
scheduler_port (int): define the dask scheduler port. Used for the unmanaged cluster (default 23875) 
dashboard_port (int): define the dask dashboard port: Used for the unmanaged cluser (default 8787)
scheduler_address (str): Address of the dask scheduler. Used for the unmanaged cluser. 
                    'localhost' if running of the main node (default 'localhost)
workers_addresses_list (list[str]): Addresses of the workers (default [monod10,monod11,monod12,monod33])
nprocs (int): number of processes for each workers (unmanaged cluster) (default 40 for single node monod)
nthreads (int): number threads/process (default 1)
save_bits_int: (bool): Save the intensity of the bits and the flipping direction
start_from_preprocessed_imgs (bool): Run the processing starting from the counting
    using preprocessed images. default: False 
resume: (bool): Restart the processsing. Determine automatically which files are already processed by checking
                the *_*decoded_* files in the results folder
reuse_cluster (str): Connect the pipeline to a previously created cluster (default False). Can be: 'connect_to_client' ,'connect_to_scheduler'
active_cluster (dask_cluster): Already active cluster to reconnect to when you want to reuse a cluster
                                (default None)
active_client (dask_client): Already active client to reconnect to when you want to reuse a cluster
                                (default None)
active_scheduler_address (str): Running cluster to connect when you want reuse a cluster
</code></pre>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>storage_experiment_fpath</code></strong></dt>
<dd>Path to folder in the storage HD where to store (or are stored) the raw data for
the current experiment</dd>
<dt><strong><code>parsed_raw_data_fpath</code></strong></dt>
<dd>Path to the zarr file containing the raw data</dd>
<dt><strong><code>analysis_parameters</code></strong></dt>
<dd>Parameters used for running the analysis</dd>
<dt><strong><code>metadata</code></strong></dt>
<dd>Dictionary with the parameters that characterize the current experiment</dd>
<dt><strong><code>preprocessed_zarr_fpath</code></strong></dt>
<dd>Path to the zarr file containing the preprocessed images</dd>
<dt><strong><code>cluster</code></strong></dt>
<dd>dask cluster running locally or on htcondor</dd>
<dt><strong><code>client</code></strong></dt>
<dd>dask client used to manage the cluster</dd>
<dt><strong><code>data</code></strong></dt>
<dd>Dataset that define the experiment</dd>
<dt><strong><code>grpd_fovs</code></strong></dt>
<dd>Dataset grouped-by field of view number</dd>
<dt><strong><code>reference_round</code></strong></dt>
<dd>Round used as starting point for the decoding of the codebooks</dd>
<dt><strong><code>tiles_org</code></strong></dt>
<dd>Describe the organization of the tiles in the dataset</dd>
<dt><strong><code>tile_corners_coords_pxl</code></strong></dt>
<dd>Coords of the tiles acquired by the microscope in the reference round</dd>
<dt><strong><code>running_functions</code></strong></dt>
<dd>Dictionary of functions used to define the type of filtering used for the
fish and reference channels</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Pipeline():
    &#34;&#34;&#34;
        General pipeline class used for running barcoded eel or serial smFISH experiments.
        The modules are used as steps to build a pipeline

        The exposed attributes allow to modify the properties of the runnning pipline
        on the fly


        Args:
            pipeline_run_name (str): Name of the running pipeline made of current datetime and
                    experiment name (ex. &#39;210507_13_25_21_LBEXP20210226_EEL_HE_2100um&#39;)
            experiment_fpath (str): Path to the experiment folder
            run_type (str): Define if it is a &#39;new&#39; or &#39;re-run&#39; (default: new)
            parsing_type (str): Define the parsing type. Can be: 
                            original/no_parsing/reparsing_from_processing_folder/reparsing_from_storage
                            (default: original)

        __Optional KWargs__:  
            raw_data_folder_storage_path (str): Path to the cold storage hard drive (default: /fish/rawdata)  
            parsed_image_tag (str): Tag to identify the zarr file with parsed images (default: img_data)  
            preprocessed_image_tag (str): Tag to identify the zarr file with preprocessed images 
                                        (default: preprocessed_img_data)  
            dataset_folder_storage_path (str): Path to the location where the dataset are stored (default: /fish/fish_datasets)  
            results_folder_storage_path (str): Path to the location where the dataset are stored (default: /fish/fish_results)
            save_intermediate_steps (bool): Determine if the processed images will be saved (default: True)
            dataset_path (str): Path to an existing dataset that will be used in the processing
            chunk_size (int): Number of FOV to process in parallel
            same_dot_radius_duplicate_dots (float): Searching distance that define two dots as identical
                                    (default: 5)
            stitching_selected (str): Define the stitched counts on which the overlapping dotes will be removed 
                                    (default: microscope_stitched) 
            hamming_distance (int): Value to select the barcodes that are passing the 
                                    screening (&lt; hamming_distance). (default: 3)


            processing_engine (str): Define the name of the system that will run the processing. Can be local/htcondor
                                    (default htcondor). If engine == local the parameters that define the cluster
                                    will be ignored
            cores (int): Number of cores/job to use in htcondor or in the local processing (default 20). In the
                            the unmanaged cluster correspond to the nummber of core for each process (nprocs)
            memory (str): Total memory for all the cores in condor (default 200GB) or per core in local setup
                        or per process (nprocs) in the unmanaged cluster
            disk (str): Size of the spillover disk for dask (default 0.1GB)
            local_directory (str): Directory where to spill over on the node (default /tmp)
            logs_directory: (str): Directory where to store dask and htcondor logs
            adaptive: (bool): Decide if the cluster can increase/decrease the number of worker accroding to
                                the processig required. (default True)
            maximum_jobs (int): Max number of jobs to run in htcondor
            scheduler_port (int): define the dask scheduler port. Used for the unmanaged cluster (default 23875) 
            dashboard_port (int): define the dask dashboard port: Used for the unmanaged cluser (default 8787)
            scheduler_address (str): Address of the dask scheduler. Used for the unmanaged cluser. 
                                &#39;localhost&#39; if running of the main node (default &#39;localhost)
            workers_addresses_list (list[str]): Addresses of the workers (default [monod10,monod11,monod12,monod33])
            nprocs (int): number of processes for each workers (unmanaged cluster) (default 40 for single node monod)
            nthreads (int): number threads/process (default 1)
            save_bits_int: (bool): Save the intensity of the bits and the flipping direction
            start_from_preprocessed_imgs (bool): Run the processing starting from the counting
                using preprocessed images. default: False 
            resume: (bool): Restart the processsing. Determine automatically which files are already processed by checking
                            the *_*decoded_* files in the results folder
            reuse_cluster (str): Connect the pipeline to a previously created cluster (default False). Can be: &#39;connect_to_client&#39; ,&#39;connect_to_scheduler&#39;
            active_cluster (dask_cluster): Already active cluster to reconnect to when you want to reuse a cluster
                                            (default None)
            active_client (dask_client): Already active client to reconnect to when you want to reuse a cluster
                                            (default None)
            active_scheduler_address (str): Running cluster to connect when you want reuse a cluster

        Attributes:
            storage_experiment_fpath: Path to folder in the storage HD where to store (or are stored) the raw data for
                                        the current experiment
            parsed_raw_data_fpath: Path to the zarr file containing the raw data
            analysis_parameters: Parameters used for running the analysis
            metadata: Dictionary with the parameters that characterize the current experiment
            preprocessed_zarr_fpath: Path to the zarr file containing the preprocessed images
            cluster: dask cluster running locally or on htcondor
            client: dask client used to manage the cluster
            data: Dataset that define the experiment
            grpd_fovs: Dataset grouped-by field of view number
            reference_round: Round used as starting point for the decoding of the codebooks
            tiles_org: Describe the organization of the tiles in the dataset
            tile_corners_coords_pxl: Coords of the tiles acquired by the microscope in the reference round
            running_functions: Dictionary of functions used to define the type of filtering used for the 
                                fish and reference channels

    &#34;&#34;&#34;

    def __init__(self, pipeline_run_name:str, experiment_fpath:str,
                run_type:str = &#39;new&#39;, parsing_type:str = &#39;original&#39;, **kwarg):

        self.pipeline_run_name = pipeline_run_name
        self.experiment_fpath = Path(experiment_fpath)
        self.run_type = run_type
        self.parsing_type = parsing_type

        # Collect some of the parameters. If missing a predefined value is assigned
        self.raw_data_folder_storage_path = kwarg.pop(&#39;raw_data_folder_storage_path&#39;, &#39;/fish/rawdata&#39;)
        self.parsed_image_tag = kwarg.pop(&#39;parsed_image_tag&#39;,&#39;img_data&#39;)
        self.preprocessed_image_tag = kwarg.pop(&#39;filtered_image_tag&#39;,&#39;preprocessed_img_data&#39;)
        self.dataset_folder_storage_path = kwarg.pop(&#39;dataset_folder_storage_path&#39;,&#39;/fish/fish_datasets&#39;)
        self.results_folder_storage_path = kwarg.pop(&#39;results_folder_storage_path&#39;,&#39;/fish/fish_results&#39;)
        self.save_intermediate_steps = kwarg.pop(&#39;save_intermediate_steps&#39;,True)
        self.dataset_path = kwarg.pop(&#39;dataset_path&#39;,&#39;&#39;)
        self.store_dataset = kwarg.pop(&#39;store_dataset&#39;,True)
        self.chunk_size = kwarg.pop(&#39;chunk_size&#39;,20)
        self.same_dot_radius_duplicate_dots = kwarg.pop(&#39;same_dot_radius_duplicate_dots&#39;,5)
        self.stitching_selected = kwarg.pop(&#39;stitching_selected&#39;,&#39;microscope_stitched&#39;)
        self.hamming_distance = kwarg.pop(&#39;hamming_distance&#39;,3)
        self.save_bits_int = kwarg.pop(&#39;save_bits_int&#39;,True)
        self.adaptive = kwarg.pop(&#39;adaptive&#39;,True)
        self.maximum_jobs = kwarg.pop(&#39;maximum_jobs&#39;,15)
        self.scheduler_port = kwarg.pop(&#39;scheduler_port&#39;,8786)
        self.dashboard_port = kwarg.pop(&#39;dashboard_port&#39;,8787)
        self.nprocs = kwarg.pop(&#39;nprocs&#39;,40)
        self.nthreads = kwarg.pop(&#39;nthreads&#39;,1)
        self.scheduler_address = kwarg.pop(&#39;scheduler_address&#39;,&#39;localhost&#39;)
        self.workers_addresses_list = kwarg.pop(&#39;workers_addresses_list&#39;,[&#39;monod10&#39;,&#39;monod11&#39;,&#39;monod12&#39;,&#39;monod33&#39;])
        self.reuse_cluster = kwarg.pop(&#39;reuse_cluster&#39;,False)
        self.active_client = kwarg.pop(&#39;active_client&#39;,None)
        self.active_cluster = kwarg.pop(&#39;active_cluster&#39;,None)
        self.active_scheduler_address = kwarg.pop(&#39;active_scheduler_address&#39;,None)
        
        self.start_from_preprocessed_imgs = kwarg.pop(&#39;maximum_jobs&#39;,False)
        self.resume = kwarg.pop(&#39;resume&#39;,False)
        self.processing_engine = kwarg.pop(&#39;processing_engine&#39;,&#39;htcondor&#39;)

        # Parameters for processing in htcondor
        self.processing_env_config = {}
        self.processing_env_config[&#39;processing_engine&#39;] = self.processing_engine
        self.processing_env_config[&#39;cores&#39;] = kwarg.pop(&#39;cores&#39;,20)
        self.processing_env_config[&#39;memory&#39;] = kwarg.pop(&#39;memory&#39;,&#39;200GB&#39;)
        self.processing_env_config[&#39;disk&#39;] = kwarg.pop(&#39;disk&#39;,&#39;0.1GB&#39;)
        self.processing_env_config[&#39;local_directory&#39;] = kwarg.pop(&#39;local_directory&#39;,&#39;/tmp&#39;)
        self.processing_env_config[&#39;logs_directory&#39;] = (self.experiment_fpath / &#39;logs&#39;).as_posix()
        self.processing_env_config[&#39;adaptive&#39;] = self.adaptive
        self.processing_env_config[&#39;maximum_jobs&#39;] = self.maximum_jobs
        self.processing_env_config[&#39;scheduler_port&#39;] = self.scheduler_port
        self.processing_env_config[&#39;dashboard_port&#39;] = self.dashboard_port
        self.processing_env_config[&#39;scheduler_address&#39;] = self.scheduler_address
        self.processing_env_config[&#39;workers_addresses_list&#39;] = self.workers_addresses_list
        self.processing_env_config[&#39;nprocs&#39;] = self.nprocs
        self.processing_env_config[&#39;nthreads&#39;] = self.nthreads


        # Define the experiment folder location in the storage HD
        self.storage_experiment_fpath = (Path(self.raw_data_folder_storage_path) / self.experiment_fpath.stem).as_posix()
        self.parsed_raw_data_fpath = self.experiment_fpath / (self.experiment_fpath.stem + &#39;_&#39; + self.parsed_image_tag + &#39;.zarr&#39;) 
    

    # -----------------------------------
    # PROCESSING STEPS
    # ------------------------------------

    def save_git_commit(self):
        hash_str =  utils.get_git_hash()
        processing_info = {}
        processing_info[&#39;git_commit_hash&#39;] = hash_str
        processing_info_fpath = self.experiment_fpath / &#39;results&#39; /&#39;git_info.yaml&#39;
        with open(processing_info_fpath, &#39;w&#39;) as new_config:
                    yaml.safe_dump(processing_info, new_config,default_flow_style=False,sort_keys=False)

    def create_folders_step(self):
        &#34;&#34;&#34;
            Create the folder structure used for the data processing. If the folder
            is already present it won&#39;t overwrite it

            Folder structure
                - codebook: contain the codebooks used for the analysis
                - original_robofish_logs: contains all the original robofish logs.
                - extra_files: contains the extra files acquired during imaging.
                - extra_processing_data: contains extra files used in the analysis
                    like the dark images for flat field correction.
                - pipeline_config: contains all the configuration files.
                - raw_data: contains the renamed .nd2 files and the corresponding .pkl metadata 
                                        files
                - output_figures: contains the reports and visualizations
                - notebooks: contain potential notebooks used for processing the data
                - probes: contains the fasta file with the probes used in the experiment
                - fresh_tissue: contain the images and the process data obtained from 
                                                        imaging the fresh tissue before eel processing
                - logs: contains the dask and htcondor logs
                - microscope_tiles_coords: contain the coords of the FOVs according to the 
                                                                                                microscope stage.
                - results: contains all the processing results            
        &#34;&#34;&#34;

        utils.create_folder_structure(self.experiment_fpath, self.run_type)

    
    def prepare_processing_dataset_step(self):
        &#34;&#34;&#34;
            If a path to an existing dataset is entered it will be loaded otherwise
            it will create a new dataset that will have all the info that characterize the
            experiment.

            Args:
                zarr_file_path (Path): Path of the file used to build the dataset

        &#34;&#34;&#34;
        self.data = data_models.Dataset()
        if self.dataset_path:
            try:
                self.data.load_dataset(self.dataset_path)
            except:
                self.logger.error(f&#34;can&#39;t load the dataset from {self.dataset_path}&#34;)
                sys.exit(f&#34;can&#39;t load the dataset from {self.dataset_path}&#34;)
        else:
            try:
                self.data.create_full_dataset_from_zmetadata(self.parsed_raw_data_fpath)
            except:
                self.logger.error(f&#34;can&#39;t create dataset from {self.parsed_raw_data_fpath}&#34;)
                sys.exit(f&#34;can&#39;t create dataset from {self.parsed_raw_data_fpath}&#34;)
        
        self.metadata = self.data.collect_metadata(self.data.dataset)
        self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)
    
    
    def create_analysis_config_file_from_dataset_step(self):
        &#34;&#34;&#34;
            Load or create the yaml file with all the parameters for running the analysis. It will first
            load the analysis_config.yaml file present in the pipeline_config folder. If not it 
            will create one using the master template stored in the config_db directory

            The following attributes created by another step must be accessible:
            - metadata

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot load/create the analysis config because missing metadata attr&#39;)
        self.analysis_parameters = configuration_files.create_analysis_config_file_from_dataset(self.experiment_fpath, self.metadata)
        

    def processing_cluster_init_step(self):
        &#34;&#34;&#34;Create new cluster and client or reuse a cluster and client previously created
           Can connect direclty to the client/cluster using the &#39;connect_to_client&#39; flag or 
           to the scheduler with the &#39;connect_to_scheduler&#39;.
        &#34;&#34;&#34;

        # Start processing environment
        if self.reuse_cluster == &#39;connect_to_client&#39;:
            self.cluster = self.active_cluster
            self.client = self.active_client
            self.client.run(gc.collect)
            self.client.run(utils.trim_memory())
        elif self.reuse_cluster == &#39;connect_to_scheduler&#39;:
            self.client = Client(self.active_scheduler_address)
            self.client.run(gc.collect)
            self.client.run(utils.trim_memory())
        else:
            self.cluster = processing_cluster_setup.start_processing_env(self.processing_env_config)
            self.client = Client(self.cluster,asynchronous=True)
            # self.logger.debug(f&#39;Dask dashboard info {self.client.scheduler_info()}&#39;)


    def nikon_nd2_parsing_graph_step(self):
        &#34;&#34;&#34;
            Run the parsing according to what is specified by the parsing_type
            argument.

        &#34;&#34;&#34;
        microscopy_file_parsers.nikon_nd2_parsing_graph(self.experiment_fpath,
                                    self.parsing_type,self.parsed_image_tag, 
                                    self.storage_experiment_fpath,
                                    self.client)
    


    def determine_tiles_organization(self):
        &#34;&#34;&#34;
            Determine the organization of the field of views in the dataset

            The following attributes created by another step must be accessible:
            - analysis_parameters
            - dataset
            - metadata

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot determine tiles organization because missing metadata attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot determine tiles organization because missing dataset attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot determine tiles organization because missing analysis_parameters attr&#39;)
        self.reference_round = self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
        self.tiles_org = stitching.organize_square_tiles(self.experiment_fpath,
                                    self.data.dataset,self.metadata,
                                    self.reference_round)
        self.tiles_org.run_tiles_organization()
        self.tile_corners_coords_pxl = self.tiles_org.tile_corners_coords_pxl


    def determine_tiles_organization_before_room_reorganisation(self):
        &#34;&#34;&#34;
            Determine the organization of the field of views in the dataset

            The following attributes created by another step must be accessible:
            - analysis_parameters
            - dataset
            - metadata

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot determine tiles organization because missing metadata attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot determine tiles organization because missing dataset attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot determine tiles organization because missing analysis_parameters attr&#39;)
        self.reference_round = self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
        self.tiles_org = stitching.organize_square_tiles_old_room(self.experiment_fpath,
                                    self.data.dataset,self.metadata,
                                    self.reference_round)
        self.tiles_org.run_tiles_organization()
        self.tile_corners_coords_pxl = self.tiles_org.tile_corners_coords_pxl




    def create_running_functions_step(self):
        &#34;&#34;&#34;
            Create the dictionary with the function names used to run a specific pipeline as defined
            in metadata[&#39;pipeline&#39;]

            The following attributes created by another step must be accessible:
            - metadata

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot create running functions because missing metadata attr&#39;)
        self.running_functions = configuration_files.create_function_runner(self.experiment_fpath,self.metadata)



    def processing_barcoded_eel_step(self):
        &#34;&#34;&#34;
            Create and run a dask delayed task graph used to process barcoded eel experiments
            It runs:
            (1) Image preprocessing
            (2) Dot calling
            (3) Field of view registration
            (4) Barcode decoding
            (5) Registration to the microscope coords
            (6) Consolidate the processed images zarr file metadata
            (7) Create a simple output for quick visualization
            
            The following attributes created by another step must be accessible:
            - metadata
            - analysis_parameters
            - running_functions
            - grpd_fovs
            - client

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
        assert self.running_functions, self.logger.error(f&#39;cannot process eel fovs because missing running_functions attr&#39;)
        assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
        assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
        assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
    
        fov_processing.processing_barcoded_eel_fov_graph(self.experiment_fpath,self.analysis_parameters,
                                    self.running_functions, self.tiles_org,self.metadata,
                                    self.grpd_fovs,self.save_intermediate_steps, 
                                    self.preprocessed_image_tag,self.client,self.chunk_size,self.save_bits_int,
                                    self.start_from_preprocessed_imgs)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;microscope_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;decoded_fov&#39;,
                                file_tag=&#39;microscope_stitched&#39;)
        # ----------------------------------------------------------------  



    def rerun_decoding_step(self):
        &#34;&#34;&#34;
            Create and run a dask delayed task graph used to process barcoded eel experiments
            It runs:
            (2) Barcode decoding
            (3) Registration to the microscope coords
            
            The following attributes created by another step must be accessible:
            - metadata
            - analysis_parameters
            - grpd_fovs
            - client

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
        assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
        assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
        assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
    

        fov_processing.processing_barcoded_eel_fov_graph_from_decoding(self.experiment_fpath,self.analysis_parameters,
                                    self.tiles_org,self.metadata,
                                    self.grpd_fovs, self.client, self.chunk_size)



    def rerun_from_registration_step(self):
        &#34;&#34;&#34;
            Create and run a dask delayed task graph from the registration step.
            Requires the raw_counts files
            
            It runs:
            (1) Field of view registration
            (2) Barcode decoding
            (3) Registration to the microscope coords
            
            The following attributes created by another step must be accessible:
            - metadata
            - analysis_parameters
            - running_functions
            - grpd_fovs
            - client
            - tiles_org

        &#34;&#34;&#34;

        assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
        assert self.running_functions, self.logger.error(f&#39;cannot process eel fovs because missing running_functions attr&#39;)
        assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
        assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
        assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
        
        fov_processing.processing_barcoded_eel_fov_starting_from_registration_graph(self.experiment_fpath,
                                    self.analysis_parameters,
                                    self.running_functions, 
                                    self.tiles_org,
                                    self.metadata,
                                    self.grpd_fovs,
                                    self.preprocessed_image_tag, 
                                    self.client, 
                                    self.chunk_size, 
                                    self.save_bits_int)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;microscope_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;decoded_fov&#39;,
                                file_tag=&#39;microscope_stitched&#39;)
        # ----------------------------------------------------------------  

    def processing_serial_fish_step(self):
        &#34;&#34;&#34;
            Create and run a dask delayed task graph used to process serial smFISH experiments
            It runs:
            (1) Image preprocessing
            (2) Dot calling
            (3) Field of view registration
            (4) Registration to the microscope coords
            (5) Consolidate the processed images zarr file metadata
            (6) Create a simple output for quick visualization
            
            The following attributes created by another step must be accessible:
            - metadata
            - analysis_parameters
            - running_functions
            - grpd_fovs
            - client

        &#34;&#34;&#34;
        assert self.metadata, self.logger.error(f&#39;cannot process smFISH fovs because missing metadata attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process smFISH fovs because missing analysis_parameters attr&#39;)
        assert self.running_functions, self.logger.error(f&#39;cannot process smFISH fovs because missing running_functions attr&#39;)
        assert self.grpd_fovs, self.logger.error(f&#39;cannot process smFISH fovs because missing grpd_fovs attr&#39;)
        assert self.client, self.logger.error(f&#39;cannot process smFISH fovs because missing client attr&#39;)
        assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
    
        
        fov_processing.processing_serial_fish_fov_graph(self.experiment_fpath,self.analysis_parameters,
                                    self.running_functions, self.tiles_org,self.metadata,
                                    self.grpd_fovs,self.save_intermediate_steps, 
                                    self.preprocessed_image_tag,self.client,self.chunk_size)

        
        # Removed the dots on the microscope stitched
        self.stitching_selected = &#39;microscope_stitched&#39;

        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)
 
        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;microscope_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;microscope_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_microscope_stitched&#39;)
        # ---------------------------------------------------------------- 

    
    def microscope_stitched_remove_dots_eel_graph_step(self):
        &#34;&#34;&#34;
            Function to remove the duplicated 
            barcodes present in the overlapping regions of the tiles after stitching
            using the microscope coords

            Args:
            ----
            hamming_distance (int): Value to select the barcodes that are passing the 
                screening (&lt; hamming_distance). Default = 3
            same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
                Default = 10
            stitching_selected (str): barcodes coords set where the duplicated dots will be
                removed

            The following attributes created by another step must be accessible:
            - dataset
            - tiles_org
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
        
        assert (isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles) | 
                isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles_old_room)), \
                            self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
        
        
        # Removed the dots on the microscope stitched
        self.stitching_selected = &#39;microscope_stitched&#39;

        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)
 
        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;microscope_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;microscope_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_microscope_stitched&#39;)
        # ---------------------------------------------------------------- 

    def stitch_and_remove_dots_eel_graph_step(self):

        &#34;&#34;&#34;
            Function to stitch the different fovs and remove the duplicated 
            barcodes present in the overlapping regions of the tiles

            Args:
            ----
            hamming_distance (int): Value to select the barcodes that are passing the 
                screening (&lt; hamming_distance). Default = 3
            same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
                Default = 10
            stitching_selected (str): barcodes coords set where the duplicated dots will be
                removed

            The following attributes created by another step must be accessible:
            - dataset
            - tiles_org
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
        assert isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles), \
                            self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
        
        
        self.adjusted_coords = stitching.stitching_graph(self.experiment_fpath,self.metadata[&#39;stitching_channel&#39;],
                                                                    self.tiles_org, self.metadata,
                                                                    self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;], 
                                                                    self.client)
        
        
        # Recalculate the overlapping regions after stitching
        self.tiles_org.tile_corners_coords_pxl = self.adjusted_coords
        self.tiles_org.determine_overlapping_regions()
        
        # Removed the dots on the global stitched
        self.stitching_selected = &#39;global_stitched&#39;
        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected,
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_global_stitched&#39;)
        # ----------------------------------------------------------------  

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_removed&#39;,
                                file_tag=&#39;removed_global_stitched&#39;)
        # ---------------------------------------------------------------- 

    def stitch_and_remove_dots_eel_graph_old_room_step(self):

        &#34;&#34;&#34;
            Function to stitch the different fovs and remove the duplicated 
            barcodes present in the overlapping regions of the tiles

            Args:
            ----
            hamming_distance (int): Value to select the barcodes that are passing the 
                screening (&lt; hamming_distance). Default = 3
            same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
                Default = 10
            stitching_selected (str): barcodes coords set where the duplicated dots will be
                removed

            The following attributes created by another step must be accessible:
            - dataset
            - tiles_org
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
        assert isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles), \
                            self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
        
        
        self.adjusted_coords = stitching.stitching_graph(self.experiment_fpath,self.metadata[&#39;stitching_channel&#39;],
                                                                    self.tiles_org, self.metadata,
                                                                    self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;], 
                                                                    self.client)
        
        
        # Recalculate the overlapping regions after stitching
        self.tiles_org.tile_corners_coords_pxl = self.adjusted_coords
        self.tiles_org.determine_overlapping_regions()
        
        # Removed the dots on the global stitched
        self.stitching_selected = &#39;global_stitched&#39;
        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected,
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_global_stitched&#39;)
        # ----------------------------------------------------------------  

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_removed&#39;,
                                file_tag=&#39;removed_global_stitched&#39;)
        # ---------------------------------------------------------------- 

    
    
    
    def stitch_and_remove_dots_eel_graph_step(self):

        &#34;&#34;&#34;
            Function to stitch the different fovs and remove the duplicated 
            barcodes present in the overlapping regions of the tiles

            Args:
            ----
            hamming_distance (int): Value to select the barcodes that are passing the 
                screening (&lt; hamming_distance). Default = 3
            same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
                Default = 10
            stitching_selected (str): barcodes coords set where the duplicated dots will be
                removed

            The following attributes created by another step must be accessible:
            - dataset
            - tiles_org
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
        assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
        assert isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles), \
                            self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
        
        
        self.adjusted_coords = stitching.stitching_graph_serial_nuclei(self.experiment_fpath,self.metadata[&#39;stitching_channel&#39;],
                                                                    self.tiles_org, self.metadata,
                                                                    self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;], 
                                                                    self.client)
        
        
        # Recalculate the overlapping regions after stitching
        self.tiles_org.tile_corners_coords_pxl = self.adjusted_coords
        self.tiles_org.determine_overlapping_regions()
        
        # Removed the dots on the global stitched
        self.stitching_selected = &#39;global_stitched&#39;
        stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                                self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                    self.stitching_selected, self.client)

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected,
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_cleaned&#39;,
                                file_tag=&#39;cleaned_global_stitched&#39;)
        # ----------------------------------------------------------------  

        # ----------------------------------------------------------------
        # GENERATE OUTPUT FOR PLOTTING
        selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
        stitching_selected = &#39;global_stitched&#39;
        io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                                selected_Hdistance, self.client,
                                input_file_tag = &#39;global_stitched_removed&#39;,
                                file_tag=&#39;removed_global_stitched&#39;)
        # ---------------------------------------------------------------- 
    
    

    
    def processing_fresh_tissue_step(self,parsing=True,
                                    tag_ref_beads=&#39;_ChannelEuropium_Cy3_&#39;,
                                    tag_nuclei=&#39;_ChannelCy3_&#39;):
        &#34;&#34;&#34;
            This function create and run a processing graph that parse and filter the nuclei staining in fresh tissue
            and parse, filter and counts the Europium beads used for the registration with the smFISH images at high
            power.

            Args:
            ----
            tag_ref_beads (str): The tag reference of the .nd2 file containing the raw beads images. Default: &#39;_ChannelEuropium_Cy3_&#39;
            tag_ref_nuclei (str): The tag reference of the .nd2 file containing the raw images of nuclei. Default: &#39;_ChannelCy3_&#39;

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot process fresh tissue because missing client attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot process fresh tissue because missing analysis_parameters attr&#39;)
        assert self.running_functions, self.logger.error(f&#39;cannot process fresh tissue because missing running_functions attr&#39;)
        fov_processing.process_fresh_sample_graph(self.experiment_fpath,self.running_functions,
                                                self.analysis_parameters, self.client,
                                                self.chunk_size,
                                                tag_ref_beads= tag_ref_beads,
                                                tag_nuclei= tag_nuclei,
                                                eel_metadata= self.metadata,
                                                parsing= parsing,
                                                save_steps_output=self.save_intermediate_steps)


        stitching.stitched_beads_on_nuclei_fresh_tissue(self.experiment_fpath,
                                      self.client,
                                      nuclei_tag=tag_nuclei,
                                      beads_tag = tag_ref_beads,
                                      round_num = 1,
                                      overlapping_percentage=5,
                                      machine=self.metadata[&#39;machine&#39;])


    # --------------------------------
    # QC STEPS (some other included in the graph function)
    # --------------------------------


    def QC_check_experiment_yaml_file_step(self):
        &#34;&#34;&#34;
            This QC function check in the fields required in the ExperimentName_config.yaml file
            are present and have the expected values. This file is important for the parsing of the
            data.
            The following keys are required:
            &#39;Stitching_type&#39;, &#39;Experiment_type&#39;, &#39;Barcode_length&#39;, &#39;Barcode&#39;, &#39;Codebook&#39;, &#39;Machine&#39;, &#39;Operator&#39;,
            &#39;Overlapping_percentage&#39;,&#39;Probes_FASTA_name&#39;,&#39;Species&#39;,&#39;Start_date&#39;,&#39;Strain&#39;,&#39;Tissue&#39;,&#39;Pipeline&#39;
        &#34;&#34;&#34;

        qc_utils.QC_check_experiment_yaml_file(self.experiment_fpath)



    def QC_registration_error_step(self):
        &#34;&#34;&#34;
            Visualise the error in the registration. It plots the error for each fov and the number of matching
            beads identified after the registration. The FOV number, the round number with the lowest number
            of matching beads and the lowest number of matching beads.

            The following attributes created by another step must be accessible:
            - analysis_parameters
            - tile_corners_coords_pxl
            - client

        &#34;&#34;&#34;
        assert self.client, self.logger.error(f&#39;cannot run QC on registration because missing client attr&#39;)
        assert self.analysis_parameters, self.logger.error(f&#39;cannot run QC on registration because missing analysis_parameters attr&#39;)
        assert self.metadata, self.logger.error(f&#39;cannot process smFISH fovs because missing metadata attr&#39;)
        assert isinstance(self.tile_corners_coords_pxl, np.ndarray), self.logger.error(f&#39;cannot run QC on registration because missing tile_corners_coords_pxl attr&#39;)
        
        qc_reg = qc_utils.QC_registration_error(self.client, self.experiment_fpath, 
                    self.analysis_parameters, self.metadata, self.tile_corners_coords_pxl)

        qc_reg.run_qc()


    # --------------------------------
    # DATA REORGANIZATION
    # --------------------------------
    
    def transfer_data_after_processing(self):
        &#34;&#34;&#34;
            Function use to clear space in the processing folder. 
            - Tranfer parsed images zarr / filtered images zarr / dataset
            - Remove the log and tmp folders
            - Transfer the remaining data to cold storage
            Before transfering the data it is necessary to close the cluster and the client
            otherwise you won&#39;t be able to remove the logs folder.
        &#34;&#34;&#34;

        data_organization.reorganize_processing_dir(self.experiment_fpath,
                            self.storage_fpath,
                            self.store_dataset,
                            self.dataset_folder_storage_path,
                            self.results_folder_storage_path)



    
    # --------------------------------
    # RUNNING OPTIONS
    # --------------------------------
    


    def run_setup(self):
        start = datetime.now()

        self.create_folders_step()

        self.logger = logger_utils.json_logger((self.experiment_fpath / &#39;logs&#39;),&#39;pipeline_run&#39;) 
        self.logger.info(f&#34;Start parsing&#34;)

        self.save_git_commit()
        self.logger.info(f&#39;Saved current git commit version&#39;)

        if self.run_type == &#39;original&#39;:
            self.QC_check_experiment_yaml_file_step()
            self.logger.info(f&#39;Checked config file&#39;)
        
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Setup completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

    def run_cluster_activation(self):
        start = datetime.now()
        self.processing_cluster_init_step()
        self.logger.info(f&#39;Started dask processing cluster&#39;)
        self.logger.info(f&#34;client dashboard {self.client.scheduler_info()[&#39;services&#39;][&#39;dashboard&#39;]}&#34;)
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Cluester activation completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
    
    def run_parsing(self):
        start = datetime.now()
        # Run parsing only if required
        self.logger.info(f&#39;Parsing started&#39;)
        if self.parsing_type != &#39;no_parsing&#39;:
            self.nikon_nd2_parsing_graph_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Parsing completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        

    
    def run_parsing_only(self):
        &#34;&#34;&#34;
            Pipeline running the data organization and the parsing
            of the .nd2 files of the entire experiment
        &#34;&#34;&#34;

        start = datetime.now()

        self.create_folders_step()

        self.logger = logger_utils.json_logger((self.experiment_fpath / &#39;logs&#39;),&#39;pipeline_run&#39;) 
        self.logger.info(f&#34;Start parsing&#34;)

        self.save_git_commit()
        self.logger.info(f&#39;Saved current git commit version&#39;)

        self.QC_check_experiment_yaml_file_step()
        self.logger.info(f&#39;Checked config file&#39;)

        self.processing_cluster_init_step()
        self.logger.info(f&#39;Started dask processing cluster&#39;)
        self.logger.info(f&#34;client dashboard {self.client.scheduler_info()[&#39;services&#39;][&#39;dashboard&#39;]}&#34;)

        # Run parsing only if required
        self.logger.info(f&#39;Parsing started&#39;)
        if self.parsing_type != &#39;no_parsing&#39;:
            self.nikon_nd2_parsing_graph_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Parsing completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        
        step_start = datetime.now()
        self.logger.info(f&#39;Started creation of the dataset&#39;)
        self.prepare_processing_dataset_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
                Dataset creation completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
    
    def run_required_steps(self):
        &#34;&#34;&#34;
            Short pipeline used to make sure that the basic required
            step are run and will be included in more complex pipelines

        &#34;&#34;&#34;
        start = datetime.now()
        step_start = datetime.now()
        self.logger.info(f&#39;Started creation of the dataset&#39;)
        self.prepare_processing_dataset_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
                Dataset creation completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        self.create_analysis_config_file_from_dataset_step()
        self.logger.info(f&#39;Created analysis_config.yaml file&#39;)
        self.determine_tiles_organization()
        self.logger.info(f&#39;Determined the tiles organization&#39;)
        self.create_running_functions_step()
        self.logger.info(f&#39;Created the running function dictionary&#39;)
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
                Required steps completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        self.logger.info(f&#34;&#34;)
    
    def run_full(self):
        &#34;&#34;&#34;
            Full run from raw images from nikon or parsed images
        &#34;&#34;&#34;

        start = datetime.now()
        self.run_setup()
        self.run_cluster_activation()
        self.run_parsing()
        self.run_required_steps()    
        
        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*barcodes_max_array*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


        if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
            step_start = datetime.now()
            self.processing_barcoded_eel_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            self.QC_registration_error_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            self.microscope_stitched_remove_dots_eel_graph_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            
            step_start = datetime.now()
            try: 
                self.stitch_and_remove_dots_eel_graph_step()
            except:
                self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
                pass
            else:
                self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.processing_fresh_tissue_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Processing fresh tissue completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        elif self.metadata[&#39;experiment_type&#39;] == &#39;smfish-serial&#39;:
            step_start = datetime.now()
            self.processing_serial_fish_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    serial smfish fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        else:
            self.logger.error(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)
            sys.exit(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)


        step_start = datetime.now()
        # self.transfer_data_after_processing()
        # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
        #             data transfer after processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

        
        self.client.close()
        self.cluster.close()
        if self.processing_engine == &#39;unmanaged_cluster&#39;:
            processing_cluster_setup.kill_process()

    

    def test_run_after_editing(self):
        &#34;&#34;&#34;
            Full run from raw images from nikon or parsed images
        &#34;&#34;&#34;

        start = datetime.now()    
        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


        if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
            step_start = datetime.now()
            self.processing_barcoded_eel_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            # step_start = datetime.now()
            # self.QC_registration_error_step()
            # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
            #         QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

        # self.client.close()
        # self.cluster.close()


    def test_run_short(self):
        start = datetime.now()
        self.run_parsing_only()
        self.run_required_steps()
        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


        if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
            step_start = datetime.now()
            self.processing_barcoded_eel_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.QC_registration_error_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        
        self.client.close()
        self.cluster.close()

    def test_run_decoding(self):
        &#34;&#34;&#34;
            Run analysis starting from the raw data files.
            Requires raw files 
        &#34;&#34;&#34;

        raw_files_path = list((self.experiment_fpath / &#39;results&#39;).glob(&#39;*_decoded_fov_*&#39;))

        start = datetime.now()
        self.run_parsing_only()
        self.run_required_steps()
        if raw_files_path:
            
            
            step_start = datetime.now()  
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.rerun_decoding_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        else:
            self.logger.error(f&#34;{self.experiment_fpath.stem} missing files with raw counts&#34;)
            raise FileNotFoundError
        
        self.client.close()
        self.cluster.close()


    def test_run_from_registration(self):
        &#34;&#34;&#34;
            Run analysis starting from the raw data files.
            Requires raw files 
        &#34;&#34;&#34;

        raw_files_path = list((self.experiment_fpath / &#39;results&#39;).glob(&#39;*_raw_counts_*&#39;))
        start = datetime.now()
        # self.run_parsing_only()
        # self.run_required_steps()
        
        if raw_files_path:

            if self.resume:
                already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
                already_done_fovs = []
                for fname in already_processed:
                    fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                    already_done_fovs.append(fov_num)
                not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
                self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
                self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)
            
            step_start = datetime.now()  
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.rerun_from_registration_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.QC_registration_error_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            step_start = datetime.now()
            self.microscope_stitched_remove_dots_eel_graph_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            try: 
                self.stitch_and_remove_dots_eel_graph_step()
            except:
                self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
                pass
            else:
                self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
        else:
            self.logger.error(f&#34;{self.experiment_fpath.stem} missing files with raw counts&#34;)
            raise FileNotFoundError
        
        self.client.close()
        self.cluster.close()

    def test_run_Lars_mouse_atlas(self):
        &#34;&#34;&#34;
        The data were acquired before the rearrangement of the room and 
        the fresh tissue data combined the DAPI/Europium in one single 
        image. 
        TODO: The fresh tissue processing need to be specifically implemented
            for this experiment

        &#34;&#34;&#34;

        start = datetime.now()
        self.run_setup()
        self.run_cluster_activation()
        self.run_parsing()
        self.run_required_steps()   
        self.determine_tiles_organization_before_room_reorganisation() 
        
        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*barcodes_max_array*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)

        # Adjust the dataset because changes in the yaml files
        self.data.dataset.loc[self.data.dataset.channel == &#39;Cy5&#39;,&#39;processing_type&#39;] = &#39;fish&#39;
        self.data.dataset.loc[self.data.dataset.channel == &#39;Europium&#39;,&#39;processing_type&#39;] = &#39;both-beads&#39;


        if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
            step_start = datetime.now()
            self.processing_barcoded_eel_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            self.QC_registration_error_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
            
            step_start = datetime.now()
            self.microscope_stitched_remove_dots_eel_graph_step()
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

            
            step_start = datetime.now()
            try: 
                self.stitch_and_remove_dots_eel_graph_step()
            except:
                self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
                pass
            else:
                self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        else:
            self.logger.error(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)
            sys.exit(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)


        step_start = datetime.now()
        # self.transfer_data_after_processing()
        # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
        #             data transfer after processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                    Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

        
        self.client.close()
        self.cluster.close()
        if self.processing_engine == &#39;unmanaged_cluster&#39;:
            processing_cluster_setup.kill_process()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysmFISH.pipeline.Pipeline.QC_check_experiment_yaml_file_step"><code class="name flex">
<span>def <span class="ident">QC_check_experiment_yaml_file_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>This QC function check in the fields required in the ExperimentName_config.yaml file
are present and have the expected values. This file is important for the parsing of the
data.
The following keys are required:
'Stitching_type', 'Experiment_type', 'Barcode_length', 'Barcode', 'Codebook', 'Machine', 'Operator',
'Overlapping_percentage','Probes_FASTA_name','Species','Start_date','Strain','Tissue','Pipeline'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def QC_check_experiment_yaml_file_step(self):
    &#34;&#34;&#34;
        This QC function check in the fields required in the ExperimentName_config.yaml file
        are present and have the expected values. This file is important for the parsing of the
        data.
        The following keys are required:
        &#39;Stitching_type&#39;, &#39;Experiment_type&#39;, &#39;Barcode_length&#39;, &#39;Barcode&#39;, &#39;Codebook&#39;, &#39;Machine&#39;, &#39;Operator&#39;,
        &#39;Overlapping_percentage&#39;,&#39;Probes_FASTA_name&#39;,&#39;Species&#39;,&#39;Start_date&#39;,&#39;Strain&#39;,&#39;Tissue&#39;,&#39;Pipeline&#39;
    &#34;&#34;&#34;

    qc_utils.QC_check_experiment_yaml_file(self.experiment_fpath)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.QC_registration_error_step"><code class="name flex">
<span>def <span class="ident">QC_registration_error_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Visualise the error in the registration. It plots the error for each fov and the number of matching
beads identified after the registration. The FOV number, the round number with the lowest number
of matching beads and the lowest number of matching beads.</p>
<p>The following attributes created by another step must be accessible:
- analysis_parameters
- tile_corners_coords_pxl
- client</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def QC_registration_error_step(self):
    &#34;&#34;&#34;
        Visualise the error in the registration. It plots the error for each fov and the number of matching
        beads identified after the registration. The FOV number, the round number with the lowest number
        of matching beads and the lowest number of matching beads.

        The following attributes created by another step must be accessible:
        - analysis_parameters
        - tile_corners_coords_pxl
        - client

    &#34;&#34;&#34;
    assert self.client, self.logger.error(f&#39;cannot run QC on registration because missing client attr&#39;)
    assert self.analysis_parameters, self.logger.error(f&#39;cannot run QC on registration because missing analysis_parameters attr&#39;)
    assert self.metadata, self.logger.error(f&#39;cannot process smFISH fovs because missing metadata attr&#39;)
    assert isinstance(self.tile_corners_coords_pxl, np.ndarray), self.logger.error(f&#39;cannot run QC on registration because missing tile_corners_coords_pxl attr&#39;)
    
    qc_reg = qc_utils.QC_registration_error(self.client, self.experiment_fpath, 
                self.analysis_parameters, self.metadata, self.tile_corners_coords_pxl)

    qc_reg.run_qc()</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.create_analysis_config_file_from_dataset_step"><code class="name flex">
<span>def <span class="ident">create_analysis_config_file_from_dataset_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Load or create the yaml file with all the parameters for running the analysis. It will first
load the analysis_config.yaml file present in the pipeline_config folder. If not it
will create one using the master template stored in the config_db directory</p>
<p>The following attributes created by another step must be accessible:
- metadata</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_analysis_config_file_from_dataset_step(self):
    &#34;&#34;&#34;
        Load or create the yaml file with all the parameters for running the analysis. It will first
        load the analysis_config.yaml file present in the pipeline_config folder. If not it 
        will create one using the master template stored in the config_db directory

        The following attributes created by another step must be accessible:
        - metadata

    &#34;&#34;&#34;
    assert self.metadata, self.logger.error(f&#39;cannot load/create the analysis config because missing metadata attr&#39;)
    self.analysis_parameters = configuration_files.create_analysis_config_file_from_dataset(self.experiment_fpath, self.metadata)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.create_folders_step"><code class="name flex">
<span>def <span class="ident">create_folders_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create the folder structure used for the data processing. If the folder
is already present it won't overwrite it</p>
<p>Folder structure
- codebook: contain the codebooks used for the analysis
- original_robofish_logs: contains all the original robofish logs.
- extra_files: contains the extra files acquired during imaging.
- extra_processing_data: contains extra files used in the analysis
like the dark images for flat field correction.
- pipeline_config: contains all the configuration files.
- raw_data: contains the renamed .nd2 files and the corresponding .pkl metadata
files
- output_figures: contains the reports and visualizations
- notebooks: contain potential notebooks used for processing the data
- probes: contains the fasta file with the probes used in the experiment
- fresh_tissue: contain the images and the process data obtained from
imaging the fresh tissue before eel processing
- logs: contains the dask and htcondor logs
- microscope_tiles_coords: contain the coords of the FOVs according to the
microscope stage.
- results: contains all the processing results</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_folders_step(self):
    &#34;&#34;&#34;
        Create the folder structure used for the data processing. If the folder
        is already present it won&#39;t overwrite it

        Folder structure
            - codebook: contain the codebooks used for the analysis
            - original_robofish_logs: contains all the original robofish logs.
            - extra_files: contains the extra files acquired during imaging.
            - extra_processing_data: contains extra files used in the analysis
                like the dark images for flat field correction.
            - pipeline_config: contains all the configuration files.
            - raw_data: contains the renamed .nd2 files and the corresponding .pkl metadata 
                                    files
            - output_figures: contains the reports and visualizations
            - notebooks: contain potential notebooks used for processing the data
            - probes: contains the fasta file with the probes used in the experiment
            - fresh_tissue: contain the images and the process data obtained from 
                                                    imaging the fresh tissue before eel processing
            - logs: contains the dask and htcondor logs
            - microscope_tiles_coords: contain the coords of the FOVs according to the 
                                                                                            microscope stage.
            - results: contains all the processing results            
    &#34;&#34;&#34;

    utils.create_folder_structure(self.experiment_fpath, self.run_type)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.create_running_functions_step"><code class="name flex">
<span>def <span class="ident">create_running_functions_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create the dictionary with the function names used to run a specific pipeline as defined
in metadata['pipeline']</p>
<p>The following attributes created by another step must be accessible:
- metadata</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_running_functions_step(self):
    &#34;&#34;&#34;
        Create the dictionary with the function names used to run a specific pipeline as defined
        in metadata[&#39;pipeline&#39;]

        The following attributes created by another step must be accessible:
        - metadata

    &#34;&#34;&#34;
    assert self.metadata, self.logger.error(f&#39;cannot create running functions because missing metadata attr&#39;)
    self.running_functions = configuration_files.create_function_runner(self.experiment_fpath,self.metadata)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.determine_tiles_organization"><code class="name flex">
<span>def <span class="ident">determine_tiles_organization</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine the organization of the field of views in the dataset</p>
<p>The following attributes created by another step must be accessible:
- analysis_parameters
- dataset
- metadata</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_tiles_organization(self):
    &#34;&#34;&#34;
        Determine the organization of the field of views in the dataset

        The following attributes created by another step must be accessible:
        - analysis_parameters
        - dataset
        - metadata

    &#34;&#34;&#34;
    assert self.metadata, self.logger.error(f&#39;cannot determine tiles organization because missing metadata attr&#39;)
    assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot determine tiles organization because missing dataset attr&#39;)
    assert self.analysis_parameters, self.logger.error(f&#39;cannot determine tiles organization because missing analysis_parameters attr&#39;)
    self.reference_round = self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    self.tiles_org = stitching.organize_square_tiles(self.experiment_fpath,
                                self.data.dataset,self.metadata,
                                self.reference_round)
    self.tiles_org.run_tiles_organization()
    self.tile_corners_coords_pxl = self.tiles_org.tile_corners_coords_pxl</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.determine_tiles_organization_before_room_reorganisation"><code class="name flex">
<span>def <span class="ident">determine_tiles_organization_before_room_reorganisation</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine the organization of the field of views in the dataset</p>
<p>The following attributes created by another step must be accessible:
- analysis_parameters
- dataset
- metadata</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_tiles_organization_before_room_reorganisation(self):
    &#34;&#34;&#34;
        Determine the organization of the field of views in the dataset

        The following attributes created by another step must be accessible:
        - analysis_parameters
        - dataset
        - metadata

    &#34;&#34;&#34;
    assert self.metadata, self.logger.error(f&#39;cannot determine tiles organization because missing metadata attr&#39;)
    assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot determine tiles organization because missing dataset attr&#39;)
    assert self.analysis_parameters, self.logger.error(f&#39;cannot determine tiles organization because missing analysis_parameters attr&#39;)
    self.reference_round = self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    self.tiles_org = stitching.organize_square_tiles_old_room(self.experiment_fpath,
                                self.data.dataset,self.metadata,
                                self.reference_round)
    self.tiles_org.run_tiles_organization()
    self.tile_corners_coords_pxl = self.tiles_org.tile_corners_coords_pxl</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.microscope_stitched_remove_dots_eel_graph_step"><code class="name flex">
<span>def <span class="ident">microscope_stitched_remove_dots_eel_graph_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove the duplicated
barcodes present in the overlapping regions of the tiles after stitching
using the microscope coords</p>
<h2 id="args">Args:</h2>
<p>hamming_distance (int): Value to select the barcodes that are passing the
screening (&lt; hamming_distance). Default = 3
same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
Default = 10
stitching_selected (str): barcodes coords set where the duplicated dots will be
removed</p>
<p>The following attributes created by another step must be accessible:
- dataset
- tiles_org
- client</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def microscope_stitched_remove_dots_eel_graph_step(self):
    &#34;&#34;&#34;
        Function to remove the duplicated 
        barcodes present in the overlapping regions of the tiles after stitching
        using the microscope coords

        Args:
        ----
        hamming_distance (int): Value to select the barcodes that are passing the 
            screening (&lt; hamming_distance). Default = 3
        same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
            Default = 10
        stitching_selected (str): barcodes coords set where the duplicated dots will be
            removed

        The following attributes created by another step must be accessible:
        - dataset
        - tiles_org
        - client

    &#34;&#34;&#34;
    assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
    assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
    
    assert (isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles) | 
            isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles_old_room)), \
                        self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
    
    
    # Removed the dots on the microscope stitched
    self.stitching_selected = &#39;microscope_stitched&#39;

    stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                            self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                self.stitching_selected, self.client)

    # ----------------------------------------------------------------
    # GENERATE OUTPUT FOR PLOTTING
    selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
    stitching_selected = &#39;microscope_stitched&#39;
    io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                            selected_Hdistance, self.client,
                            input_file_tag = &#39;microscope_stitched_cleaned&#39;,
                            file_tag=&#39;cleaned_microscope_stitched&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.nikon_nd2_parsing_graph_step"><code class="name flex">
<span>def <span class="ident">nikon_nd2_parsing_graph_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the parsing according to what is specified by the parsing_type
argument.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nikon_nd2_parsing_graph_step(self):
    &#34;&#34;&#34;
        Run the parsing according to what is specified by the parsing_type
        argument.

    &#34;&#34;&#34;
    microscopy_file_parsers.nikon_nd2_parsing_graph(self.experiment_fpath,
                                self.parsing_type,self.parsed_image_tag, 
                                self.storage_experiment_fpath,
                                self.client)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.prepare_processing_dataset_step"><code class="name flex">
<span>def <span class="ident">prepare_processing_dataset_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>If a path to an existing dataset is entered it will be loaded otherwise
it will create a new dataset that will have all the info that characterize the
experiment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>zarr_file_path</code></strong> :&ensp;<code>Path</code></dt>
<dd>Path of the file used to build the dataset</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_processing_dataset_step(self):
    &#34;&#34;&#34;
        If a path to an existing dataset is entered it will be loaded otherwise
        it will create a new dataset that will have all the info that characterize the
        experiment.

        Args:
            zarr_file_path (Path): Path of the file used to build the dataset

    &#34;&#34;&#34;
    self.data = data_models.Dataset()
    if self.dataset_path:
        try:
            self.data.load_dataset(self.dataset_path)
        except:
            self.logger.error(f&#34;can&#39;t load the dataset from {self.dataset_path}&#34;)
            sys.exit(f&#34;can&#39;t load the dataset from {self.dataset_path}&#34;)
    else:
        try:
            self.data.create_full_dataset_from_zmetadata(self.parsed_raw_data_fpath)
        except:
            self.logger.error(f&#34;can&#39;t create dataset from {self.parsed_raw_data_fpath}&#34;)
            sys.exit(f&#34;can&#39;t create dataset from {self.parsed_raw_data_fpath}&#34;)
    
    self.metadata = self.data.collect_metadata(self.data.dataset)
    self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.processing_barcoded_eel_step"><code class="name flex">
<span>def <span class="ident">processing_barcoded_eel_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create and run a dask delayed task graph used to process barcoded eel experiments
It runs:
(1) Image preprocessing
(2) Dot calling
(3) Field of view registration
(4) Barcode decoding
(5) Registration to the microscope coords
(6) Consolidate the processed images zarr file metadata
(7) Create a simple output for quick visualization</p>
<p>The following attributes created by another step must be accessible:
- metadata
- analysis_parameters
- running_functions
- grpd_fovs
- client</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def processing_barcoded_eel_step(self):
    &#34;&#34;&#34;
        Create and run a dask delayed task graph used to process barcoded eel experiments
        It runs:
        (1) Image preprocessing
        (2) Dot calling
        (3) Field of view registration
        (4) Barcode decoding
        (5) Registration to the microscope coords
        (6) Consolidate the processed images zarr file metadata
        (7) Create a simple output for quick visualization
        
        The following attributes created by another step must be accessible:
        - metadata
        - analysis_parameters
        - running_functions
        - grpd_fovs
        - client

    &#34;&#34;&#34;
    assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
    assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
    assert self.running_functions, self.logger.error(f&#39;cannot process eel fovs because missing running_functions attr&#39;)
    assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
    assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
    assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)

    fov_processing.processing_barcoded_eel_fov_graph(self.experiment_fpath,self.analysis_parameters,
                                self.running_functions, self.tiles_org,self.metadata,
                                self.grpd_fovs,self.save_intermediate_steps, 
                                self.preprocessed_image_tag,self.client,self.chunk_size,self.save_bits_int,
                                self.start_from_preprocessed_imgs)

    # ----------------------------------------------------------------
    # GENERATE OUTPUT FOR PLOTTING
    selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
    stitching_selected = &#39;microscope_stitched&#39;
    io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                            selected_Hdistance, self.client,
                            input_file_tag = &#39;decoded_fov&#39;,
                            file_tag=&#39;microscope_stitched&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.processing_cluster_init_step"><code class="name flex">
<span>def <span class="ident">processing_cluster_init_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create new cluster and client or reuse a cluster and client previously created
Can connect direclty to the client/cluster using the 'connect_to_client' flag or
to the scheduler with the 'connect_to_scheduler'.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def processing_cluster_init_step(self):
    &#34;&#34;&#34;Create new cluster and client or reuse a cluster and client previously created
       Can connect direclty to the client/cluster using the &#39;connect_to_client&#39; flag or 
       to the scheduler with the &#39;connect_to_scheduler&#39;.
    &#34;&#34;&#34;

    # Start processing environment
    if self.reuse_cluster == &#39;connect_to_client&#39;:
        self.cluster = self.active_cluster
        self.client = self.active_client
        self.client.run(gc.collect)
        self.client.run(utils.trim_memory())
    elif self.reuse_cluster == &#39;connect_to_scheduler&#39;:
        self.client = Client(self.active_scheduler_address)
        self.client.run(gc.collect)
        self.client.run(utils.trim_memory())
    else:
        self.cluster = processing_cluster_setup.start_processing_env(self.processing_env_config)
        self.client = Client(self.cluster,asynchronous=True)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.processing_fresh_tissue_step"><code class="name flex">
<span>def <span class="ident">processing_fresh_tissue_step</span></span>(<span>self, parsing=True, tag_ref_beads='_ChannelEuropium_Cy3_', tag_nuclei='_ChannelCy3_')</span>
</code></dt>
<dd>
<div class="desc"><p>This function create and run a processing graph that parse and filter the nuclei staining in fresh tissue
and parse, filter and counts the Europium beads used for the registration with the smFISH images at high
power.</p>
<h2 id="args">Args:</h2>
<p>tag_ref_beads (str): The tag reference of the .nd2 file containing the raw beads images. Default: '<em>ChannelEuropium_Cy3</em>'
tag_ref_nuclei (str): The tag reference of the .nd2 file containing the raw images of nuclei. Default: '<em>ChannelCy3</em>'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def processing_fresh_tissue_step(self,parsing=True,
                                tag_ref_beads=&#39;_ChannelEuropium_Cy3_&#39;,
                                tag_nuclei=&#39;_ChannelCy3_&#39;):
    &#34;&#34;&#34;
        This function create and run a processing graph that parse and filter the nuclei staining in fresh tissue
        and parse, filter and counts the Europium beads used for the registration with the smFISH images at high
        power.

        Args:
        ----
        tag_ref_beads (str): The tag reference of the .nd2 file containing the raw beads images. Default: &#39;_ChannelEuropium_Cy3_&#39;
        tag_ref_nuclei (str): The tag reference of the .nd2 file containing the raw images of nuclei. Default: &#39;_ChannelCy3_&#39;

    &#34;&#34;&#34;
    assert self.client, self.logger.error(f&#39;cannot process fresh tissue because missing client attr&#39;)
    assert self.analysis_parameters, self.logger.error(f&#39;cannot process fresh tissue because missing analysis_parameters attr&#39;)
    assert self.running_functions, self.logger.error(f&#39;cannot process fresh tissue because missing running_functions attr&#39;)
    fov_processing.process_fresh_sample_graph(self.experiment_fpath,self.running_functions,
                                            self.analysis_parameters, self.client,
                                            self.chunk_size,
                                            tag_ref_beads= tag_ref_beads,
                                            tag_nuclei= tag_nuclei,
                                            eel_metadata= self.metadata,
                                            parsing= parsing,
                                            save_steps_output=self.save_intermediate_steps)


    stitching.stitched_beads_on_nuclei_fresh_tissue(self.experiment_fpath,
                                  self.client,
                                  nuclei_tag=tag_nuclei,
                                  beads_tag = tag_ref_beads,
                                  round_num = 1,
                                  overlapping_percentage=5,
                                  machine=self.metadata[&#39;machine&#39;])</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.processing_serial_fish_step"><code class="name flex">
<span>def <span class="ident">processing_serial_fish_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create and run a dask delayed task graph used to process serial smFISH experiments
It runs:
(1) Image preprocessing
(2) Dot calling
(3) Field of view registration
(4) Registration to the microscope coords
(5) Consolidate the processed images zarr file metadata
(6) Create a simple output for quick visualization</p>
<p>The following attributes created by another step must be accessible:
- metadata
- analysis_parameters
- running_functions
- grpd_fovs
- client</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def processing_serial_fish_step(self):
    &#34;&#34;&#34;
        Create and run a dask delayed task graph used to process serial smFISH experiments
        It runs:
        (1) Image preprocessing
        (2) Dot calling
        (3) Field of view registration
        (4) Registration to the microscope coords
        (5) Consolidate the processed images zarr file metadata
        (6) Create a simple output for quick visualization
        
        The following attributes created by another step must be accessible:
        - metadata
        - analysis_parameters
        - running_functions
        - grpd_fovs
        - client

    &#34;&#34;&#34;
    assert self.metadata, self.logger.error(f&#39;cannot process smFISH fovs because missing metadata attr&#39;)
    assert self.analysis_parameters, self.logger.error(f&#39;cannot process smFISH fovs because missing analysis_parameters attr&#39;)
    assert self.running_functions, self.logger.error(f&#39;cannot process smFISH fovs because missing running_functions attr&#39;)
    assert self.grpd_fovs, self.logger.error(f&#39;cannot process smFISH fovs because missing grpd_fovs attr&#39;)
    assert self.client, self.logger.error(f&#39;cannot process smFISH fovs because missing client attr&#39;)
    assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)

    
    fov_processing.processing_serial_fish_fov_graph(self.experiment_fpath,self.analysis_parameters,
                                self.running_functions, self.tiles_org,self.metadata,
                                self.grpd_fovs,self.save_intermediate_steps, 
                                self.preprocessed_image_tag,self.client,self.chunk_size)

    
    # Removed the dots on the microscope stitched
    self.stitching_selected = &#39;microscope_stitched&#39;

    stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                            self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                self.stitching_selected, self.client)

    # ----------------------------------------------------------------
    # GENERATE OUTPUT FOR PLOTTING
    selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
    stitching_selected = &#39;microscope_stitched&#39;
    io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                            selected_Hdistance, self.client,
                            input_file_tag = &#39;microscope_stitched_cleaned&#39;,
                            file_tag=&#39;cleaned_microscope_stitched&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.rerun_decoding_step"><code class="name flex">
<span>def <span class="ident">rerun_decoding_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create and run a dask delayed task graph used to process barcoded eel experiments
It runs:
(2) Barcode decoding
(3) Registration to the microscope coords</p>
<p>The following attributes created by another step must be accessible:
- metadata
- analysis_parameters
- grpd_fovs
- client</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rerun_decoding_step(self):
    &#34;&#34;&#34;
        Create and run a dask delayed task graph used to process barcoded eel experiments
        It runs:
        (2) Barcode decoding
        (3) Registration to the microscope coords
        
        The following attributes created by another step must be accessible:
        - metadata
        - analysis_parameters
        - grpd_fovs
        - client

    &#34;&#34;&#34;
    assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
    assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
    assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
    assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
    assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)


    fov_processing.processing_barcoded_eel_fov_graph_from_decoding(self.experiment_fpath,self.analysis_parameters,
                                self.tiles_org,self.metadata,
                                self.grpd_fovs, self.client, self.chunk_size)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.rerun_from_registration_step"><code class="name flex">
<span>def <span class="ident">rerun_from_registration_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create and run a dask delayed task graph from the registration step.
Requires the raw_counts files</p>
<p>It runs:
(1) Field of view registration
(2) Barcode decoding
(3) Registration to the microscope coords</p>
<p>The following attributes created by another step must be accessible:
- metadata
- analysis_parameters
- running_functions
- grpd_fovs
- client
- tiles_org</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rerun_from_registration_step(self):
    &#34;&#34;&#34;
        Create and run a dask delayed task graph from the registration step.
        Requires the raw_counts files
        
        It runs:
        (1) Field of view registration
        (2) Barcode decoding
        (3) Registration to the microscope coords
        
        The following attributes created by another step must be accessible:
        - metadata
        - analysis_parameters
        - running_functions
        - grpd_fovs
        - client
        - tiles_org

    &#34;&#34;&#34;

    assert self.metadata, self.logger.error(f&#39;cannot process eel fovs because missing metadata attr&#39;)
    assert self.analysis_parameters, self.logger.error(f&#39;cannot process eel fovs because missing analysis_parameters attr&#39;)
    assert self.running_functions, self.logger.error(f&#39;cannot process eel fovs because missing running_functions attr&#39;)
    assert self.grpd_fovs, self.logger.error(f&#39;cannot process eel fovs because missing grpd_fovs attr&#39;)
    assert self.client, self.logger.error(f&#39;cannot process eel fovs because missing client attr&#39;)
    assert self.tiles_org, self.logger.error(f&#39;cannot process eel fovs because missing tiles organization attr&#39;)
    
    fov_processing.processing_barcoded_eel_fov_starting_from_registration_graph(self.experiment_fpath,
                                self.analysis_parameters,
                                self.running_functions, 
                                self.tiles_org,
                                self.metadata,
                                self.grpd_fovs,
                                self.preprocessed_image_tag, 
                                self.client, 
                                self.chunk_size, 
                                self.save_bits_int)

    # ----------------------------------------------------------------
    # GENERATE OUTPUT FOR PLOTTING
    selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
    stitching_selected = &#39;microscope_stitched&#39;
    io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                            selected_Hdistance, self.client,
                            input_file_tag = &#39;decoded_fov&#39;,
                            file_tag=&#39;microscope_stitched&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.run_cluster_activation"><code class="name flex">
<span>def <span class="ident">run_cluster_activation</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_cluster_activation(self):
    start = datetime.now()
    self.processing_cluster_init_step()
    self.logger.info(f&#39;Started dask processing cluster&#39;)
    self.logger.info(f&#34;client dashboard {self.client.scheduler_info()[&#39;services&#39;][&#39;dashboard&#39;]}&#34;)
    self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
            Cluester activation completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.run_full"><code class="name flex">
<span>def <span class="ident">run_full</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Full run from raw images from nikon or parsed images</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_full(self):
    &#34;&#34;&#34;
        Full run from raw images from nikon or parsed images
    &#34;&#34;&#34;

    start = datetime.now()
    self.run_setup()
    self.run_cluster_activation()
    self.run_parsing()
    self.run_required_steps()    
    
    if self.resume:
        already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*barcodes_max_array*.parquet&#39;)
        already_done_fovs = []
        for fname in already_processed:
            fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
            already_done_fovs.append(fov_num)
        not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
        self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
        self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


    if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
        step_start = datetime.now()
        self.processing_barcoded_eel_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        
        step_start = datetime.now()
        self.QC_registration_error_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        
        step_start = datetime.now()
        self.microscope_stitched_remove_dots_eel_graph_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        
        step_start = datetime.now()
        try: 
            self.stitch_and_remove_dots_eel_graph_step()
        except:
            self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
            pass
        else:
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        step_start = datetime.now()
        self.processing_fresh_tissue_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Processing fresh tissue completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

    elif self.metadata[&#39;experiment_type&#39;] == &#39;smfish-serial&#39;:
        step_start = datetime.now()
        self.processing_serial_fish_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                serial smfish fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
    else:
        self.logger.error(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)
        sys.exit(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)


    step_start = datetime.now()
    # self.transfer_data_after_processing()
    # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
    #             data transfer after processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

    self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

    
    self.client.close()
    self.cluster.close()
    if self.processing_engine == &#39;unmanaged_cluster&#39;:
        processing_cluster_setup.kill_process()</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.run_parsing"><code class="name flex">
<span>def <span class="ident">run_parsing</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_parsing(self):
    start = datetime.now()
    # Run parsing only if required
    self.logger.info(f&#39;Parsing started&#39;)
    if self.parsing_type != &#39;no_parsing&#39;:
        self.nikon_nd2_parsing_graph_step()
    self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
            Parsing completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.run_parsing_only"><code class="name flex">
<span>def <span class="ident">run_parsing_only</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Pipeline running the data organization and the parsing
of the .nd2 files of the entire experiment</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_parsing_only(self):
    &#34;&#34;&#34;
        Pipeline running the data organization and the parsing
        of the .nd2 files of the entire experiment
    &#34;&#34;&#34;

    start = datetime.now()

    self.create_folders_step()

    self.logger = logger_utils.json_logger((self.experiment_fpath / &#39;logs&#39;),&#39;pipeline_run&#39;) 
    self.logger.info(f&#34;Start parsing&#34;)

    self.save_git_commit()
    self.logger.info(f&#39;Saved current git commit version&#39;)

    self.QC_check_experiment_yaml_file_step()
    self.logger.info(f&#39;Checked config file&#39;)

    self.processing_cluster_init_step()
    self.logger.info(f&#39;Started dask processing cluster&#39;)
    self.logger.info(f&#34;client dashboard {self.client.scheduler_info()[&#39;services&#39;][&#39;dashboard&#39;]}&#34;)

    # Run parsing only if required
    self.logger.info(f&#39;Parsing started&#39;)
    if self.parsing_type != &#39;no_parsing&#39;:
        self.nikon_nd2_parsing_graph_step()
    self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
            Parsing completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
    
    step_start = datetime.now()
    self.logger.info(f&#39;Started creation of the dataset&#39;)
    self.prepare_processing_dataset_step()
    self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
            Dataset creation completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.run_required_steps"><code class="name flex">
<span>def <span class="ident">run_required_steps</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Short pipeline used to make sure that the basic required
step are run and will be included in more complex pipelines</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_required_steps(self):
    &#34;&#34;&#34;
        Short pipeline used to make sure that the basic required
        step are run and will be included in more complex pipelines

    &#34;&#34;&#34;
    start = datetime.now()
    step_start = datetime.now()
    self.logger.info(f&#39;Started creation of the dataset&#39;)
    self.prepare_processing_dataset_step()
    self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
            Dataset creation completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
    self.create_analysis_config_file_from_dataset_step()
    self.logger.info(f&#39;Created analysis_config.yaml file&#39;)
    self.determine_tiles_organization()
    self.logger.info(f&#39;Determined the tiles organization&#39;)
    self.create_running_functions_step()
    self.logger.info(f&#39;Created the running function dictionary&#39;)
    self.logger.info(f&#34;{self.experiment_fpath.stem} timing:\
            Required steps completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
    self.logger.info(f&#34;&#34;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.run_setup"><code class="name flex">
<span>def <span class="ident">run_setup</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_setup(self):
    start = datetime.now()

    self.create_folders_step()

    self.logger = logger_utils.json_logger((self.experiment_fpath / &#39;logs&#39;),&#39;pipeline_run&#39;) 
    self.logger.info(f&#34;Start parsing&#34;)

    self.save_git_commit()
    self.logger.info(f&#39;Saved current git commit version&#39;)

    if self.run_type == &#39;original&#39;:
        self.QC_check_experiment_yaml_file_step()
        self.logger.info(f&#39;Checked config file&#39;)
    
    self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
            Setup completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.save_git_commit"><code class="name flex">
<span>def <span class="ident">save_git_commit</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_git_commit(self):
    hash_str =  utils.get_git_hash()
    processing_info = {}
    processing_info[&#39;git_commit_hash&#39;] = hash_str
    processing_info_fpath = self.experiment_fpath / &#39;results&#39; /&#39;git_info.yaml&#39;
    with open(processing_info_fpath, &#39;w&#39;) as new_config:
                yaml.safe_dump(processing_info, new_config,default_flow_style=False,sort_keys=False)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.stitch_and_remove_dots_eel_graph_old_room_step"><code class="name flex">
<span>def <span class="ident">stitch_and_remove_dots_eel_graph_old_room_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to stitch the different fovs and remove the duplicated
barcodes present in the overlapping regions of the tiles</p>
<h2 id="args">Args:</h2>
<p>hamming_distance (int): Value to select the barcodes that are passing the
screening (&lt; hamming_distance). Default = 3
same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
Default = 10
stitching_selected (str): barcodes coords set where the duplicated dots will be
removed</p>
<p>The following attributes created by another step must be accessible:
- dataset
- tiles_org
- client</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stitch_and_remove_dots_eel_graph_old_room_step(self):

    &#34;&#34;&#34;
        Function to stitch the different fovs and remove the duplicated 
        barcodes present in the overlapping regions of the tiles

        Args:
        ----
        hamming_distance (int): Value to select the barcodes that are passing the 
            screening (&lt; hamming_distance). Default = 3
        same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
            Default = 10
        stitching_selected (str): barcodes coords set where the duplicated dots will be
            removed

        The following attributes created by another step must be accessible:
        - dataset
        - tiles_org
        - client

    &#34;&#34;&#34;
    assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
    assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
    assert isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles), \
                        self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
    
    
    self.adjusted_coords = stitching.stitching_graph(self.experiment_fpath,self.metadata[&#39;stitching_channel&#39;],
                                                                self.tiles_org, self.metadata,
                                                                self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;], 
                                                                self.client)
    
    
    # Recalculate the overlapping regions after stitching
    self.tiles_org.tile_corners_coords_pxl = self.adjusted_coords
    self.tiles_org.determine_overlapping_regions()
    
    # Removed the dots on the global stitched
    self.stitching_selected = &#39;global_stitched&#39;
    stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                            self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                self.stitching_selected, self.client)

    # ----------------------------------------------------------------
    # GENERATE OUTPUT FOR PLOTTING
    selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
    stitching_selected = &#39;global_stitched&#39;
    io.simple_output_plotting(self.experiment_fpath, stitching_selected,
                            selected_Hdistance, self.client,
                            input_file_tag = &#39;global_stitched_cleaned&#39;,
                            file_tag=&#39;cleaned_global_stitched&#39;)
    # ----------------------------------------------------------------  

    # ----------------------------------------------------------------
    # GENERATE OUTPUT FOR PLOTTING
    selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
    stitching_selected = &#39;global_stitched&#39;
    io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                            selected_Hdistance, self.client,
                            input_file_tag = &#39;global_stitched_removed&#39;,
                            file_tag=&#39;removed_global_stitched&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.stitch_and_remove_dots_eel_graph_step"><code class="name flex">
<span>def <span class="ident">stitch_and_remove_dots_eel_graph_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to stitch the different fovs and remove the duplicated
barcodes present in the overlapping regions of the tiles</p>
<h2 id="args">Args:</h2>
<p>hamming_distance (int): Value to select the barcodes that are passing the
screening (&lt; hamming_distance). Default = 3
same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
Default = 10
stitching_selected (str): barcodes coords set where the duplicated dots will be
removed</p>
<p>The following attributes created by another step must be accessible:
- dataset
- tiles_org
- client</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stitch_and_remove_dots_eel_graph_step(self):

    &#34;&#34;&#34;
        Function to stitch the different fovs and remove the duplicated 
        barcodes present in the overlapping regions of the tiles

        Args:
        ----
        hamming_distance (int): Value to select the barcodes that are passing the 
            screening (&lt; hamming_distance). Default = 3
        same_dot_radius_duplicate_dots (int): Searching distance that define two dots as identical
            Default = 10
        stitching_selected (str): barcodes coords set where the duplicated dots will be
            removed

        The following attributes created by another step must be accessible:
        - dataset
        - tiles_org
        - client

    &#34;&#34;&#34;
    assert self.client, self.logger.error(f&#39;cannot remove duplicated dots because missing client attr&#39;)
    assert isinstance(self.data.dataset, pd.DataFrame), self.logger.error(f&#39;cannot remove duplicated dots because missing dataset attr&#39;)
    assert isinstance(self.tiles_org,pysmFISH.stitching.organize_square_tiles), \
                        self.logger.error(f&#39;cannot remove duplicated dots because tiles_org is missing attr&#39;)
    
    
    self.adjusted_coords = stitching.stitching_graph_serial_nuclei(self.experiment_fpath,self.metadata[&#39;stitching_channel&#39;],
                                                                self.tiles_org, self.metadata,
                                                                self.analysis_parameters[&#39;RegistrationReferenceHybridization&#39;], 
                                                                self.client)
    
    
    # Recalculate the overlapping regions after stitching
    self.tiles_org.tile_corners_coords_pxl = self.adjusted_coords
    self.tiles_org.determine_overlapping_regions()
    
    # Removed the dots on the global stitched
    self.stitching_selected = &#39;global_stitched&#39;
    stitching.remove_duplicated_dots_graph(self.experiment_fpath,self.data.dataset,self.tiles_org,
                            self.hamming_distance,self.same_dot_radius_duplicate_dots, 
                                self.stitching_selected, self.client)

    # ----------------------------------------------------------------
    # GENERATE OUTPUT FOR PLOTTING
    selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
    stitching_selected = &#39;global_stitched&#39;
    io.simple_output_plotting(self.experiment_fpath, stitching_selected,
                            selected_Hdistance, self.client,
                            input_file_tag = &#39;global_stitched_cleaned&#39;,
                            file_tag=&#39;cleaned_global_stitched&#39;)
    # ----------------------------------------------------------------  

    # ----------------------------------------------------------------
    # GENERATE OUTPUT FOR PLOTTING
    selected_Hdistance = 3 / self.metadata[&#39;barcode_length&#39;]
    stitching_selected = &#39;global_stitched&#39;
    io.simple_output_plotting(self.experiment_fpath, stitching_selected, 
                            selected_Hdistance, self.client,
                            input_file_tag = &#39;global_stitched_removed&#39;,
                            file_tag=&#39;removed_global_stitched&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.test_run_Lars_mouse_atlas"><code class="name flex">
<span>def <span class="ident">test_run_Lars_mouse_atlas</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>The data were acquired before the rearrangement of the room and
the fresh tissue data combined the DAPI/Europium in one single
image.
TODO: The fresh tissue processing need to be specifically implemented
for this experiment</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_run_Lars_mouse_atlas(self):
    &#34;&#34;&#34;
    The data were acquired before the rearrangement of the room and 
    the fresh tissue data combined the DAPI/Europium in one single 
    image. 
    TODO: The fresh tissue processing need to be specifically implemented
        for this experiment

    &#34;&#34;&#34;

    start = datetime.now()
    self.run_setup()
    self.run_cluster_activation()
    self.run_parsing()
    self.run_required_steps()   
    self.determine_tiles_organization_before_room_reorganisation() 
    
    if self.resume:
        already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*barcodes_max_array*.parquet&#39;)
        already_done_fovs = []
        for fname in already_processed:
            fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
            already_done_fovs.append(fov_num)
        not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
        self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
        self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)

    # Adjust the dataset because changes in the yaml files
    self.data.dataset.loc[self.data.dataset.channel == &#39;Cy5&#39;,&#39;processing_type&#39;] = &#39;fish&#39;
    self.data.dataset.loc[self.data.dataset.channel == &#39;Europium&#39;,&#39;processing_type&#39;] = &#39;both-beads&#39;


    if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
        step_start = datetime.now()
        self.processing_barcoded_eel_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        
        step_start = datetime.now()
        self.QC_registration_error_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        
        step_start = datetime.now()
        self.microscope_stitched_remove_dots_eel_graph_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        
        step_start = datetime.now()
        try: 
            self.stitch_and_remove_dots_eel_graph_step()
        except:
            self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
            pass
        else:
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

    else:
        self.logger.error(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)
        sys.exit(f&#34;the experiment type {self.metadata[&#39;experiment_type&#39;]} is unknown&#34;)


    step_start = datetime.now()
    # self.transfer_data_after_processing()
    # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
    #             data transfer after processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

    self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)

    
    self.client.close()
    self.cluster.close()
    if self.processing_engine == &#39;unmanaged_cluster&#39;:
        processing_cluster_setup.kill_process()</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.test_run_after_editing"><code class="name flex">
<span>def <span class="ident">test_run_after_editing</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Full run from raw images from nikon or parsed images</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_run_after_editing(self):
    &#34;&#34;&#34;
        Full run from raw images from nikon or parsed images
    &#34;&#34;&#34;

    start = datetime.now()    
    if self.resume:
        already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
        already_done_fovs = []
        for fname in already_processed:
            fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
            already_done_fovs.append(fov_num)
        not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
        self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
        self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


    if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
        step_start = datetime.now()
        self.processing_barcoded_eel_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        # step_start = datetime.now()
        # self.QC_registration_error_step()
        # self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
        #         QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

    self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.test_run_decoding"><code class="name flex">
<span>def <span class="ident">test_run_decoding</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Run analysis starting from the raw data files.
Requires raw files</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_run_decoding(self):
    &#34;&#34;&#34;
        Run analysis starting from the raw data files.
        Requires raw files 
    &#34;&#34;&#34;

    raw_files_path = list((self.experiment_fpath / &#39;results&#39;).glob(&#39;*_decoded_fov_*&#39;))

    start = datetime.now()
    self.run_parsing_only()
    self.run_required_steps()
    if raw_files_path:
        
        
        step_start = datetime.now()  
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        step_start = datetime.now()
        self.rerun_decoding_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
    else:
        self.logger.error(f&#34;{self.experiment_fpath.stem} missing files with raw counts&#34;)
        raise FileNotFoundError
    
    self.client.close()
    self.cluster.close()</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.test_run_from_registration"><code class="name flex">
<span>def <span class="ident">test_run_from_registration</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Run analysis starting from the raw data files.
Requires raw files</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_run_from_registration(self):
    &#34;&#34;&#34;
        Run analysis starting from the raw data files.
        Requires raw files 
    &#34;&#34;&#34;

    raw_files_path = list((self.experiment_fpath / &#39;results&#39;).glob(&#39;*_raw_counts_*&#39;))
    start = datetime.now()
    # self.run_parsing_only()
    # self.run_required_steps()
    
    if raw_files_path:

        if self.resume:
            already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
            already_done_fovs = []
            for fname in already_processed:
                fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
                already_done_fovs.append(fov_num)
            not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
            self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
            self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)
        
        step_start = datetime.now()  
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        step_start = datetime.now()
        self.rerun_from_registration_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                eel fov processing from dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        step_start = datetime.now()
        self.QC_registration_error_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        step_start = datetime.now()
        self.microscope_stitched_remove_dots_eel_graph_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                removal overlapping dots in microscope stitched {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)
        
        step_start = datetime.now()
        try: 
            self.stitch_and_remove_dots_eel_graph_step()
        except:
            self.logger.info(f&#34;Stitching using dots didn&#39;t work&#34;)
            pass
        else:
            self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Stitching and removal of duplicated dots completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
    else:
        self.logger.error(f&#34;{self.experiment_fpath.stem} missing files with raw counts&#34;)
        raise FileNotFoundError
    
    self.client.close()
    self.cluster.close()</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.test_run_short"><code class="name flex">
<span>def <span class="ident">test_run_short</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_run_short(self):
    start = datetime.now()
    self.run_parsing_only()
    self.run_required_steps()
    if self.resume:
        already_processed = (Path(self.experiment_fpath) / &#39;results&#39;).glob(&#39;*decoded*.parquet&#39;)
        already_done_fovs = []
        for fname in already_processed:
            fov_num = int(fname.stem.split(&#39;_&#39;)[-1])
            already_done_fovs.append(fov_num)
        not_processed_fovs = set(self.grpd_fovs.groups.keys()).difference(set(already_done_fovs))
        self.data.dataset = self.data.dataset.loc[self.data.dataset.fov_num.isin(not_processed_fovs), :]
        self.grpd_fovs = self.data.dataset.groupby(&#39;fov_num&#39;)


    if self.metadata[&#39;experiment_type&#39;] == &#39;eel-barcoded&#39;:
        step_start = datetime.now()
        self.processing_barcoded_eel_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                eel fov processing completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

        step_start = datetime.now()
        self.QC_registration_error_step()
        self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                QC registration completed in {utils.nice_deltastring(datetime.now() - step_start)}.&#34;)

    self.logger.info(f&#34;{self.experiment_fpath.stem} timing: \
                Pipeline run completed in {utils.nice_deltastring(datetime.now() - start)}.&#34;)
    
    self.client.close()
    self.cluster.close()</code></pre>
</details>
</dd>
<dt id="pysmFISH.pipeline.Pipeline.transfer_data_after_processing"><code class="name flex">
<span>def <span class="ident">transfer_data_after_processing</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Function use to clear space in the processing folder.
- Tranfer parsed images zarr / filtered images zarr / dataset
- Remove the log and tmp folders
- Transfer the remaining data to cold storage
Before transfering the data it is necessary to close the cluster and the client
otherwise you won't be able to remove the logs folder.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transfer_data_after_processing(self):
    &#34;&#34;&#34;
        Function use to clear space in the processing folder. 
        - Tranfer parsed images zarr / filtered images zarr / dataset
        - Remove the log and tmp folders
        - Transfer the remaining data to cold storage
        Before transfering the data it is necessary to close the cluster and the client
        otherwise you won&#39;t be able to remove the logs folder.
    &#34;&#34;&#34;

    data_organization.reorganize_processing_dir(self.experiment_fpath,
                        self.storage_fpath,
                        self.store_dataset,
                        self.dataset_folder_storage_path,
                        self.results_folder_storage_path)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysmFISH" href="index.html">pysmFISH</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pysmFISH.pipeline.Pipeline" href="#pysmFISH.pipeline.Pipeline">Pipeline</a></code></h4>
<ul class="">
<li><code><a title="pysmFISH.pipeline.Pipeline.QC_check_experiment_yaml_file_step" href="#pysmFISH.pipeline.Pipeline.QC_check_experiment_yaml_file_step">QC_check_experiment_yaml_file_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.QC_registration_error_step" href="#pysmFISH.pipeline.Pipeline.QC_registration_error_step">QC_registration_error_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.create_analysis_config_file_from_dataset_step" href="#pysmFISH.pipeline.Pipeline.create_analysis_config_file_from_dataset_step">create_analysis_config_file_from_dataset_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.create_folders_step" href="#pysmFISH.pipeline.Pipeline.create_folders_step">create_folders_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.create_running_functions_step" href="#pysmFISH.pipeline.Pipeline.create_running_functions_step">create_running_functions_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.determine_tiles_organization" href="#pysmFISH.pipeline.Pipeline.determine_tiles_organization">determine_tiles_organization</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.determine_tiles_organization_before_room_reorganisation" href="#pysmFISH.pipeline.Pipeline.determine_tiles_organization_before_room_reorganisation">determine_tiles_organization_before_room_reorganisation</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.microscope_stitched_remove_dots_eel_graph_step" href="#pysmFISH.pipeline.Pipeline.microscope_stitched_remove_dots_eel_graph_step">microscope_stitched_remove_dots_eel_graph_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.nikon_nd2_parsing_graph_step" href="#pysmFISH.pipeline.Pipeline.nikon_nd2_parsing_graph_step">nikon_nd2_parsing_graph_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.prepare_processing_dataset_step" href="#pysmFISH.pipeline.Pipeline.prepare_processing_dataset_step">prepare_processing_dataset_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.processing_barcoded_eel_step" href="#pysmFISH.pipeline.Pipeline.processing_barcoded_eel_step">processing_barcoded_eel_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.processing_cluster_init_step" href="#pysmFISH.pipeline.Pipeline.processing_cluster_init_step">processing_cluster_init_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.processing_fresh_tissue_step" href="#pysmFISH.pipeline.Pipeline.processing_fresh_tissue_step">processing_fresh_tissue_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.processing_serial_fish_step" href="#pysmFISH.pipeline.Pipeline.processing_serial_fish_step">processing_serial_fish_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.rerun_decoding_step" href="#pysmFISH.pipeline.Pipeline.rerun_decoding_step">rerun_decoding_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.rerun_from_registration_step" href="#pysmFISH.pipeline.Pipeline.rerun_from_registration_step">rerun_from_registration_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.run_cluster_activation" href="#pysmFISH.pipeline.Pipeline.run_cluster_activation">run_cluster_activation</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.run_full" href="#pysmFISH.pipeline.Pipeline.run_full">run_full</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.run_parsing" href="#pysmFISH.pipeline.Pipeline.run_parsing">run_parsing</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.run_parsing_only" href="#pysmFISH.pipeline.Pipeline.run_parsing_only">run_parsing_only</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.run_required_steps" href="#pysmFISH.pipeline.Pipeline.run_required_steps">run_required_steps</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.run_setup" href="#pysmFISH.pipeline.Pipeline.run_setup">run_setup</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.save_git_commit" href="#pysmFISH.pipeline.Pipeline.save_git_commit">save_git_commit</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.stitch_and_remove_dots_eel_graph_old_room_step" href="#pysmFISH.pipeline.Pipeline.stitch_and_remove_dots_eel_graph_old_room_step">stitch_and_remove_dots_eel_graph_old_room_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.stitch_and_remove_dots_eel_graph_step" href="#pysmFISH.pipeline.Pipeline.stitch_and_remove_dots_eel_graph_step">stitch_and_remove_dots_eel_graph_step</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.test_run_Lars_mouse_atlas" href="#pysmFISH.pipeline.Pipeline.test_run_Lars_mouse_atlas">test_run_Lars_mouse_atlas</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.test_run_after_editing" href="#pysmFISH.pipeline.Pipeline.test_run_after_editing">test_run_after_editing</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.test_run_decoding" href="#pysmFISH.pipeline.Pipeline.test_run_decoding">test_run_decoding</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.test_run_from_registration" href="#pysmFISH.pipeline.Pipeline.test_run_from_registration">test_run_from_registration</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.test_run_short" href="#pysmFISH.pipeline.Pipeline.test_run_short">test_run_short</a></code></li>
<li><code><a title="pysmFISH.pipeline.Pipeline.transfer_data_after_processing" href="#pysmFISH.pipeline.Pipeline.transfer_data_after_processing">transfer_data_after_processing</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>