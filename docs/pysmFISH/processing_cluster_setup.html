<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysmFISH.processing_cluster_setup API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysmFISH.processing_cluster_setup</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import *
import dask
import os
import signal
import sys
from dask_jobqueue.htcondor import HTCondorCluster
from dask.distributed import LocalCluster, SSHCluster
from psutil import virtual_memory

from pysmFISH.logger_utils import selected_logger

def htcondor_cluster_setup(htcondor_cluster_setup: dict):
    &#34;&#34;&#34;Utility function to start a HTCondor cluster

    Args:
        htcondor_cluster_setup (dict): cluster_setup_dictionary
        dictionary with the info for the cluster setup

    &#34;&#34;&#34;

    logger = selected_logger()
    cores = htcondor_cluster_setup[&#39;cores&#39;]
    memory = htcondor_cluster_setup[&#39;memory&#39;]
    disk = htcondor_cluster_setup[&#39;disk&#39;]
    local_directory = htcondor_cluster_setup[&#39;local_directory&#39;]
    log_directory = htcondor_cluster_setup[&#39;logs_directory&#39;]
    cluster = HTCondorCluster(cores=cores, memory=memory, 
                        disk=disk,local_directory=local_directory,
                        log_directory=log_directory,
                        processes=1)
    logger.info(f&#39;created cluster with {cores} cores and {memory} memory&#39;)
    # make cluster more resilient
    cluster.scheduler.allowed_failures = 1000
    return cluster

# extra = [&#34;--lifetime&#34;, &#34;500m&#34;,
#                         &#34;--death_timeout&#34;,&#34;5000&#34; ]

#  &#34;--lifetime-stagger&#34;, &#34;10m&#34;,
# death_timeout=5000,


def local_cluster_setup(cores:int, memory:str):
    &#34;&#34;&#34;Utility to set up a dask cluster on a local computer. I will use
    all the cpus-1 and scatter the memory. In thi

    Args:
        cores (int): number of cores of the computer to use for processing
        memory (str): memory for each core (ex. 5GB) 
    Returns:
       cluster: dask cluster
    &#34;&#34;&#34;

    # total_ram = virtual_memory()
    # total_ram = total_ram.available
    
    # cores = dask.multiprocessing.multiprocessing.cpu_count()-1

    # Calculate the total ram to use for each worker
    # worker_memory_limit = 0.9
    # worker_memory = (total_ram*worker_memory_limit)/cores

    #cores = 5
    # worker_memory = 10000000000
    # cluster = LocalCluster(n_workers=cores, threads_per_worker=1, memory_limit=worker_memory)
    # cluster = LocalCluster(n_workers=cores, memory_limit=worker_memory)
    cluster = LocalCluster(n_workers=cores, memory_limit=memory, processes=True,threads_per_worker=1)



    return cluster


def unmanaged_cluster_setup(htcondor_cluster_setup:Dict):
    &#34;&#34;&#34;Create and start a unmanaged cluster. The cluster is
    created by ssh into the machines. Because there is bug in
    OpenSSH in not enough to kill the cluster by using
    client.close()
    cluster.close()
    it is necessary to kill the distributed process using
    kill_distributed_process()

    Args:
        htcondor_cluster_setup (Dict): dictionary with the info for the cluster setup

    Returns:
       cluster: dask cluster
    &#34;&#34;&#34;
    
    logger = selected_logger()
    cores = htcondor_cluster_setup[&#39;cores&#39;]
    memory = htcondor_cluster_setup[&#39;memory&#39;]
    disk = htcondor_cluster_setup[&#39;disk&#39;]
    local_directory = htcondor_cluster_setup[&#39;local_directory&#39;]
    log_directory = htcondor_cluster_setup[&#39;logs_directory&#39;]
    scheduler_port = htcondor_cluster_setup[&#39;scheduler_port&#39;]
    dashboard_port = htcondor_cluster_setup[&#39;dashboard_port&#39;]
    nprocs = htcondor_cluster_setup[&#39;nprocs&#39;]
    scheduler_address = htcondor_cluster_setup[&#39;scheduler_address&#39;]
    workers_addresses_list = htcondor_cluster_setup[&#39;workers_addresses_list&#39;]
    nthreads = htcondor_cluster_setup[&#39;nthreads&#39;]
    
    
    all_addresses = [scheduler_address] + workers_addresses_list
    
    worker_options = {&#34;nprocs&#34;:nprocs,
                     &#34;memory_limit&#34;:memory,
                     &#34;nthreads&#34;:nthreads,
                     &#34;local_directory&#34;:local_directory}
    
    
    cluster = SSHCluster(
        all_addresses,
        connect_options={&#34;known_hosts&#34;: None},
        worker_options=worker_options,
        scheduler_options={&#34;port&#34;: scheduler_port, 
                       &#34;dashboard_address&#34;:dashboard_port}
        )

    return cluster



def kill_process(process_name:str=&#39;distributed.cli.dask.scheduler&#39;):
    &#34;&#34;&#34;General function used to kill a process by name directly from a
    script (https://www.geeksforgeeks.org/kill-a-process-by-name-using-python/)
    
    I needed the function to compensate for the bug in OpenSSH that
    doesn&#39;t allow the destruction of the cluster using the standard
    dask commands ( https://github.com/dask/distributed/issues/3420 ). 
    By default the distributed process will be killed.

    Args:
        process_name (str, optional): [description]. Defaults to &#39;distributed.cli.dask.scheduler&#39;.
    &#34;&#34;&#34;
   # https://github.com/dask/distributed/issues/3420  
    # Ask user for the name of process
    
    logger = selected_logger()
    try:
        # iterating through each instance of the process
        for line in os.popen(&#34;ps ax | grep &#34; + process_name + &#34; | grep -v grep&#34;):
            fields = line.split()
             
            # extracting Process ID from the output
            pid = fields[0]
             
            # terminating process
            os.kill(int(pid), signal.SIGKILL)
        logger.info(&#34;Process Successfully terminated&#34;)
         
    except:
        logger.error(&#34;Error Encountered while running script&#34;)




def start_processing_env(processing_env_config:Dict):
    &#34;&#34;&#34;Function to start the processing env. In the current setup
    is set up to run on the local computer or in a HPC cluster 
    manged by HTCondor. The max number of jobs in htcondor is hardcoded to 15
    and the cluster is adaptive.

    Args:
        processing_env_config (Dict): Parameters that define the 
                    cluster characteristics.

    &#34;&#34;&#34;
    
    logger = selected_logger()
    processing_engine = processing_env_config[&#39;processing_engine&#39;]
    if processing_engine == &#39;htcondor&#39;:
        cluster = htcondor_cluster_setup(processing_env_config)
        cluster.scale(jobs=1)
        # Always put a minimum to avoid the cluster to shut down
        minimum_jobs = 1
        if processing_env_config[&#39;adaptive&#39;]:
            cluster.adapt(minimum_jobs=minimum_jobs,maximum_jobs=processing_env_config[&#39;maximum_jobs&#39;])
            logger.info(f&#34;Started adaptive cluster&#34;)
        else:
            cluster.scale(jobs=processing_env_config[&#39;maximum_jobs&#39;])
            logger.info(f&#34;Started non adaptive cluster&#34;)
        return cluster
    elif processing_engine == &#39;local&#39;:
        cluster = local_cluster_setup(processing_env_config[&#39;cores&#39;],processing_env_config[&#39;memory&#39;])
        return cluster
    elif processing_engine == &#39;unmanaged_cluster&#39;:
        cluster = unmanaged_cluster_setup(processing_env_config)
        logger.info(f&#34;Started unmanaged cluster&#34;)
        return cluster
    else:
        logger.error(f&#39;the processing engine is not defined check the name&#39;)
        sys.exit(f&#39;the processing engine is not defined check the name&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pysmFISH.processing_cluster_setup.htcondor_cluster_setup"><code class="name flex">
<span>def <span class="ident">htcondor_cluster_setup</span></span>(<span>htcondor_cluster_setup: dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Utility function to start a HTCondor cluster</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>htcondor_cluster_setup</code></strong> :&ensp;<code>dict</code></dt>
<dd>cluster_setup_dictionary</dd>
</dl>
<p>dictionary with the info for the cluster setup</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def htcondor_cluster_setup(htcondor_cluster_setup: dict):
    &#34;&#34;&#34;Utility function to start a HTCondor cluster

    Args:
        htcondor_cluster_setup (dict): cluster_setup_dictionary
        dictionary with the info for the cluster setup

    &#34;&#34;&#34;

    logger = selected_logger()
    cores = htcondor_cluster_setup[&#39;cores&#39;]
    memory = htcondor_cluster_setup[&#39;memory&#39;]
    disk = htcondor_cluster_setup[&#39;disk&#39;]
    local_directory = htcondor_cluster_setup[&#39;local_directory&#39;]
    log_directory = htcondor_cluster_setup[&#39;logs_directory&#39;]
    cluster = HTCondorCluster(cores=cores, memory=memory, 
                        disk=disk,local_directory=local_directory,
                        log_directory=log_directory,
                        processes=1)
    logger.info(f&#39;created cluster with {cores} cores and {memory} memory&#39;)
    # make cluster more resilient
    cluster.scheduler.allowed_failures = 1000
    return cluster</code></pre>
</details>
</dd>
<dt id="pysmFISH.processing_cluster_setup.kill_process"><code class="name flex">
<span>def <span class="ident">kill_process</span></span>(<span>process_name: str = 'distributed.cli.dask.scheduler')</span>
</code></dt>
<dd>
<div class="desc"><p>General function used to kill a process by name directly from a
script (<a href="https://www.geeksforgeeks.org/kill-a-process-by-name-using-python/">https://www.geeksforgeeks.org/kill-a-process-by-name-using-python/</a>)</p>
<p>I needed the function to compensate for the bug in OpenSSH that
doesn't allow the destruction of the cluster using the standard
dask commands ( <a href="https://github.com/dask/distributed/issues/3420">https://github.com/dask/distributed/issues/3420</a> ).
By default the distributed process will be killed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>process_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to 'distributed.cli.dask.scheduler'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kill_process(process_name:str=&#39;distributed.cli.dask.scheduler&#39;):
    &#34;&#34;&#34;General function used to kill a process by name directly from a
    script (https://www.geeksforgeeks.org/kill-a-process-by-name-using-python/)
    
    I needed the function to compensate for the bug in OpenSSH that
    doesn&#39;t allow the destruction of the cluster using the standard
    dask commands ( https://github.com/dask/distributed/issues/3420 ). 
    By default the distributed process will be killed.

    Args:
        process_name (str, optional): [description]. Defaults to &#39;distributed.cli.dask.scheduler&#39;.
    &#34;&#34;&#34;
   # https://github.com/dask/distributed/issues/3420  
    # Ask user for the name of process
    
    logger = selected_logger()
    try:
        # iterating through each instance of the process
        for line in os.popen(&#34;ps ax | grep &#34; + process_name + &#34; | grep -v grep&#34;):
            fields = line.split()
             
            # extracting Process ID from the output
            pid = fields[0]
             
            # terminating process
            os.kill(int(pid), signal.SIGKILL)
        logger.info(&#34;Process Successfully terminated&#34;)
         
    except:
        logger.error(&#34;Error Encountered while running script&#34;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.processing_cluster_setup.local_cluster_setup"><code class="name flex">
<span>def <span class="ident">local_cluster_setup</span></span>(<span>cores: int, memory: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Utility to set up a dask cluster on a local computer. I will use
all the cpus-1 and scatter the memory. In thi</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cores</code></strong> :&ensp;<code>int</code></dt>
<dd>number of cores of the computer to use for processing</dd>
<dt><strong><code>memory</code></strong> :&ensp;<code>str</code></dt>
<dd>memory for each core (ex. 5GB) </dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>cluster</code></dt>
<dd>dask cluster</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def local_cluster_setup(cores:int, memory:str):
    &#34;&#34;&#34;Utility to set up a dask cluster on a local computer. I will use
    all the cpus-1 and scatter the memory. In thi

    Args:
        cores (int): number of cores of the computer to use for processing
        memory (str): memory for each core (ex. 5GB) 
    Returns:
       cluster: dask cluster
    &#34;&#34;&#34;

    # total_ram = virtual_memory()
    # total_ram = total_ram.available
    
    # cores = dask.multiprocessing.multiprocessing.cpu_count()-1

    # Calculate the total ram to use for each worker
    # worker_memory_limit = 0.9
    # worker_memory = (total_ram*worker_memory_limit)/cores

    #cores = 5
    # worker_memory = 10000000000
    # cluster = LocalCluster(n_workers=cores, threads_per_worker=1, memory_limit=worker_memory)
    # cluster = LocalCluster(n_workers=cores, memory_limit=worker_memory)
    cluster = LocalCluster(n_workers=cores, memory_limit=memory, processes=True,threads_per_worker=1)



    return cluster</code></pre>
</details>
</dd>
<dt id="pysmFISH.processing_cluster_setup.start_processing_env"><code class="name flex">
<span>def <span class="ident">start_processing_env</span></span>(<span>processing_env_config: Dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to start the processing env. In the current setup
is set up to run on the local computer or in a HPC cluster
manged by HTCondor. The max number of jobs in htcondor is hardcoded to 15
and the cluster is adaptive.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>processing_env_config</code></strong> :&ensp;<code>Dict</code></dt>
<dd>Parameters that define the
cluster characteristics.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_processing_env(processing_env_config:Dict):
    &#34;&#34;&#34;Function to start the processing env. In the current setup
    is set up to run on the local computer or in a HPC cluster 
    manged by HTCondor. The max number of jobs in htcondor is hardcoded to 15
    and the cluster is adaptive.

    Args:
        processing_env_config (Dict): Parameters that define the 
                    cluster characteristics.

    &#34;&#34;&#34;
    
    logger = selected_logger()
    processing_engine = processing_env_config[&#39;processing_engine&#39;]
    if processing_engine == &#39;htcondor&#39;:
        cluster = htcondor_cluster_setup(processing_env_config)
        cluster.scale(jobs=1)
        # Always put a minimum to avoid the cluster to shut down
        minimum_jobs = 1
        if processing_env_config[&#39;adaptive&#39;]:
            cluster.adapt(minimum_jobs=minimum_jobs,maximum_jobs=processing_env_config[&#39;maximum_jobs&#39;])
            logger.info(f&#34;Started adaptive cluster&#34;)
        else:
            cluster.scale(jobs=processing_env_config[&#39;maximum_jobs&#39;])
            logger.info(f&#34;Started non adaptive cluster&#34;)
        return cluster
    elif processing_engine == &#39;local&#39;:
        cluster = local_cluster_setup(processing_env_config[&#39;cores&#39;],processing_env_config[&#39;memory&#39;])
        return cluster
    elif processing_engine == &#39;unmanaged_cluster&#39;:
        cluster = unmanaged_cluster_setup(processing_env_config)
        logger.info(f&#34;Started unmanaged cluster&#34;)
        return cluster
    else:
        logger.error(f&#39;the processing engine is not defined check the name&#39;)
        sys.exit(f&#39;the processing engine is not defined check the name&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.processing_cluster_setup.unmanaged_cluster_setup"><code class="name flex">
<span>def <span class="ident">unmanaged_cluster_setup</span></span>(<span>htcondor_cluster_setup: Dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Create and start a unmanaged cluster. The cluster is
created by ssh into the machines. Because there is bug in
OpenSSH in not enough to kill the cluster by using
client.close()
cluster.close()
it is necessary to kill the distributed process using
kill_distributed_process()</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>htcondor_cluster_setup</code></strong> :&ensp;<code>Dict</code></dt>
<dd>dictionary with the info for the cluster setup</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>cluster</code></dt>
<dd>dask cluster</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unmanaged_cluster_setup(htcondor_cluster_setup:Dict):
    &#34;&#34;&#34;Create and start a unmanaged cluster. The cluster is
    created by ssh into the machines. Because there is bug in
    OpenSSH in not enough to kill the cluster by using
    client.close()
    cluster.close()
    it is necessary to kill the distributed process using
    kill_distributed_process()

    Args:
        htcondor_cluster_setup (Dict): dictionary with the info for the cluster setup

    Returns:
       cluster: dask cluster
    &#34;&#34;&#34;
    
    logger = selected_logger()
    cores = htcondor_cluster_setup[&#39;cores&#39;]
    memory = htcondor_cluster_setup[&#39;memory&#39;]
    disk = htcondor_cluster_setup[&#39;disk&#39;]
    local_directory = htcondor_cluster_setup[&#39;local_directory&#39;]
    log_directory = htcondor_cluster_setup[&#39;logs_directory&#39;]
    scheduler_port = htcondor_cluster_setup[&#39;scheduler_port&#39;]
    dashboard_port = htcondor_cluster_setup[&#39;dashboard_port&#39;]
    nprocs = htcondor_cluster_setup[&#39;nprocs&#39;]
    scheduler_address = htcondor_cluster_setup[&#39;scheduler_address&#39;]
    workers_addresses_list = htcondor_cluster_setup[&#39;workers_addresses_list&#39;]
    nthreads = htcondor_cluster_setup[&#39;nthreads&#39;]
    
    
    all_addresses = [scheduler_address] + workers_addresses_list
    
    worker_options = {&#34;nprocs&#34;:nprocs,
                     &#34;memory_limit&#34;:memory,
                     &#34;nthreads&#34;:nthreads,
                     &#34;local_directory&#34;:local_directory}
    
    
    cluster = SSHCluster(
        all_addresses,
        connect_options={&#34;known_hosts&#34;: None},
        worker_options=worker_options,
        scheduler_options={&#34;port&#34;: scheduler_port, 
                       &#34;dashboard_address&#34;:dashboard_port}
        )

    return cluster</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysmFISH" href="index.html">pysmFISH</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pysmFISH.processing_cluster_setup.htcondor_cluster_setup" href="#pysmFISH.processing_cluster_setup.htcondor_cluster_setup">htcondor_cluster_setup</a></code></li>
<li><code><a title="pysmFISH.processing_cluster_setup.kill_process" href="#pysmFISH.processing_cluster_setup.kill_process">kill_process</a></code></li>
<li><code><a title="pysmFISH.processing_cluster_setup.local_cluster_setup" href="#pysmFISH.processing_cluster_setup.local_cluster_setup">local_cluster_setup</a></code></li>
<li><code><a title="pysmFISH.processing_cluster_setup.start_processing_env" href="#pysmFISH.processing_cluster_setup.start_processing_env">start_processing_env</a></code></li>
<li><code><a title="pysmFISH.processing_cluster_setup.unmanaged_cluster_setup" href="#pysmFISH.processing_cluster_setup.unmanaged_cluster_setup">unmanaged_cluster_setup</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>