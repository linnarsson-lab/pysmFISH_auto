<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>pysmFISH.utilities_tasks API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysmFISH.utilities_tasks</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import *
import os
import zarr
import shutil
import yaml
import datetime
import xarray as xr
import numpy as np
from pathlib import Path
from collections import OrderedDict

from prefect import task
from prefect.engine import signals

from pysmFISH.logger_utils import prefect_logging_setup


# to avoid reference for nested structures
# https://stackoverflow.com/questions/13518819/avoid-references-in-pyyaml (comment)
yaml.SafeDumper.ignore_aliases = lambda *args : True


# @task(name=&#39;check_completed_transfer_to_monod&#39;)
def check_completed_transfer_to_monod(path_tmp_storage_server:str, flag_file_key:str):
    &#34;&#34;&#34;
    Function to scan the folder where the data are transferred from the machines.
    It looks for files that are generated upon transfer completion (flag_file).
    To keep thing simple and not overload the system only one of the flag_files
    identified in the scanning of the folder will be processed.
    
    NB: In order to process only the experiments designed for the pipeline 
    the folder name finish with auto ExpName_auto

    Args: 
    path_tmp_storage_server: str
        path where the data are transferred from the microscopes
    flag_file_key: str
        string that define the flag_files. The flag key should not have _auto_
        in the name.


    Returns:
        experiment_path: Posix
            experiment path in the tmp folder
    &#34;&#34;&#34;

    logger = prefect_logging_setup(&#39;check_completed_transfer_to_monod&#39;)
    path_tmp_storage_server = Path(path_tmp_storage_server)
    flag_file_key_general = &#39;*_auto_&#39; + flag_file_key
    flag_file_key = &#39;_&#39; + flag_file_key
    flagged_files_list = list(path_tmp_storage_server.glob(flag_file_key_general))
    

    if flagged_files_list:
        flagged_file_path = flagged_files_list[0]
        experiment_name = (flagged_file_path.name).split(flag_file_key)[0]
        os.remove(flagged_file_path)
        logger.info(f&#39;{experiment_name} ready to be processed&#39;)
        return flagged_file_path.parent / experiment_name
    else:
        logger.info(f&#39;No new experiments to be processed&#39;)
        skip_signal = signals.SKIP(&#34;No new experiments to be processed&#34;)
        skip_signal.flag = True
        skip_signal.value = None
        raise skip_signal



@task(name=&#39;move_data&#39;)
def move_data(path_source_location:str,path_destination:str, flag_file_key:str):
    &#34;&#34;&#34;
    Function used to transfer the files to another location

    Args:
        path_source_location: str
            path to the data to be moved
        path_destination: str
            path to the destination of the transfer
        flag_file_key: str
        string that define the flag_files. The flag key should not have _auto_
        in the name.


    Returns:
        experiment_path: Posix
            path to the data after transferring
    &#34;&#34;&#34;

    logger = prefect_logging_setup(&#39;transfer_data&#39;)
    path_source_location = Path(path_source_location)
    path_destination = Path(path_destination)

    try:
        os.stat(path_source_location)
    except:
        logger.error(f&#39; The {path_source_location} directory is missing&#39;)
        fail_signal = signals.FAIL(&#39;The source directory is missing&#39;)
        fail_signal.flag = True
        fail_signal.value = None
        raise fail_signal
    
    if os.stat(path_destination):
        os.stat(path_destination)
        shutil.move(path_source_location.as_posix(),path_destination.as_posix())
        tag_file_name = path_destination / (path_source_location.stem + &#39;_&#39; + flag_file_key)
        open(tag_file_name,&#39;w&#39;).close()
        logger.info(f&#39;data moved from {path_source_location} to {path_destination}&#39;)
    else:
        logger.info(f&#39; The {path_destination} directory is missing&#39;)
        fail_signal = signals.FAIL(&#39;The destination directory is missing&#39;)
        fail_signal.flag = True
        fail_signal.value = None
        raise fail_signal






@task(name=&#39;create_folder_structure&#39;)
def create_folder_structure(experiment_fpath:str):
    &#34;&#34;&#34;
    Function used to create the folder structure where to sort the files
    generated by the machines and the saving the data created during the
    processing. It creates the backbone structure common to all analysis

    original_robofish_logs: contains all the original robofish logs.
        extra_files: contains the extra files acquired during imaging.
        extra_processing_data: contains extra files used in the analysis 
                                                                                                like the dark images for flat field correction.
    pipeline_config: contains all the configuration files.
    raw_data: contains the renamed .nd2 files and the corresponding 
                                                pickle configuration files. It is the directory that is 
                                                backed up on the server.
        output_figures: contains the reports and visualizations
    notebooks: will contain potential notebooks used for processing the data
    probes: will contains the fasta file with the probes used in the experiment
    tmp: save temporary data
    

    Args:
        experiment_fpath: str
            folder path of the experiment
    &#34;&#34;&#34;
    logger = prefect_logging_setup(&#39;created_raw_tmp_dir&#39;)
    experiment_fpath = Path(experiment_fpath)
    folders_list = [&#39;raw_data&#39;,
                    &#39;original_robofish_logs&#39;,
                    &#39;extra_processing_data&#39;,
                    &#39;extra_files&#39;,
                    &#39;pipeline_config&#39;,
                    &#39;output_figures&#39;,
                    &#39;notebooks&#39;,
                    &#39;probes&#39;,
                    &#39;tmp&#39;]
    for folder_name in folders_list:
        try:
            os.stat(experiment_fpath / folder_name )
            logger.info(f&#39;{folder_name} already exist&#39;)
        except FileNotFoundError:
            os.mkdir(experiment_fpath / folder_name)
            os.chmod(experiment_fpath / folder_name,0o777)


@task(name=&#39;upload-extra-files&#39;)
def collect_extra_files(experiment_fpath:str, experiment_info:Dict):
    &#34;&#34;&#34;
    Function used to collect extra files required for the processing
    - instrument_name_dark_img for field correction
    - codebook if the experiment is barcoded

    &#34;&#34;&#34;
    logger = prefect_logging_setup(&#39;collect_extra_files&#39;)
    experiment_fpath = Path(experiment_fpath)

    try:
        machine = experiment_info[&#39;Machine&#39;]
    except NameError:
        machine = &#39;NOT_DEFINED&#39;

    dark_img_fpath = experiment_fpath.parent / &#39;dark_imgs&#39; / (experiment_info[&#39;Machine&#39;] + &#39;_dark_img.npy&#39;)
    try:
        shutil.copy(dark_img_fpath, (experiment_fpath / &#39;extra_processing_data&#39;))
    except FileNotFoundError:
        logger.error(&#39;missing dark image&#39;)
        raise signals.FAIL(&#39;missing dark image&#39;)
    else:
        processing_env_config_fpath = experiment_fpath.parent / &#39;config_db&#39; / &#39;processing_env_config.yaml&#39;
        try:
            shutil.copy(processing_env_config_fpath, (experiment_fpath / &#39;pipeline_config&#39;))
        except FileNotFoundError:
            logger.error(&#39;missing pipeline env config file&#39;)
            raise signals.FAIL(&#39;missing pipeline env config file&#39;)
        else:
            probes_fpath = experiment_fpath.parent / &#39;probes_sets&#39; / (experiment_info[&#39;Probes&#39;] + &#39;.fasta&#39;)
            try:
                shutil.copy(probes_fpath, (experiment_fpath / &#39;probes&#39;))
            except FileNotFoundError:
                logger.error(&#39;missing probes set file&#39;)
                raise signals.FAIL(&#39;missing probes set file&#39;)
            else:
                if &#39;barcoded&#39; in experiment_info[&#39;Experiment_type&#39;]:
                    codebooks_folder = experiment_fpath.parent / &#39;codebooks&#39;
                    codebook_code = experiment_info[&#39;Codebook&#39;]
                    
                    codebook_fpath = codebooks_folder / (codebook_code + &#39;_codebook.parquet&#39;)
                    
                    # Create codebook folder in the experiment folder
                    try:
                        os.stat(experiment_fpath / &#39;codebook&#39; )
                        logger.info(f&#39;codebook folder already exist&#39;)
                    except FileNotFoundError:
                        os.mkdir(experiment_fpath / &#39;codebook&#39;)
                        os.chmod(experiment_fpath / &#39;codebook&#39;,0o777)
                    try:
                        shutil.copy(codebook_fpath, (experiment_fpath / &#39;codebook&#39;))
                    except FileNotFoundError:
                        logger.error(&#39;codebook is missing&#39;)
                        raise signals.FAIL(&#39;codebook is missing&#39;)


@task(name = &#39;load_data_array&#39;)
def load_data_array(input_tuple):
    &#34;&#34;&#34;
    Function used to load the images out memory as xarray data array
    
    Args:
        input_tuple: tuple
        contains the path to the zarr file to process (str) and the
        number of the fov to analyse

    Return:
        img_data_array: xarray data array
        data array with the image and all the required metadata
    &#34;&#34;&#34;
    logger = prefect_logging_setup(&#39;load-data-array&#39;)
    zarr_store, fov_num = input_tuple

    try:
        dataset = xr.open_zarr(zarr_store)
    except:
        logger.error(f&#39;cannot load the dataset, check the zarr store {zarr_store}&#39;)
        signals.FAIL(f&#39;cannot load the dataset, check the zarr store {zarr_store}&#39;)
    else:
        try:
            img_data_array = dataset[str(fov_num)]
        except:
            logger.error(f&#39;cannot load the data array, check if fov {fov_num} is missing&#39;)
            signals.FAIL(f&#39;cannot load the data array, check if fov {fov_num} is missing&#39;)
        else:
            return img_data_array


@task(name=&#39;create_empty_zarr_file&#39;)
def create_empty_zarr_file(experiment_fpath:str,tag:str)-&gt; str:
    &#34;&#34;&#34;
    Function that create and empty zarr file 

    Args:
        experiment_fpath: str
            location of the folder to be processed
        tag: str
            tag to add to the file name
    Return:
        empty_fpath: str
            path of the created file

    &#34;&#34;&#34;
    
    logger = prefect_logging_setup(f&#39;create empty {tag} file&#39;)
    experiment_fpath = Path(experiment_fpath)
    experiment_name = experiment_fpath.stem
    zarr_fpath = experiment_fpath / (experiment_name + &#39;_&#39; + tag + &#39;.zarr&#39;)
    
    dataset = xr.Dataset()
    try:
        dataset.to_zarr(zarr_fpath, mode=&#39;w&#39;, consolidated=True)
    except:
        logger.error(f&#39;cannot create {tag} file&#39;)
        signals.FAIL(f&#39;cannot create {tag} file&#39;)
    else:
        return zarr_fpath



@task(name = &#39;sort-data-folder&#39;)
def sort_data_folder(experiment_fpath:str,experiment_info:Dict):
    &#34;&#34;&#34;
    Function used to sort the data in the experiment folder in the
    subfolders
    
    Args:
        experiment_fpath: str
            location of the folder to be processed
        experiment_info: ordered dict
            ordered dict with the parsed info generated by the instrument
    
    &#34;&#34;&#34;
    logger = prefect_logging_setup(&#39;sort-data-folder&#39;)
    experiment_fpath = Path(experiment_fpath)

    # Move the robofish generated logs
    robofish_logs = experiment_fpath.glob(&#39;*.log&#39;)
    for log in robofish_logs:
        shutil.move(log.as_posix(), (experiment_fpath / &#39;original_robofish_logs&#39;).as_posix())

    # Move the crosses images
    crosses_files = experiment_fpath.glob(&#39;*_cross.nd2&#39;)
    for cross in crosses_files:
        shutil.move(cross.as_posix(), (experiment_fpath / &#39;extra_files&#39;).as_posix())

    # Move the coords files
    coords_files = experiment_fpath.glob(&#39;*.xml&#39;)
    for coord in coords_files:
        shutil.move(coord.as_posix(), (experiment_fpath / &#39;extra_files&#39;).as_posix())

    # Move the fresh nuclei file for eel
    if &#39;eel&#39; in experiment_info[&#39;Experiment_type&#39;]:
        nuclei_files = list(experiment_fpath.glob(&#39;*ChannelDAPI*&#39;))
        if len(nuclei_files):
            try:
                os.stat(experiment_fpath / &#39;fresh_nuclei&#39; )
                logger.info(f&#39;fresh_nuclei already exist&#39;)
            except FileNotFoundError:
                os.mkdir(experiment_fpath / &#39;fresh_nuclei&#39;)
                os.chmod(experiment_fpath / &#39;fresh_nuclei&#39;,0o777)
            
            for nuclei in nuclei_files:
                shutil.move(nuclei.as_posix(), (experiment_fpath / &#39;fresh_nuclei&#39;).as_posix())
        else:
            logger.info(f&#39;The experiment do not have images of fresh nuclei&#39;)


@task(name = &#39;consolidate-metadata&#39;)
def consolidate_zarr_metadata(parsed_raw_data_fpath:str):
    &#34;&#34;&#34;
    Function to consolidate all the zarr metadata in one unique
    json file for eady indexing and searching

    Args:
        parsed_raw_data_fpath: str
            path to the file with all the parsed images
    
    Returns:
        consolidated_grp: zarr group
            zarr groups instance with the consolidated metadata
    &#34;&#34;&#34;

    logger = prefect_logging_setup(f&#39;consolidate-metadata&#39;)
    
    try:
        store = zarr.DirectoryStore(parsed_raw_data_fpath)
        consolidated_grp = zarr.consolidate_metadata(store)
    except:
        logger.error(f&#39;cannot consolidate metadata of the parsed zarr file&#39;)
        signals.FAIL(f&#39;cannot consolidate metadata of the parsed zarr file&#39;)
    
    else:
        return consolidated_grp


@task(name = &#39;load-raw-images-and-filtering-attrs&#39;)
def load_raw_images(zarr_grp_name:str,parsed_raw_data_fpath:str)-&gt;np.ndarray:
    &#34;&#34;&#34;
    Function used to load a raw image and metadata from the 
    parsed raw file and the attrs for the filtering
        parsed_raw_data_fpath: str
            fpath to zarr store containing the parsed raw images
        zarr_grp_name: str
            fpath to the group to process. The group contain the raw images and the 
            corresponding metadata

            grp = experiment_name_channel_fov_X
                dataset = raw_data_fov_X

    &#34;&#34;&#34;
    logger = prefect_logging_setup(f&#39;consolidate-metadata&#39;)
    st = zarr.DirectoryStore(parsed_raw_data_fpath)
    root = zarr.group(store=st,overwrite=False)

    metadata = root[zarr_grp_name].attrs
    img = root[zarr_grp_name][metadata[&#39;fov_name&#39;]][...]
    return (img, dict(metadata))

@task(name=&#39;sort-images-according-processing-type&#39;)
def sorting_grps(grps, experiment_info, analysis_parameters):
    &#34;&#34;&#34;
    Function used to separate the group names according to 
    the processing type

        grps = zarr.hierarchy.Group
            consolidate hierarchy zarr groups for fast access to metadata
        experiment_info: ordered dict
            ordered dict with the parsed info generated by the instrument
        analysis_parameters: dict
            dict with all the parameters to use for analysis  
    &#34;&#34;&#34;

    fish_grp = []
    beads_grp = []
    staining_grp = []
    for name, grp in grps.items():
        if grp.attrs[&#39;stitching_channel&#39;] == grp.attrs[&#39;channel&#39;]:
            beads_grp.append(name)
        elif &#39;_ST&#39; in grp.attrs[&#39;target_name&#39;]:
            staining_grp.append(name)
        else:
            fish_grp.append(name)

    logger = prefect_logging_setup(f&#39;sort-groups&#39;)
    if experiment_info[&#39;Stitching_type&#39;] == &#39;small-beads&#39;:
        beads_selected_parameters  = analysis_parameters[&#39;small-beads&#39;]
        beads_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    elif experiment_info[&#39;Stitching_type&#39;] == &#39;large-beads&#39;:
        beads_selected_parameters  = analysis_parameters[&#39;small-beads&#39;]
        beads_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    else:
        logger.error(f&#39;Wrong stitching type in the experiment file&#39;)
        err = signals.FAIL(f&#39;Wrong stitching type in the experiment file&#39;)
        raise err
    
    fish_selected_parameters = analysis_parameters[&#39;fish&#39;]
    fish_selected_parameters[&#39;BarcodesExtractionResolution&#39;] = analysis_parameters[&#39;BarcodesExtractionResolution&#39;]
    fish_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]

    staining_selected_parameters = analysis_parameters[&#39;staining&#39;]
    staining_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]

    return fish_grp, fish_selected_parameters, beads_grp, beads_selected_parameters, staining_grp, staining_selected_parameters


@task(name=&#39;sort-images-according-processing-type&#39;)
def sorting_grps_fov(grps, experiment_info, analysis_parameters):
    &#34;&#34;&#34;
    Function used to separate the group names according to 
    the processing type and by fov to reduce the number of calls
    to the writing.

    Args:
    -----
        grps = zarr.hierarchy.Group
            consolidate hierarchy zarr groups for fast access to metadata
        experiment_info: ordered dict
            ordered dict with the parsed info generated by the instrument
        analysis_parameters: dict
            dict with all the parameters to use for analysis  
    &#34;&#34;&#34;

    fish_grp = {}
    beads_grp = {}
    staining_grp = []
    for name, grp in grps.items():
        if grp.attrs[&#39;stitching_channel&#39;] == grp.attrs[&#39;channel&#39;]:
            if grp.attrs[&#39;channel&#39;] not in beads_grp.keys():
                beads_grp[grp.attrs[&#39;channel&#39;]] = {}
            if grp.attrs[&#39;fov_num&#39;] not in beads_grp[grp.attrs[&#39;channel&#39;]].keys():
                beads_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]] = []
                beads_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]].append(name)
            else:
                beads_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]].append(name)
        elif &#39;_ST&#39; in grp.attrs[&#39;target_name&#39;]:
            staining_grp.append(name)
        else:
            if grp.attrs[&#39;channel&#39;] not in fish_grp.keys():
                fish_grp[grp.attrs[&#39;channel&#39;]] = {}
            if  grp.attrs[&#39;fov_num&#39;] not in fish_grp[grp.attrs[&#39;channel&#39;]].keys():
                fish_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]] = []
                fish_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]].append(name)
            else:
                fish_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]].append(name)

    fish_list = []
    beads_list = []
    for channel in fish_grp.keys():
        for fov, names in fish_grp[channel].items():
            fish_list.append(names)
    
    if len(fish_grp.keys()) &gt; 1:
        fish_list = [el for sgr in fish_list for el in sgr]
    
    for channel in beads_grp.keys():
        for fov, names in beads_grp[channel].items():
            beads_list.append(names)
    
    if len(beads_grp.keys()) &gt; 1:
        beads_list = [el for sgr in beads_list for el in sgr]


    logger = prefect_logging_setup(f&#39;sort-groups&#39;)
    if experiment_info[&#39;Stitching_type&#39;] == &#39;small-beads&#39;:
        beads_selected_parameters  = analysis_parameters[&#39;small-beads&#39;]
        beads_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    elif experiment_info[&#39;Stitching_type&#39;] == &#39;large-beads&#39;:
        beads_selected_parameters  = analysis_parameters[&#39;small-beads&#39;]
        beads_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    else:
        logger.error(f&#39;Wrong stitching type in the experiment file&#39;)
        err = signals.FAIL(f&#39;Wrong stitching type in the experiment file&#39;)
        raise err
    
    fish_selected_parameters = analysis_parameters[&#39;fish&#39;]
    fish_selected_parameters[&#39;BarcodesExtractionResolution&#39;] = analysis_parameters[&#39;BarcodesExtractionResolution&#39;]
    fish_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]

    staining_selected_parameters = analysis_parameters[&#39;staining&#39;]
    staining_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]

    logger.info(f&#39;fish group {fish_list}&#39;)
    logger.info(f&#39;beads group {beads_list}&#39;)

    return fish_list, fish_selected_parameters, beads_list, beads_selected_parameters, staining_grp, staining_selected_parameters</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pysmFISH.utilities_tasks.check_completed_transfer_to_monod"><code class="name flex">
<span>def <span class="ident">check_completed_transfer_to_monod</span></span>(<span>path_tmp_storage_server: str, flag_file_key: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to scan the folder where the data are transferred from the machines.
It looks for files that are generated upon transfer completion (flag_file).
To keep thing simple and not overload the system only one of the flag_files
identified in the scanning of the folder will be processed.</p>
<p>NB: In order to process only the experiments designed for the pipeline
the folder name finish with auto ExpName_auto</p>
<p>Args:
path_tmp_storage_server: str
path where the data are transferred from the microscopes
flag_file_key: str
string that define the flag_files. The flag key should not have <em>auto</em>
in the name.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>experiment_path</code></dt>
<dd>Posix
experiment path in the tmp folder</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_completed_transfer_to_monod(path_tmp_storage_server:str, flag_file_key:str):
    &#34;&#34;&#34;
    Function to scan the folder where the data are transferred from the machines.
    It looks for files that are generated upon transfer completion (flag_file).
    To keep thing simple and not overload the system only one of the flag_files
    identified in the scanning of the folder will be processed.
    
    NB: In order to process only the experiments designed for the pipeline 
    the folder name finish with auto ExpName_auto

    Args: 
    path_tmp_storage_server: str
        path where the data are transferred from the microscopes
    flag_file_key: str
        string that define the flag_files. The flag key should not have _auto_
        in the name.


    Returns:
        experiment_path: Posix
            experiment path in the tmp folder
    &#34;&#34;&#34;

    logger = prefect_logging_setup(&#39;check_completed_transfer_to_monod&#39;)
    path_tmp_storage_server = Path(path_tmp_storage_server)
    flag_file_key_general = &#39;*_auto_&#39; + flag_file_key
    flag_file_key = &#39;_&#39; + flag_file_key
    flagged_files_list = list(path_tmp_storage_server.glob(flag_file_key_general))
    

    if flagged_files_list:
        flagged_file_path = flagged_files_list[0]
        experiment_name = (flagged_file_path.name).split(flag_file_key)[0]
        os.remove(flagged_file_path)
        logger.info(f&#39;{experiment_name} ready to be processed&#39;)
        return flagged_file_path.parent / experiment_name
    else:
        logger.info(f&#39;No new experiments to be processed&#39;)
        skip_signal = signals.SKIP(&#34;No new experiments to be processed&#34;)
        skip_signal.flag = True
        skip_signal.value = None
        raise skip_signal</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.collect_extra_files"><code class="name flex">
<span>def <span class="ident">collect_extra_files</span></span>(<span>experiment_fpath: str, experiment_info: Dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Function used to collect extra files required for the processing
- instrument_name_dark_img for field correction
- codebook if the experiment is barcoded</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name=&#39;upload-extra-files&#39;)
def collect_extra_files(experiment_fpath:str, experiment_info:Dict):
    &#34;&#34;&#34;
    Function used to collect extra files required for the processing
    - instrument_name_dark_img for field correction
    - codebook if the experiment is barcoded

    &#34;&#34;&#34;
    logger = prefect_logging_setup(&#39;collect_extra_files&#39;)
    experiment_fpath = Path(experiment_fpath)

    try:
        machine = experiment_info[&#39;Machine&#39;]
    except NameError:
        machine = &#39;NOT_DEFINED&#39;

    dark_img_fpath = experiment_fpath.parent / &#39;dark_imgs&#39; / (experiment_info[&#39;Machine&#39;] + &#39;_dark_img.npy&#39;)
    try:
        shutil.copy(dark_img_fpath, (experiment_fpath / &#39;extra_processing_data&#39;))
    except FileNotFoundError:
        logger.error(&#39;missing dark image&#39;)
        raise signals.FAIL(&#39;missing dark image&#39;)
    else:
        processing_env_config_fpath = experiment_fpath.parent / &#39;config_db&#39; / &#39;processing_env_config.yaml&#39;
        try:
            shutil.copy(processing_env_config_fpath, (experiment_fpath / &#39;pipeline_config&#39;))
        except FileNotFoundError:
            logger.error(&#39;missing pipeline env config file&#39;)
            raise signals.FAIL(&#39;missing pipeline env config file&#39;)
        else:
            probes_fpath = experiment_fpath.parent / &#39;probes_sets&#39; / (experiment_info[&#39;Probes&#39;] + &#39;.fasta&#39;)
            try:
                shutil.copy(probes_fpath, (experiment_fpath / &#39;probes&#39;))
            except FileNotFoundError:
                logger.error(&#39;missing probes set file&#39;)
                raise signals.FAIL(&#39;missing probes set file&#39;)
            else:
                if &#39;barcoded&#39; in experiment_info[&#39;Experiment_type&#39;]:
                    codebooks_folder = experiment_fpath.parent / &#39;codebooks&#39;
                    codebook_code = experiment_info[&#39;Codebook&#39;]
                    
                    codebook_fpath = codebooks_folder / (codebook_code + &#39;_codebook.parquet&#39;)
                    
                    # Create codebook folder in the experiment folder
                    try:
                        os.stat(experiment_fpath / &#39;codebook&#39; )
                        logger.info(f&#39;codebook folder already exist&#39;)
                    except FileNotFoundError:
                        os.mkdir(experiment_fpath / &#39;codebook&#39;)
                        os.chmod(experiment_fpath / &#39;codebook&#39;,0o777)
                    try:
                        shutil.copy(codebook_fpath, (experiment_fpath / &#39;codebook&#39;))
                    except FileNotFoundError:
                        logger.error(&#39;codebook is missing&#39;)
                        raise signals.FAIL(&#39;codebook is missing&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.consolidate_zarr_metadata"><code class="name flex">
<span>def <span class="ident">consolidate_zarr_metadata</span></span>(<span>parsed_raw_data_fpath: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to consolidate all the zarr metadata in one unique
json file for eady indexing and searching</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>parsed_raw_data_fpath</code></strong></dt>
<dd>str
path to the file with all the parsed images</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>consolidated_grp</code></dt>
<dd>zarr group
zarr groups instance with the consolidated metadata</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name = &#39;consolidate-metadata&#39;)
def consolidate_zarr_metadata(parsed_raw_data_fpath:str):
    &#34;&#34;&#34;
    Function to consolidate all the zarr metadata in one unique
    json file for eady indexing and searching

    Args:
        parsed_raw_data_fpath: str
            path to the file with all the parsed images
    
    Returns:
        consolidated_grp: zarr group
            zarr groups instance with the consolidated metadata
    &#34;&#34;&#34;

    logger = prefect_logging_setup(f&#39;consolidate-metadata&#39;)
    
    try:
        store = zarr.DirectoryStore(parsed_raw_data_fpath)
        consolidated_grp = zarr.consolidate_metadata(store)
    except:
        logger.error(f&#39;cannot consolidate metadata of the parsed zarr file&#39;)
        signals.FAIL(f&#39;cannot consolidate metadata of the parsed zarr file&#39;)
    
    else:
        return consolidated_grp</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.create_empty_zarr_file"><code class="name flex">
<span>def <span class="ident">create_empty_zarr_file</span></span>(<span>experiment_fpath: str, tag: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Function that create and empty zarr file </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>experiment_fpath</code></strong></dt>
<dd>str
location of the folder to be processed</dd>
<dt><strong><code>tag</code></strong></dt>
<dd>str
tag to add to the file name</dd>
</dl>
<h2 id="return">Return</h2>
<p>empty_fpath: str
path of the created file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name=&#39;create_empty_zarr_file&#39;)
def create_empty_zarr_file(experiment_fpath:str,tag:str)-&gt; str:
    &#34;&#34;&#34;
    Function that create and empty zarr file 

    Args:
        experiment_fpath: str
            location of the folder to be processed
        tag: str
            tag to add to the file name
    Return:
        empty_fpath: str
            path of the created file

    &#34;&#34;&#34;
    
    logger = prefect_logging_setup(f&#39;create empty {tag} file&#39;)
    experiment_fpath = Path(experiment_fpath)
    experiment_name = experiment_fpath.stem
    zarr_fpath = experiment_fpath / (experiment_name + &#39;_&#39; + tag + &#39;.zarr&#39;)
    
    dataset = xr.Dataset()
    try:
        dataset.to_zarr(zarr_fpath, mode=&#39;w&#39;, consolidated=True)
    except:
        logger.error(f&#39;cannot create {tag} file&#39;)
        signals.FAIL(f&#39;cannot create {tag} file&#39;)
    else:
        return zarr_fpath</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.create_folder_structure"><code class="name flex">
<span>def <span class="ident">create_folder_structure</span></span>(<span>experiment_fpath: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Function used to create the folder structure where to sort the files
generated by the machines and the saving the data created during the
processing. It creates the backbone structure common to all analysis</p>
<p>original_robofish_logs: contains all the original robofish logs.
extra_files: contains the extra files acquired during imaging.
extra_processing_data: contains extra files used in the analysis
like the dark images for flat field correction.
pipeline_config: contains all the configuration files.
raw_data: contains the renamed .nd2 files and the corresponding
pickle configuration files. It is the directory that is
backed up on the server.
output_figures: contains the reports and visualizations
notebooks: will contain potential notebooks used for processing the data
probes: will contains the fasta file with the probes used in the experiment
tmp: save temporary data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>experiment_fpath</code></strong></dt>
<dd>str
folder path of the experiment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name=&#39;create_folder_structure&#39;)
def create_folder_structure(experiment_fpath:str):
    &#34;&#34;&#34;
    Function used to create the folder structure where to sort the files
    generated by the machines and the saving the data created during the
    processing. It creates the backbone structure common to all analysis

    original_robofish_logs: contains all the original robofish logs.
        extra_files: contains the extra files acquired during imaging.
        extra_processing_data: contains extra files used in the analysis 
                                                                                                like the dark images for flat field correction.
    pipeline_config: contains all the configuration files.
    raw_data: contains the renamed .nd2 files and the corresponding 
                                                pickle configuration files. It is the directory that is 
                                                backed up on the server.
        output_figures: contains the reports and visualizations
    notebooks: will contain potential notebooks used for processing the data
    probes: will contains the fasta file with the probes used in the experiment
    tmp: save temporary data
    

    Args:
        experiment_fpath: str
            folder path of the experiment
    &#34;&#34;&#34;
    logger = prefect_logging_setup(&#39;created_raw_tmp_dir&#39;)
    experiment_fpath = Path(experiment_fpath)
    folders_list = [&#39;raw_data&#39;,
                    &#39;original_robofish_logs&#39;,
                    &#39;extra_processing_data&#39;,
                    &#39;extra_files&#39;,
                    &#39;pipeline_config&#39;,
                    &#39;output_figures&#39;,
                    &#39;notebooks&#39;,
                    &#39;probes&#39;,
                    &#39;tmp&#39;]
    for folder_name in folders_list:
        try:
            os.stat(experiment_fpath / folder_name )
            logger.info(f&#39;{folder_name} already exist&#39;)
        except FileNotFoundError:
            os.mkdir(experiment_fpath / folder_name)
            os.chmod(experiment_fpath / folder_name,0o777)</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.load_data_array"><code class="name flex">
<span>def <span class="ident">load_data_array</span></span>(<span>input_tuple)</span>
</code></dt>
<dd>
<div class="desc"><p>Function used to load the images out memory as xarray data array</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_tuple</code></strong></dt>
<dd>tuple</dd>
</dl>
<p>contains the path to the zarr file to process (str) and the
number of the fov to analyse</p>
<h2 id="return">Return</h2>
<p>img_data_array: xarray data array
data array with the image and all the required metadata</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name = &#39;load_data_array&#39;)
def load_data_array(input_tuple):
    &#34;&#34;&#34;
    Function used to load the images out memory as xarray data array
    
    Args:
        input_tuple: tuple
        contains the path to the zarr file to process (str) and the
        number of the fov to analyse

    Return:
        img_data_array: xarray data array
        data array with the image and all the required metadata
    &#34;&#34;&#34;
    logger = prefect_logging_setup(&#39;load-data-array&#39;)
    zarr_store, fov_num = input_tuple

    try:
        dataset = xr.open_zarr(zarr_store)
    except:
        logger.error(f&#39;cannot load the dataset, check the zarr store {zarr_store}&#39;)
        signals.FAIL(f&#39;cannot load the dataset, check the zarr store {zarr_store}&#39;)
    else:
        try:
            img_data_array = dataset[str(fov_num)]
        except:
            logger.error(f&#39;cannot load the data array, check if fov {fov_num} is missing&#39;)
            signals.FAIL(f&#39;cannot load the data array, check if fov {fov_num} is missing&#39;)
        else:
            return img_data_array</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.load_raw_images"><code class="name flex">
<span>def <span class="ident">load_raw_images</span></span>(<span>zarr_grp_name: str, parsed_raw_data_fpath: str) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Function used to load a raw image and metadata from the
parsed raw file and the attrs for the filtering
parsed_raw_data_fpath: str
fpath to zarr store containing the parsed raw images
zarr_grp_name: str
fpath to the group to process. The group contain the raw images and the
corresponding metadata</p>
<pre><code>    grp = experiment_name_channel_fov_X
        dataset = raw_data_fov_X
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name = &#39;load-raw-images-and-filtering-attrs&#39;)
def load_raw_images(zarr_grp_name:str,parsed_raw_data_fpath:str)-&gt;np.ndarray:
    &#34;&#34;&#34;
    Function used to load a raw image and metadata from the 
    parsed raw file and the attrs for the filtering
        parsed_raw_data_fpath: str
            fpath to zarr store containing the parsed raw images
        zarr_grp_name: str
            fpath to the group to process. The group contain the raw images and the 
            corresponding metadata

            grp = experiment_name_channel_fov_X
                dataset = raw_data_fov_X

    &#34;&#34;&#34;
    logger = prefect_logging_setup(f&#39;consolidate-metadata&#39;)
    st = zarr.DirectoryStore(parsed_raw_data_fpath)
    root = zarr.group(store=st,overwrite=False)

    metadata = root[zarr_grp_name].attrs
    img = root[zarr_grp_name][metadata[&#39;fov_name&#39;]][...]
    return (img, dict(metadata))</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.move_data"><code class="name flex">
<span>def <span class="ident">move_data</span></span>(<span>path_source_location: str, path_destination: str, flag_file_key: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Function used to transfer the files to another location</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path_source_location</code></strong></dt>
<dd>str
path to the data to be moved</dd>
<dt><strong><code>path_destination</code></strong></dt>
<dd>str
path to the destination of the transfer</dd>
<dt><strong><code>flag_file_key</code></strong></dt>
<dd>str</dd>
</dl>
<p>string that define the flag_files. The flag key should not have <em>auto</em>
in the name.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>experiment_path</code></dt>
<dd>Posix
path to the data after transferring</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name=&#39;move_data&#39;)
def move_data(path_source_location:str,path_destination:str, flag_file_key:str):
    &#34;&#34;&#34;
    Function used to transfer the files to another location

    Args:
        path_source_location: str
            path to the data to be moved
        path_destination: str
            path to the destination of the transfer
        flag_file_key: str
        string that define the flag_files. The flag key should not have _auto_
        in the name.


    Returns:
        experiment_path: Posix
            path to the data after transferring
    &#34;&#34;&#34;

    logger = prefect_logging_setup(&#39;transfer_data&#39;)
    path_source_location = Path(path_source_location)
    path_destination = Path(path_destination)

    try:
        os.stat(path_source_location)
    except:
        logger.error(f&#39; The {path_source_location} directory is missing&#39;)
        fail_signal = signals.FAIL(&#39;The source directory is missing&#39;)
        fail_signal.flag = True
        fail_signal.value = None
        raise fail_signal
    
    if os.stat(path_destination):
        os.stat(path_destination)
        shutil.move(path_source_location.as_posix(),path_destination.as_posix())
        tag_file_name = path_destination / (path_source_location.stem + &#39;_&#39; + flag_file_key)
        open(tag_file_name,&#39;w&#39;).close()
        logger.info(f&#39;data moved from {path_source_location} to {path_destination}&#39;)
    else:
        logger.info(f&#39; The {path_destination} directory is missing&#39;)
        fail_signal = signals.FAIL(&#39;The destination directory is missing&#39;)
        fail_signal.flag = True
        fail_signal.value = None
        raise fail_signal</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.sort_data_folder"><code class="name flex">
<span>def <span class="ident">sort_data_folder</span></span>(<span>experiment_fpath: str, experiment_info: Dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Function used to sort the data in the experiment folder in the
subfolders</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>experiment_fpath</code></strong></dt>
<dd>str
location of the folder to be processed</dd>
<dt><strong><code>experiment_info</code></strong></dt>
<dd>ordered dict
ordered dict with the parsed info generated by the instrument</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name = &#39;sort-data-folder&#39;)
def sort_data_folder(experiment_fpath:str,experiment_info:Dict):
    &#34;&#34;&#34;
    Function used to sort the data in the experiment folder in the
    subfolders
    
    Args:
        experiment_fpath: str
            location of the folder to be processed
        experiment_info: ordered dict
            ordered dict with the parsed info generated by the instrument
    
    &#34;&#34;&#34;
    logger = prefect_logging_setup(&#39;sort-data-folder&#39;)
    experiment_fpath = Path(experiment_fpath)

    # Move the robofish generated logs
    robofish_logs = experiment_fpath.glob(&#39;*.log&#39;)
    for log in robofish_logs:
        shutil.move(log.as_posix(), (experiment_fpath / &#39;original_robofish_logs&#39;).as_posix())

    # Move the crosses images
    crosses_files = experiment_fpath.glob(&#39;*_cross.nd2&#39;)
    for cross in crosses_files:
        shutil.move(cross.as_posix(), (experiment_fpath / &#39;extra_files&#39;).as_posix())

    # Move the coords files
    coords_files = experiment_fpath.glob(&#39;*.xml&#39;)
    for coord in coords_files:
        shutil.move(coord.as_posix(), (experiment_fpath / &#39;extra_files&#39;).as_posix())

    # Move the fresh nuclei file for eel
    if &#39;eel&#39; in experiment_info[&#39;Experiment_type&#39;]:
        nuclei_files = list(experiment_fpath.glob(&#39;*ChannelDAPI*&#39;))
        if len(nuclei_files):
            try:
                os.stat(experiment_fpath / &#39;fresh_nuclei&#39; )
                logger.info(f&#39;fresh_nuclei already exist&#39;)
            except FileNotFoundError:
                os.mkdir(experiment_fpath / &#39;fresh_nuclei&#39;)
                os.chmod(experiment_fpath / &#39;fresh_nuclei&#39;,0o777)
            
            for nuclei in nuclei_files:
                shutil.move(nuclei.as_posix(), (experiment_fpath / &#39;fresh_nuclei&#39;).as_posix())
        else:
            logger.info(f&#39;The experiment do not have images of fresh nuclei&#39;)</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.sorting_grps"><code class="name flex">
<span>def <span class="ident">sorting_grps</span></span>(<span>grps, experiment_info, analysis_parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>Function used to separate the group names according to
the processing type</p>
<pre><code>grps = zarr.hierarchy.Group
    consolidate hierarchy zarr groups for fast access to metadata
experiment_info: ordered dict
    ordered dict with the parsed info generated by the instrument
analysis_parameters: dict
    dict with all the parameters to use for analysis
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name=&#39;sort-images-according-processing-type&#39;)
def sorting_grps(grps, experiment_info, analysis_parameters):
    &#34;&#34;&#34;
    Function used to separate the group names according to 
    the processing type

        grps = zarr.hierarchy.Group
            consolidate hierarchy zarr groups for fast access to metadata
        experiment_info: ordered dict
            ordered dict with the parsed info generated by the instrument
        analysis_parameters: dict
            dict with all the parameters to use for analysis  
    &#34;&#34;&#34;

    fish_grp = []
    beads_grp = []
    staining_grp = []
    for name, grp in grps.items():
        if grp.attrs[&#39;stitching_channel&#39;] == grp.attrs[&#39;channel&#39;]:
            beads_grp.append(name)
        elif &#39;_ST&#39; in grp.attrs[&#39;target_name&#39;]:
            staining_grp.append(name)
        else:
            fish_grp.append(name)

    logger = prefect_logging_setup(f&#39;sort-groups&#39;)
    if experiment_info[&#39;Stitching_type&#39;] == &#39;small-beads&#39;:
        beads_selected_parameters  = analysis_parameters[&#39;small-beads&#39;]
        beads_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    elif experiment_info[&#39;Stitching_type&#39;] == &#39;large-beads&#39;:
        beads_selected_parameters  = analysis_parameters[&#39;small-beads&#39;]
        beads_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    else:
        logger.error(f&#39;Wrong stitching type in the experiment file&#39;)
        err = signals.FAIL(f&#39;Wrong stitching type in the experiment file&#39;)
        raise err
    
    fish_selected_parameters = analysis_parameters[&#39;fish&#39;]
    fish_selected_parameters[&#39;BarcodesExtractionResolution&#39;] = analysis_parameters[&#39;BarcodesExtractionResolution&#39;]
    fish_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]

    staining_selected_parameters = analysis_parameters[&#39;staining&#39;]
    staining_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]

    return fish_grp, fish_selected_parameters, beads_grp, beads_selected_parameters, staining_grp, staining_selected_parameters</code></pre>
</details>
</dd>
<dt id="pysmFISH.utilities_tasks.sorting_grps_fov"><code class="name flex">
<span>def <span class="ident">sorting_grps_fov</span></span>(<span>grps, experiment_info, analysis_parameters)</span>
</code></dt>
<dd>
<div class="desc"><p>Function used to separate the group names according to
the processing type and by fov to reduce the number of calls
to the writing.</p>
<h2 id="args">Args:</h2>
<pre><code>grps = zarr.hierarchy.Group
    consolidate hierarchy zarr groups for fast access to metadata
experiment_info: ordered dict
    ordered dict with the parsed info generated by the instrument
analysis_parameters: dict
    dict with all the parameters to use for analysis
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@task(name=&#39;sort-images-according-processing-type&#39;)
def sorting_grps_fov(grps, experiment_info, analysis_parameters):
    &#34;&#34;&#34;
    Function used to separate the group names according to 
    the processing type and by fov to reduce the number of calls
    to the writing.

    Args:
    -----
        grps = zarr.hierarchy.Group
            consolidate hierarchy zarr groups for fast access to metadata
        experiment_info: ordered dict
            ordered dict with the parsed info generated by the instrument
        analysis_parameters: dict
            dict with all the parameters to use for analysis  
    &#34;&#34;&#34;

    fish_grp = {}
    beads_grp = {}
    staining_grp = []
    for name, grp in grps.items():
        if grp.attrs[&#39;stitching_channel&#39;] == grp.attrs[&#39;channel&#39;]:
            if grp.attrs[&#39;channel&#39;] not in beads_grp.keys():
                beads_grp[grp.attrs[&#39;channel&#39;]] = {}
            if grp.attrs[&#39;fov_num&#39;] not in beads_grp[grp.attrs[&#39;channel&#39;]].keys():
                beads_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]] = []
                beads_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]].append(name)
            else:
                beads_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]].append(name)
        elif &#39;_ST&#39; in grp.attrs[&#39;target_name&#39;]:
            staining_grp.append(name)
        else:
            if grp.attrs[&#39;channel&#39;] not in fish_grp.keys():
                fish_grp[grp.attrs[&#39;channel&#39;]] = {}
            if  grp.attrs[&#39;fov_num&#39;] not in fish_grp[grp.attrs[&#39;channel&#39;]].keys():
                fish_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]] = []
                fish_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]].append(name)
            else:
                fish_grp[grp.attrs[&#39;channel&#39;]][grp.attrs[&#39;fov_num&#39;]].append(name)

    fish_list = []
    beads_list = []
    for channel in fish_grp.keys():
        for fov, names in fish_grp[channel].items():
            fish_list.append(names)
    
    if len(fish_grp.keys()) &gt; 1:
        fish_list = [el for sgr in fish_list for el in sgr]
    
    for channel in beads_grp.keys():
        for fov, names in beads_grp[channel].items():
            beads_list.append(names)
    
    if len(beads_grp.keys()) &gt; 1:
        beads_list = [el for sgr in beads_list for el in sgr]


    logger = prefect_logging_setup(f&#39;sort-groups&#39;)
    if experiment_info[&#39;Stitching_type&#39;] == &#39;small-beads&#39;:
        beads_selected_parameters  = analysis_parameters[&#39;small-beads&#39;]
        beads_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    elif experiment_info[&#39;Stitching_type&#39;] == &#39;large-beads&#39;:
        beads_selected_parameters  = analysis_parameters[&#39;small-beads&#39;]
        beads_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]
    else:
        logger.error(f&#39;Wrong stitching type in the experiment file&#39;)
        err = signals.FAIL(f&#39;Wrong stitching type in the experiment file&#39;)
        raise err
    
    fish_selected_parameters = analysis_parameters[&#39;fish&#39;]
    fish_selected_parameters[&#39;BarcodesExtractionResolution&#39;] = analysis_parameters[&#39;BarcodesExtractionResolution&#39;]
    fish_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]

    staining_selected_parameters = analysis_parameters[&#39;staining&#39;]
    staining_selected_parameters[&#39;RegistrationReferenceHybridization&#39;] = analysis_parameters[&#39;RegistrationReferenceHybridization&#39;]

    logger.info(f&#39;fish group {fish_list}&#39;)
    logger.info(f&#39;beads group {beads_list}&#39;)

    return fish_list, fish_selected_parameters, beads_list, beads_selected_parameters, staining_grp, staining_selected_parameters</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysmFISH" href="index.html">pysmFISH</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pysmFISH.utilities_tasks.check_completed_transfer_to_monod" href="#pysmFISH.utilities_tasks.check_completed_transfer_to_monod">check_completed_transfer_to_monod</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.collect_extra_files" href="#pysmFISH.utilities_tasks.collect_extra_files">collect_extra_files</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.consolidate_zarr_metadata" href="#pysmFISH.utilities_tasks.consolidate_zarr_metadata">consolidate_zarr_metadata</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.create_empty_zarr_file" href="#pysmFISH.utilities_tasks.create_empty_zarr_file">create_empty_zarr_file</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.create_folder_structure" href="#pysmFISH.utilities_tasks.create_folder_structure">create_folder_structure</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.load_data_array" href="#pysmFISH.utilities_tasks.load_data_array">load_data_array</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.load_raw_images" href="#pysmFISH.utilities_tasks.load_raw_images">load_raw_images</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.move_data" href="#pysmFISH.utilities_tasks.move_data">move_data</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.sort_data_folder" href="#pysmFISH.utilities_tasks.sort_data_folder">sort_data_folder</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.sorting_grps" href="#pysmFISH.utilities_tasks.sorting_grps">sorting_grps</a></code></li>
<li><code><a title="pysmFISH.utilities_tasks.sorting_grps_fov" href="#pysmFISH.utilities_tasks.sorting_grps_fov">sorting_grps_fov</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>