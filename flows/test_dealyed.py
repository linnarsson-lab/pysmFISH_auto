import prefect
from prefect import task, Flow, Parameter, flatten, unmapped
from prefect.engine.executors import DaskExecutor
from prefect.environments import LocalEnvironment
from prefect import Task
from prefect.environments.storage import Local

from pysmFISH.configuration_files_tasks import load_experiment_config_file
from pysmFISH.data_model import shoji_db_fish
from pysmFISH.io import load_analysis_parameters
from pysmFISH.utilities_tasks import consolidate_zarr_metadata
from pysmFISH.utilities_tasks import sorting_grps
from pysmFISH.prefect_tasks import single_fish_filter_count
from pysmFISH.prefect_tasks import single_beads_filter_count

from pysmFISH.dask_cluster_utilities_tasks import start_processing_env
import dask
from dask.distributed import Client

from pysmFISH.utilities_tasks import open_consolidated_metadata

from prefect.utilities.debug import raise_on_exception
from pysmFISH.dask_cluster_utilities_tasks import start_processing_env
from pysmFISH.configuration_files_tasks import load_processing_env_config_file, load_experiment_config_file

def fc(zarr_grp_name,
            parsed_raw_data_fpath,
            FlatFieldKernel,
            FilteringSmallKernel, 
            LaplacianKernel,
            min_distance,
            min_obj_size,
            max_obj_size,
            num_peaks_per_label):

    fish_counter = single_fish_filter_count()
    filtered_fish_images_metadata = fish_counter.run(zarr_grp_name,
                            parsed_raw_data_fpath,
                            FlatFieldKernel,
                            FilteringSmallKernel,
                            LaplacianKernel,
                            min_distance,
                            min_obj_size,
                            max_obj_size,
                            num_peaks_per_label)




experiment_fpath = '/wsfish/smfish_ssd/AMEXP20201110_EEL_HumanH1930001V1C_auto'
parsed_raw_data_fpath = '/wsfish/smfish_ssd/AMEXP20201110_EEL_HumanH1930001V1C_auto/AMEXP20201110_EEL_HumanH1930001V1C_auto_img_data.zarr'
config_db_fpath ='/wsfish/smfish_ssd/config_db'

processing_env_config = load_processing_env_config_file(config_db_fpath)

# Load experiment configuration file generated by robofish machines
load_exp_cfg = load_experiment_config_file(experiment_fpath)
experiment_info = load_exp_cfg.run(experiment_fpath) 
    
#     # Create the shoji database that will contain the data
#     create_db = shoji_db_fish()
#     ref = create_db(experiment_info)
    
# Get the list of raw image groups to preprocess
analysis_loader = load_analysis_parameters()
analysis_parameters = analysis_loader.run(experiment_info['EXP_name'])

#     # Consolidate zarr metadata
#     # consolidator = consolidate_zarr_metadata()
#     # consolidated_zarr_grp = consolidator(parsed_raw_data_fpath)
#     # consolidated_zarr_grp.set_upstream([parsed_raw_data_fpath])
    
    # To avoid time for consolidate data
consolidated_zarr_grp = open_consolidated_metadata(parsed_raw_data_fpath)

#     # Sort the type of images according to processing
grp_sorter = sorting_grps()
sorted_grps = grp_sorter.run(consolidated_zarr_grp,experiment_info,analysis_parameters)

cluster = start_processing_env(processing_env_config,experiment_info)

client = Client(cluster)

futures = []
for el in sorted_grps[0][0:1000]:
    future = dask.delayed(fc)(el,
                            parsed_raw_data_fpath=parsed_raw_data_fpath,
                            FlatFieldKernel=sorted_grps[1]['PreprocessingFishFlatFieldKernel'],
                            FilteringSmallKernel=sorted_grps[1]['PreprocessingFishFilteringSmallKernel'],
                            LaplacianKernel=sorted_grps[1]['PreprocessingFishFilteringLaplacianKernel'],
                            min_distance=sorted_grps[1]['CountingFishMinObjDistance'],
                            min_obj_size=sorted_grps[1]['CountingFishMinObjSize'],
                            max_obj_size=sorted_grps[1]['CountingFishMaxObjSize'],
                            num_peaks_per_label=sorted_grps[1]['CountingFishNumPeaksPerLabel'])
    futures.append(future)
print(len(futures))
dask.compute(*futures)

client.close()
cluster.close()

#     beads_counter = single_beads_filter_count(task_run_name=lambda **kwargs: f"filtering-counting-{kwargs['zarr_grp_name']}")
#     filtered_beads_images_metadata = beads_counter.map(zarr_grp_name=sorted_grps[2],
#                             parsed_raw_data_fpath=unmapped(parsed_raw_data_fpath),
#                             FlatFieldKernel=unmapped(sorted_grps[3]['PreprocessingBeadsRegistrationFlatFieldKernel']),
#                             FilteringSmallKernel=unmapped(sorted_grps[3]['PreprocessingBeadsRegistrationFilteringSmallKernel']),
#                             LaplacianKernel=unmapped(sorted_grps[3]['PreprocessingBeadsRegistrationFilteringLaplacianKernel']),
#                             min_distance=unmapped(sorted_grps[3]['CountingBeadsRegistrationMinObjDistance']),
#                             min_obj_size=unmapped(sorted_grps[3]['CountingBeadsRegistrationMinObjSize']),
#                             max_obj_size=unmapped(sorted_grps[3]['CountingBeadsRegistrationMaxObjSize']),
#                             num_peaks_per_label=unmapped(sorted_grps[3]['CountingBeadsRegistrationNumPeaksPerLabel']))

#     # Add processing for staining images


# # with raise_on_exception():
# #     flow.run()

# flow.register(project_name="test")
# # flow.run_agent()