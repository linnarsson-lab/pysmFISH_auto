from typing import *
import os
import shutil
import yaml
import datetime
import xarray as xr
from pathlib import Path
from collections import OrderedDict

from prefect import task
from prefect.engine import signals

from pysmFISH.logger_utils import prefect_logging_setup


# to avoid reference for nested structures
# https://stackoverflow.com/questions/13518819/avoid-references-in-pyyaml (comment)
yaml.SafeDumper.ignore_aliases = lambda *args : True



@task(name='check_completed_transfer_to_monod')
def check_completed_transfer_to_monod(path_tmp_storage_server:str, flag_file_key:str):
    """
    Function to scan the folder where the data are transferred from the machines.
    It looks for files that are generated upon transfer completion (flag_file).
    To keep thing simple and not overload the system only one of the flag_files
    identified in the scanning of the folder will be processed.
    
    NB: In order to process only the experiments designed for the pipeline 
    the folder name finish with auto ExpName_auto

    Args: 
    path_tmp_storage_server: str
        path where the data are transferred from the microscopes
    flag_file_key: str
        string that define the flag_files. The flag key should not have _auto_
        in the name.


    Returns:
        experiment_path: Posix
            experiment path in the tmp folder
    """

    logger = prefect_logging_setup('check_completed_transfer_to_monod')
    path_tmp_storage_server = Path(path_tmp_storage_server)
    flag_file_key_general = '*_auto_' + flag_file_key
    flag_file_key = '_' + flag_file_key
    flagged_files_list = list(path_tmp_storage_server.glob(flag_file_key_general))
    

    if flagged_files_list:
        flagged_file_path = flagged_files_list[0]
        experiment_name = (flagged_file_path.name).split(flag_file_key)[0]
        logger.info(f'{experiment_name} ready to be processed')
        os.remove(flagged_file_path)
        return flagged_file_path.parent / experiment_name
    else:
        logger.info(f'No new experiments to be processed')
        skip_signal = signals.SKIP("No new experiments to be processed")
        skip_signal.flag = True
        skip_signal.value = None
        raise skip_signal


@task(name='load_experiments_info_file')
def load_experiment_config_file(experiment_fpath:str):
    """
    Function that load the experiment general information generated by the
    machines.

    Args:
        experiment_fpath: str
            location of the folder to be processed
    Return:
        experiment_info: ordered dict
            ordered dict with the parsed info generated by the instrument

    """
    
    logger = prefect_logging_setup('load_experiments_info_file')

    experiment_fpath = Path(experiment_fpath)
    experiment_name = experiment_fpath.stem
    search_key = experiment_name + '_config.yaml'
    
    try:
        experiment_info_fpath = list(experiment_fpath.glob(search_key))[0]
    except:
        logger.error(f'No experiment info file in {experiment_fpath}')
        fail_signal = signals.FAIL("No experiment info file in the folder")
        fail_signal.flag = True
        fail_signal.value = None
        raise fail_signal
    
    try:
        experiment_info = OrderedDict(yaml.safe_load(open(experiment_info_fpath, 'rb')))
        return experiment_info
    except:
        logger.error(f'Experiment info file has the wrong name in {experiment_fpath}')
        fail_signal = signals.FAIL("Experiment info file has the wrong name in the folder")
        fail_signal.flag = True
        fail_signal.value = None
        raise fail_signal



@task(name='create_folder_structure')
def create_folder_structure(experiment_fpath:str):
    """
    Function used to create the folder structure where to sort the files
    generated by the machines and the saving the data created during the
    processing. It creates the backbone structure common to all analysis

    original_robofish_logs: contains all the original robofish logs.
	extra_files: contains the extra files acquired during imaging.
	extra_processing_data: contains extra files used in the analysis 
												like the dark images for flat field correction.
    pipeline_config: contains all the configuration files.
    raw_data: contains the renamed .nd2 files and the corresponding 
						pickle configuration files. It is the directory that is 
						backed up on the server.
    parsed_tmp: temporary directory where to save parsed files
	output_figures: contains the reports and visualizations
    notebooks: will contain potential notebooks used for processing the data
    probes: will contains the fasta file with the probes used in the experiment
    tmp: save temporary data
    

    Args:
        experiment_fpath: str
            folder path of the experiment
    """
    logger = prefect_logging_setup('created_raw_tmp_dir')
    experiment_fpath = Path(experiment_fpath)
    folders_list = ['raw_data',
                    'parsed_tmp',
                    'original_robofish_logs',
                    'extra_processing_data',
                    'extra_files',
                    'pipeline_config',
                    'output_figures',
                    'notebooks',
                    'probes',
                    'tmp']
    for folder_name in folders_list:
        try:
            os.stat(experiment_fpath / folder_name )
            logger.info(f'{folder_name} already exist')
        except FileNotFoundError:
            os.mkdir(experiment_fpath / folder_name)
            os.chmod(experiment_fpath / folder_name,0o777)


@task(name='upload-extra-files')
def collect_extra_files(experiment_fpath:str, experiment_info:Dict):
    """
    Function used to collect extra files required for the processing
    - instrument_name_dark_img for field correction
    - codebook if the experiment is barcoded

    """
    logger = prefect_logging_setup('collect_extra_files')
    experiment_fpath = Path(experiment_fpath)
    dark_img_fpath = experiment_fpath.parent / 'dark_imgs' / (experiment_info['Machine'] + '_dark_img.npy')
    try:
        shutil.copy(dark_img_fpath, (experiment_fpath / 'extra_processing_data'))
    except FileNotFoundError:
        logger.error('missing dark image')
        raise signals.FAIL('missing dark image')
    else:
        probes_fpath = experiment_fpath.parent / 'probes_sets' / (experiment_info['probes'] + '.fasta')
        try:
            shutil.copy(probes_fpath, (experiment_fpath / 'probes'))
        except FileNotFoundError:
            logger.error('missing probes set file')
            raise signals.FAIL('missing probes set file')
        else:
            if 'barcoded' in experiment_info['experiment_type']:
                codebooks_folder = experiment_fpath.parent / 'codebooks'
                codebook_code = experiment_info['Codebook']
                
                codebook_fpath = codebooks_folder / (codebook_code + '_codebook.parquet')
                
                # Create codebook folder in the experiment folder
                try:
                    os.stat(experiment_fpath / 'codebook' )
                    logger.info(f'codebook folder already exist')
                except FileNotFoundError:
                    os.mkdir(experiment_fpath / 'codebook')
                    os.chmod(experiment_fpath / 'codebook',0o777)
                try:
                    shutil.copy(codebook_fpath, (experiment_fpath / 'codebook'))
                except FileNotFoundError:
                    logger.error('codebook is missing')
                    raise signals.FAIL('codebook is missing')


@task(name = 'load_data_array')
def load_data_array(input_tuple):
    """
    Function used to load the images out memory as xarray data array
    
    Args:
        input_tuple: tuple
        contains the path to the zarr file to process (str) and the
        number of the fov to analyse

    Return:
        img_data_array: xarray data array
        data array with the image and all the required metadata
    """
    logger = prefect_logging_setup('load-data-array')
    zarr_store, fov_num = input_tuple

    try:
        dataset = xr.open_zarr(zarr_store)
    except:
        logger.error(f'cannot load the dataset, check the zarr store {zarr_store}')
        signals.FAIL(f'cannot load the dataset, check the zarr store {zarr_store}')
    else:
        try:
            img_data_array = dataset[str(fov_num)]
        except:
            logger.error(f'cannot load the data array, check if fov {fov_num} is missing')
            signals.FAIL(f'cannot load the data array, check if fov {fov_num} is missing')
        else:
            return img_data_array
