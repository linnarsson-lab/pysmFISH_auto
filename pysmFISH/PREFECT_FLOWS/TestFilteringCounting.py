import prefect
from prefect import task, Flow, Parameter, flatten, unmapped, Task
from prefect.executors import DaskExecutor
from prefect.run_configs import LocalRun, UniversalRun

from pysmFISH.configuration_files_tasks import load_experiment_config_file
from pysmFISH.data_model import shoji_db_fish
from pysmFISH.io import load_analysis_parameters
from pysmFISH.utilities_tasks import consolidate_zarr_metadata
from pysmFISH.utilities_tasks import sorting_grps
from pysmFISH.prefect_tasks import single_fish_filter_count
from pysmFISH.prefect_tasks import single_beads_filter_count

from pysmFISH.utilities_tasks import open_consolidated_metadata

from prefect.utilities.debug import raise_on_exception

import time

@task
def chunking(data_list):
    chunk_size = 1000
    chunks = [data_list[x:x+chunk_size] for x in range(0, len(data), chunk_size)]
    return chunks

@task
def submap(data_list):
    pass

with Flow("test_filt_count",run_config=LocalRun(), 
    executor = DaskExecutor(address='tcp://193.10.16.58:19547',debug=True)) as flow:

    experiment_fpath = Parameter('experiment_fpath', default = '/wsfish/smfish_ssd/AMEXP20201110_EEL_HumanH1930001V1C_auto')
    parsed_raw_data_fpath = Parameter('parsed_raw_data_fpath',default='/wsfish/smfish_ssd/AMEXP20201110_EEL_HumanH1930001V1C_auto/AMEXP20201110_EEL_HumanH1930001V1C_auto_img_data.zarr')
     
    # Load experiment configuration file generated by robofish machines
    load_exp_cfg = load_experiment_config_file()
    experiment_info = load_exp_cfg(experiment_fpath) 
    
    # Create the shoji database that will contain the data
    create_db = shoji_db_fish()
    ref = create_db(experiment_info)
    
    # Get the list of raw image groups to preprocess
    analysis_loader = load_analysis_parameters()
    analysis_parameters = analysis_loader(experiment_info['EXP_name'])
    analysis_parameters.set_upstream([ref])

    # # Consolidate zarr metadata
    # consolidator = consolidate_zarr_metadata()
    # consolidated_zarr_grp = consolidator(parsed_raw_data_fpath)
    # consolidated_zarr_grp.set_upstream([parsed_raw_data_fpath])
    
    # To avoid time for consolidate data
    consolidated_zarr_grp = open_consolidated_metadata(parsed_raw_data_fpath)

    # Sort the type of images according to processing
    grp_sorter = sorting_grps()
    sorted_grps = grp_sorter(consolidated_zarr_grp,experiment_info,analysis_parameters)
    sorted_grps.set_upstream(consolidated_zarr_grp)

    
    #PORT
    # fish_chunks = chunking(sorted_grps[0],upstream_tasks=[sorted_grps])

    # for chunk in fish_chunks:
    fish_counter = single_fish_filter_count(task_run_name=lambda **kwargs: f"filtering-counting-{kwargs['zarr_grp_name']}")
    filtered_fish_images_metadata = fish_counter.map(zarr_grp_name=sorted_grps[0],
                            parsed_raw_data_fpath=unmapped(parsed_raw_data_fpath),
                            FlatFieldKernel=unmapped(sorted_grps[1]['PreprocessingFishFlatFieldKernel']),
                            FilteringSmallKernel=unmapped(sorted_grps[1]['PreprocessingFishFilteringSmallKernel']),
                            LaplacianKernel=unmapped(sorted_grps[1]['PreprocessingFishFilteringLaplacianKernel']),
                            min_distance=unmapped(sorted_grps[1]['CountingFishMinObjDistance']),
                            min_obj_size=unmapped(sorted_grps[1]['CountingFishMinObjSize']),
                            max_obj_size=unmapped(sorted_grps[1]['CountingFishMaxObjSize']),
                            num_peaks_per_label=unmapped(sorted_grps[1]['CountingFishNumPeaksPerLabel']))


    beads_counter = single_beads_filter_count(task_run_name=lambda **kwargs: f"filtering-counting-{kwargs['zarr_grp_name']}")
    filtered_beads_images_metadata = beads_counter.map(zarr_grp_name=sorted_grps[2],
                            parsed_raw_data_fpath=unmapped(parsed_raw_data_fpath),
                            FlatFieldKernel=unmapped(sorted_grps[3]['PreprocessingBeadsRegistrationFlatFieldKernel']),
                            FilteringSmallKernel=unmapped(sorted_grps[3]['PreprocessingBeadsRegistrationFilteringSmallKernel']),
                            LaplacianKernel=unmapped(sorted_grps[3]['PreprocessingBeadsRegistrationFilteringLaplacianKernel']),
                            min_distance=unmapped(sorted_grps[3]['CountingBeadsRegistrationMinObjDistance']),
                            min_obj_size=unmapped(sorted_grps[3]['CountingBeadsRegistrationMinObjSize']),
                            max_obj_size=unmapped(sorted_grps[3]['CountingBeadsRegistrationMaxObjSize']),
                            num_peaks_per_label=unmapped(sorted_grps[3]['CountingBeadsRegistrationNumPeaksPerLabel']))
    filtered_beads_images_metadata.set_upstream(filtered_fish_images_metadata)
    # Add processing for staining images


# with raise_on_exception():
#     flow.run()

# assert flow.reference_tasks() == {pr}
flow.register(project_name="test")
# flow.run_agent()