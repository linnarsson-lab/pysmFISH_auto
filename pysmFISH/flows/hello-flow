import prefect
from prefect import task, Flow, Parameter, flatten, unmapped
from prefect.engine.executors import DaskExecutor
from prefect.utilities.debug import raise_on_exception

from datetime import timedelta, datetime
from prefect.schedules import IntervalSchedule
from prefect.environments import RemoteDaskEnvironment,LocalEnvironment

from pysmFISH.dask_cluster_utilities_tasks import start_processing_env, local_cluster_setup
from pysmFISH.utilities_tasks import consolidate_zarr_metadata, sorting_grps
from pysmFISH.configuration_files_tasks import load_processing_env_config_file, load_experiment_config_file
from pysmFISH.io import load_analysis_parameters
from pysmFISH.prefect_tasks import single_fish_filter_count,single_beads_filter_count
from pysmFISH.microscopy_file_parsers_tasks import nd2_raw_files_selector_general
from pysmFISH.utilities_tasks import create_empty_zarr_file
from pysmFISH.data_model import create_shoji_db
from pysmFISH.microscopy_file_parsers_tasks import nikon_nd2_reparser_zarr

import logging
import time
from pathlib import Path
from prefect import Client
from prefect.utilities.debug import is_serializable
from prefect.engine import signals


experiment_fpath = '/wsfish/smfish_ssd/LBEXP20201014_EEL_Mouse_2420um_auto'

flag_file_key = Parameter('flag_file_key', default='transfer_to_monod_completed.txt')
processing_hd_location = Parameter('processing_hd_location',default='/wsfish/smfish_ssd')
# processing_hd_location = Parameter('processing_hd_location',default='/Users/simone/Documents/local_data_storage/prefect_test/whd')

# get info for submitting the error notification to github
config_db_fpath = processing_hd_location.default + '/config_db'
processing_env_config = load_processing_env_config_file(config_db_fpath)
experiment_info = load_experiment_config_file(experiment_fpath)
cluster = start_processing_env(processing_env_config,experiment_info)

experiment_info = load_experiment_config_file(experiment_fpath)
# executor = DaskExecutor(address=cluster.scheduler_address)
# print(cluster.scheduler_address)

# with Flow("thello-flow",environment=LocalEnvironment(executor=executor)) as flow:
with Flow("thello-flow",environment=LocalEnvironment(DaskExecutor(address='tcp://193.10.16.58:18938'))) as flow:
    
    experiment_fpath = Parameter('experiment_fpath',default=experiment_fpath)
    experiment_info = Parameter('experiment_info',default=experiment_info)

    # Create the shoji database that will contain the data
    ref = create_shoji_db(experiment_info)
    # Get the list of raw image groups to preprocess
    analysis_parameters = load_analysis_parameters(experiment_name=experiment_info['EXP_name'],upstream_tasks=[ref])
    

    # ----------------------------------------------------------------------------------------------------------------------
    # PARSE NIKON ND2 FILES

    # Reparsing .nd2 files stored in the raw_data subfolder
    # raw_files_fpath = Parameter('raw_files_fpath',default=(experiment_fpath.default + '/raw_data'))
    # all_raw_files = nd2_raw_files_selector_general(folder_fpath=raw_files_fpath,upstream_tasks=[raw_files_fpath])
    
    # # Run the crosscheck for all the pkl files
    # check_matching_metadata_robofish(all_raw_files)
    # report_input_files_errors(git_repo,experiment_fpath,git_token)

    # tag = 'img_data'
    # parsed_raw_data_fpath = create_empty_zarr_file(experiment_fpath,tag,upstream_tasks=[all_raw_files])
    # autoparser = nikon_nd2_reparser_zarr.map(nd2_file_path=all_raw_files,parsed_raw_data_fpath=unmapped(parsed_raw_data_fpath),
    #                             experiment_info=unmapped(experiment_info))
    
    # Code to use when testing without parsing
    parsed_raw_data_fpath = Parameter('parsed_raw_data_fpath',default='/wsfish/smfish_ssd/LBEXP20201014_EEL_Mouse_2420um_auto_img_data.zarr')
             
    consolidated_zarr_grp = consolidate_zarr_metadata(parsed_raw_data_fpath,upstream_tasks=[analysis_parameters])
    
    # When integrated with parsing
    # consolidated_zarr_grp = consolidate_zarr_metadata(parsed_raw_data_fpath,upstream_tasks=[autoparser])        
    # ----------------------------------------------------------------------------------------------------------------------


    # ----------------------------------------------------------------------------------------------------------------------
    # FILTERING AND COUNTING
    
    # Sort the type of images according to processing
    # Order of output from the sorting_grps:
    # fish_grp, fish_selected_parameters, beads_grp, beads_selected_parameters,\
    # staining_grp, staining_selected_parameters
    sorted_grps = sorting_grps(consolidated_zarr_grp,experiment_info,analysis_parameters,upstream_tasks=[consolidated_zarr_grp])

    filtered_fish_images_metadata = single_fish_filter_count.map(zarr_grp_name=sorted_grps[0][0:10],
                    parsed_raw_data_fpath=unmapped(parsed_raw_data_fpath),
                    experiment_fpath=unmapped(experiment_fpath),
                    FlatFieldKernel=unmapped(sorted_grps[1]['PreprocessingFishFlatFieldKernel']),
                    FilteringSmallKernel=unmapped(sorted_grps[1]['PreprocessingFishFilteringSmallKernel']),
                    LaplacianKernel=unmapped(sorted_grps[1]['PreprocessingFishFilteringLaplacianKernel']),
                    min_distance=unmapped(sorted_grps[1]['CountingFishMinObjDistance']),
                    min_obj_size=unmapped(sorted_grps[1]['CountingFishMinObjSize']),
                    max_obj_size=unmapped(sorted_grps[1]['CountingFishMaxObjSize']),
                    num_peaks_per_label=unmapped(sorted_grps[1]['CountingFishNumPeaksPerLabel']))



# execution_env = LocalEnvironment(executor=executor)
#flow.environment = RemoteDaskEnvironment(cluster.scheduler_address)
flow.register(project_name="test")
# flow_state = flow.run(executor=executor)
# from prefect.agent.local.agent import LocalAgent
# agent = LocalAgent(name='peppino')
# agent.start()
# flow.run_agent()

