# import logging
# import pickle
# import subprocess
# import lxml.etree as etree
# import argparse
# import yaml
# import os
# import re
# import sys
# import time
# import shutil
# import numpy as np
# import zarr
# import sys
# from typing import *
from pathlib import Path
# from itertools import product
# from collections import OrderedDict


# from pysmFISH.utils import load_pipeline_config_file, create_dir, load_running_analysis_config_file
# from nd2reader import ND2Reader

""" 
The parsing of the nikon files require the bftools. We use only the inf
command and the location is inferred using os.system('inf')
bftool 
"""


def nd2_processing_files_selector(experiment_fpath: str) -> list:
    """
    Identify the nd2 raw microscopy files generated by
    the robofish machine. The files must contain CountXXXXX in the name. 

    Args:
        experiment_fpath: str 
            Path to the folder to process. It need to contain the '_auto'
            suffix in order to be process with the automated pipeline

    Returns:
        all_files_to_process: list
            List of PosixPath of the microscopy files to process
        
    """

    assert '_auto' in experiment_fpath, 'no _auto in the experiment name'

    experiment_fpath = Path(experiment_fpath)
    searching_key = '*Count*.nd2'
    all_files_to_process = list(experiment_fpath.glob(searching_key))

    assert all_files_to_process, 'no .nd2 raw files to process'
    
    return all_files_to_process




# class nikon_nd2_autoparser():
#     """
#     This parser not consider the possibility to have multiple experiment running at
#     the same time. In addition the data once are parsed by hybridization are saved in
#     the same folder. The parser will used the new set of configuration files that will be
#     copied in a config folder inside the experiment folder. 

#     This nd2 parser process one .nd2 file

#     Attributes
#     ----------
#     nd2_file_path: str
#         Path to the .nd2 file to be parsed

#     """

#     def __init__(self,nd2_file_path: str):
        
#         self.nd2_file_path = Path(nd2_file_path)
#         self.logger = logging.getLogger(__name__)

#         if 'raw_data' in str(self.nd2_file_path):
#             self.experiment_fpath = self.nd2_file_path.parent.parent
#         else:
#             self.experiment_fpath = self.nd2_file_path.parent


#         self.experiment_config = load_pipeline_config_file(self.experiment_fpath / 'pipeline_config' / 'experiment.yaml')
#         self.logger.info(f"loaded all metadata")

#         self.experiment_name = self.experiment_config['experiment_name']
#         self.logger.info(f"experiment_name f{self.experiment_name}")
#         self.running_analysis_config = load_running_analysis_config_file(self.experiment_fpath)

#         self.analysis_name = self.running_analysis_config['analysis_name']
#         self.logger.info(f"analysis_name f{self.analysis_name}")

#         # Create the directory where to store the raw files that need to be transfer
#         # for backup
#         self.raw_files_dir = self.experiment_fpath / 'raw_data'
#         # create_dir(self.raw_files_dir)
#         # self.logger.info(f"raw files dir {self.raw_files_dir}")

#         # Create the directory where to save the tmp parsed files
#         # zarr files organized by round. it will contain also the tmp config files.
#         self.parsed_tmp = self.experiment_fpath / 'parsed_tmp'
       
#         if self.running_analysis_config['processing_engine'] == 'htcondor':
#             try:
#                 self.ram = float(self.running_analysis_config['dask_clusters_htcondor']['parse_nd2_files_cluster_setup']['memory'].split('GB')[0])*1000000000
#                 self.ram = self.ram*0.9 # to avoid crushing of dask workers ~/.config/dask/distributed.yaml 
#             except:
#                 self.logger.error('missing ram definition')
#         elif self.running_analysis_config['processing_engine'] == 'local':
#             self.ram = float(self.running_analysis_config['dask_local_cluster']['parse_nd2_files_cluster_setup']['memory'].split('GB')[0])*1000000000

#         # Add condition for processing on local HD
#         self.parsing_option = self.running_analysis_config['pipeline_steps']['parse_nd2_files']
#         if  self.parsing_option == 'original':
#             self.reparsing = False
#             # collect the count code
#             try:
#                 self.count_code = re.search(r'(Count)\d{5}', self.nd2_file_path.stem).group()
#             except:
#                 self.count_code = None
#                 self.logger.error(f'Cannot find the robofish logs for {self.nd2_file_path.stem}')
#                 sys.exit(f'Cannot find the robofish logs for {self.nd2_file_path.stem}')

#         elif self.parsing_option == 'reparsing':
#             self.reparsing = True
#         else:
#             self.logger.error('wrong selection criteria. It must be original or reparsing')
#             sys.exit('wrong selection criteria. It must be original or reparsing')


#     def extract_nd2_metadata(self):
#         # Load the file
#         with ND2Reader(str(self.nd2_file_path)) as nd2f_hdl:
#             self.nd2_file_metadata = nd2f_hdl.metadata
#             self.image_channel = self.nd2_file_metadata['channels'][0] # Works because each file contains only one image

#     def retrieve_info_file_dict(self):
#         all_info_files = self.experiment_fpath.glob('*.pkl')
#         try:
#             self.info_file = [info_file for info_file in all_info_files if self.count_code  in info_file.stem][0]
#         except IndexError:
#             sys.exit('info file missing')

#         self.info_data = pickle.load(open(self.info_file, 'rb'))
#         self.hybridization_name = self.info_data['channels']['Hybridization']
#         self.tag_name = self.experiment_name + '_' + self.hybridization_name + '_' + self.image_channel
        
    
#     def retrieve_info_file_dict_reparsing(self):
#         nd2_fname = self.nd2_file_path.stem
#         self.info_file = self.raw_files_dir / (nd2_fname + '_info.pkl')
#         try:
#             self.info_data = pickle.load(open(self.info_file, 'rb'))
#         except IndexError:
#             sys.exit('info file missing')

#         self.hybridization_name = self.info_data['channels']['Hybridization']
#         self.tag_name = self.experiment_name + '_' + self.hybridization_name + '_' + self.image_channel


#     def extract_omexml_tree(self):

#         """
#         Function that extract the metadata using the 
#         ome bftools that output and omexml format

#         Parameters:
#         -----------
#         input_image_path: Path
#             Path to the .nd2 image file that needs to be
#             processed.

#         """

#         # Pass the input image 
#         input_image = str(self.nd2_file_path)
#         inf = ['bftools/showinf', "-nopix", "-omexml-only", input_image]
#         p = subprocess.Popen(inf, stdout=subprocess.PIPE)
#         stdout = p.communicate()[0]
#         parser = etree.XMLParser(recover=True)
#         self.omexml_tree_data = etree.fromstring(stdout, parser)

#         # Pass the parsed dict
#         self.channel_coords_dict = {}
#         for image_xml in self.omexml_tree_data:
#             if image_xml.tag.endswith('Image'):
#                 image_number = int(image_xml.attrib['ID'].strip('Image:'))
#                 for tags in image_xml:
#                     if tags.tag.endswith('Pixels'):
#                         x_positions = []
#                         y_positions = []
#                         z_positions = []
#                         for plane in tags:
#                             if plane.tag.endswith('Plane'):
#                                 att = plane.attrib
#                                 x_positions.append(float(att['PositionX']))
#                                 y_positions.append(float(att['PositionY']))
#                                 z_positions.append(float(att['PositionZ']))
#                         x_coord = sum(x_positions)/len(x_positions)
#                         y_coord = sum(y_positions)/len(y_positions)
#                         z_coord = sum(z_positions)/len(z_positions)
                        
            
#                 self.channel_coords_dict[image_number] = [x_coord,y_coord,z_coord]


#     def rename_nd2_file(self):
#         new_file_name = self.experiment_name + '_' + self.hybridization_name + '_' + self.image_channel + '.nd2'
#         new_file_path = self.raw_files_dir / new_file_name
#         self.nd2_file_path.rename(new_file_path)
#         self.nd2_file_path = new_file_path
#         self.logger.info(f'file renamed to {new_file_name}')

#     def copy_info_pkl_file(self):
#         new_file_name = self.experiment_name + '_' + self.hybridization_name + '_' + self.image_channel + '_info.pkl'
#         new_file_path = self.raw_files_dir / new_file_name
#         # Must copy the pkl file in order to be able to use the file for the other channels
#         shutil.copy(str(self.info_file), str(new_file_path))
#         # Updated the location of the info file value
#         self.info_file = new_file_path
#         self.logger.info(f'copy the pkl file and renamed to {new_file_name}')

#     def update_pipeline_config_file(self):
#         self.new_pipeline_odict = OrderedDict()
#         self.new_pipeline_odict['info_file'] = str(self.info_file)
#         self.new_pipeline_odict['nd2_file_path'] = str(self.nd2_file_path)
#         self.new_pipeline_odict['count_code'] = self.count_code
#         self.new_pipeline_odict['info_data'] = self.info_data
#         self.new_pipeline_odict['nd2_file_metadata'] = self.nd2_file_metadata
#         self.new_pipeline_odict['nd2_file_metadata']['fields_of_view'] = list(self.new_pipeline_odict['nd2_file_metadata']['fields_of_view'])
#         self.new_pipeline_odict['nd2_file_metadata']['z_levels'] = list(self.new_pipeline_odict['nd2_file_metadata']['z_levels'])
#         self.new_pipeline_odict['channel_coords_dict'] = self.channel_coords_dict
#         self.new_pipeline_odict['channel'] = self.image_channel
#         self.new_pipeline_odict['run_directory'] = str(self.experiment_fpath)
#         self.new_file_path = self.parsed_tmp / (self.experiment_name + '_' + self.hybridization_name + '_' + self.image_channel +'_tmp_config.yaml')
#         with open(self.new_file_path, 'w') as new_config:
#             yaml.safe_dump(dict(self.new_pipeline_odict), new_config,default_flow_style=False,sort_keys=False)

#     def update_pipeline_config_file_reparsed(self):
#         self.new_pipeline_odict = OrderedDict()
#         self.new_pipeline_odict['info_file'] = str(self.info_file)
#         self.new_pipeline_odict['info_data'] = self.info_data
#         self.new_pipeline_odict['nd2_file_metadata'] = self.nd2_file_metadata
#         self.new_pipeline_odict['nd2_file_metadata']['fields_of_view'] = list(self.new_pipeline_odict['nd2_file_metadata']['fields_of_view'])
#         self.new_pipeline_odict['nd2_file_metadata']['z_levels'] = list(self.new_pipeline_odict['nd2_file_metadata']['z_levels'])
#         self.new_pipeline_odict['channel_coords_dict'] = self.channel_coords_dict
#         self.new_pipeline_odict['channel'] = self.image_channel
#         self.new_file_path = self.parsed_tmp / (self.experiment_name + '_' + self.hybridization_name + '_' + self.image_channel +'_tmp_config.yaml')
#         with open(self.new_file_path, 'w') as new_config:
#             yaml.safe_dump(dict(self.new_pipeline_odict), new_config,default_flow_style=False,sort_keys=False)


#     def nd2file_save_images_single_fov(self):
        
#         time_start = time.time()

#         # Load the file
#         nd2f_hdl = ND2Reader(self.nd2_file_path)

#         # set filtering of loaded image as single image chunks
#         nd2f_hdl.bundle_axes = 'zyx'
#         # set iteration over the fields of view
#         nd2f_hdl.iter_axes = 'v'

    
#         # Create the zarr file where to save the data
#         self.tmp_dir_path = self.parsed_tmp / (self.experiment_name + '_' + self.hybridization_name + '_' + self.image_channel + '_raw_images_tmp.zarr')
#         self.parsed_store = zarr.DirectoryStore(str(self.tmp_dir_path))
#         self.parsed_root = zarr.group(store=self.parsed_store, overwrite=True)
#         self.parsed_root.attrs['analysis'] = self.analysis_name
#         self.parsed_root.attrs['experiment'] = self.experiment_name
        
#         size_counter = 0
#         tmp_storage={}   
        
#         self.logger.info("nd2 file conversion started")
#         for fov in  self.nd2_file_metadata['fields_of_view']:

#             size_counter += nd2f_hdl[fov].nbytes
#             if size_counter < self.ram:
#                 tmp_storage[fov]={}
#                 tmp_storage[fov] =np.array(nd2f_hdl[fov],dtype=np.uint16)
#                 self.logger.debug(f'position {fov} stored in RAM')
#                 self.logger.debug(f'tmp storage reached {size_counter}')
#             else:
#                 for pos in tmp_storage.keys():
#                     if len(tmp_storage[pos].shape) == 3:
#                         self.parsed_root.create_dataset(pos, data=tmp_storage[pos], shape=tmp_storage[pos].shape, chunks=(1,None,None),overwrite=True)
#                     elif len(tmp_storage[pos].shape) == 2:
#                         self.parsed_root.create_dataset(pos, data=tmp_storage[pos], shape=tmp_storage[pos].shape, chunks=(None,None),overwrite=True)
#                     self.logger.debug(f'position {pos} saved on HD')

#                     # tmp_file_path = self.tmp_dir_path / (self.experiment_name + '_' + self.hybridization_name + '_' + self.image_channel + '_pos_' + str(pos) + '.npy')
#                     # np.save(tmp_file_path,tmp_storage[pos],allow_pickle=False)
#                     # self.logger.debug('position %s saved on HD', str(pos))

#                 tmp_storage={}
#                 size_counter = nd2f_hdl[fov].nbytes
#                 tmp_storage[fov]={}
#                 tmp_storage[fov] =np.array(nd2f_hdl[fov],dtype=np.uint16)
                       
#         # Save the data that remained in the tmp_storage (if RAM is used) 
#         self.logger.info('start writing the remaining positions')
#         for pos in tmp_storage.keys():
#             if len(tmp_storage[pos].shape) == 3:
#                 self.parsed_root.create_dataset(pos, data=tmp_storage[pos], shape=tmp_storage[pos].shape, chunks=(1,None,None))
#             elif len(tmp_storage[pos].shape) == 2:
#                 self.parsed_root.create_dataset(pos, data=tmp_storage[pos], shape=tmp_storage[pos].shape, chunks=(None,None))
#             self.logger.debug(f'position {pos} saved on HD')
            
#         # Close the file
#         nd2f_hdl.close()

#         self.logger.info(f'nd2 parsing completed in {time.time()-time_start}')

    
#     def deploy_initial_parsing(self):
#         self.extract_nd2_metadata()
#         self.retrieve_info_file_dict()
#         self.extract_omexml_tree()
#         self.rename_nd2_file()
#         self.copy_info_pkl_file()
#         self.update_pipeline_config_file()
#         self.nd2file_save_images_single_fov()

#     def deploy_reparsing(self): 
#         self.extract_nd2_metadata()
#         self.retrieve_info_file_dict_reparsing()
#         self.extract_omexml_tree()
#         self.fname = self.nd2_file_path.stem
#         self.hybridization_name = self.fname.split('_')[-2]
#         self.image_channel = self.fname.split('_')[-1]
#         self.extract_nd2_metadata()
#         self.update_pipeline_config_file_reparsed()   
#         self.nd2file_save_images_single_fov()
          

#     def deploy_switch(self):
#         if self.reparsing:
#             self.deploy_reparsing()
#         else:
#             self.deploy_initial_parsing()
