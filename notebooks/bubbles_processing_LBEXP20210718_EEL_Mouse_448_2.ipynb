{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# pysmFISH pipeline running template\n",
    "\n",
    "This jupyter lab notebook is used to run automated data analysis via papermill. The data will be run through the entire pipeline (full run). A copy of the run notebook will be stored in the processed experiment folder inside the notebooks subfolder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from pysmFISH.pipeline import Pipeline\n",
    "from pysmFISH.utils import end_processing_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS TAGGED PARAMETERS\n",
    "\n",
    "# REQUIRED ARGUMENTS\n",
    "# -------------------\n",
    "\n",
    "# Path to the experiment folder\n",
    "experiment_fpath = '/fish/work_std/LBEXP20210718_EEL_Mouse_448_2' \n",
    "\n",
    "# Define if it is a 'new' or 're-run' (default: new)\n",
    "run_type = 'new'\n",
    "\n",
    "# Define the parsing type. Can be: \n",
    "# original/no_parsing/reparsing_from_processing_folder/reparsing_from_storage\n",
    "# (default: original)\n",
    "parsing_type = 'original'\n",
    "\n",
    "# OPTIONAL KWARGS\n",
    "# ----------------\n",
    "\n",
    "# Path to the cold storage hard drive (default: /fish/rawdata)\n",
    "raw_data_folder_storage_path = '/fish/rawdata'\n",
    "\n",
    "# Tag to identify the zarr file with parsed images (default: img_data)\n",
    "parsed_image_tag = 'img_data'\n",
    "\n",
    "# Tag to identify the zarr file with preprocessed images (default: preprocessed_img_data)\n",
    "preprocessed_image_tag = 'preprocessed_img_data'\n",
    "\n",
    "# Path to the location where the dataset are stored (default: /fish/fish_datasets)\n",
    "dataset_folder_storage_path = '/fish/fish_datasets'\n",
    "\n",
    "# Path to the location where the dataset are stored (default: /fish/fish_results)\n",
    "results_folder_storage_path = '/fish/fish_results'\n",
    "\n",
    "# Determine if the processed images will be saved (default: True)\n",
    "save_intermediate_steps = True\n",
    "\n",
    "# Path to an existing dataset that will be used in the processing\n",
    "dataset_path = ''\n",
    "\n",
    "# Number of FOV to process in parallel\n",
    "chunk_size = 50\n",
    "\n",
    "# Searching distance that define two dots as identical (default: 10)\n",
    "same_dot_radius_duplicate_dots = 10\n",
    "\n",
    "# Define the stitched counts on which the overlapping dotes will be removed \n",
    "# (default: microscope_stitched) \n",
    "stitching_selected = 'microscope_stitched'\n",
    "\n",
    "# Value to select the barcodes that are passing the \n",
    "# screening (< hamming_distance). (default: 3)\n",
    "hamming_distance = 3\n",
    "\n",
    "# Define the name of the system that will run the processing. Can be local/htcondor\n",
    "# (default htcondor). If engine == local the parameters that define the cluster\n",
    "# will be ignored\n",
    "processing_engine = 'htcondor'\n",
    "\n",
    "# Number of cores to use in htcondor (default 20)\n",
    "cores = 20\n",
    "\n",
    "# Total memory for all the cores in htcondor (default 200GB)\n",
    "memory = '200GB'\n",
    "\n",
    "# Size of the spillover disk for dask in htcondor (default 0.1GB)\n",
    "disk = '0.1GB'\n",
    "\n",
    "# Directory where to spill over on the node in htcondor (default /tmp)\n",
    "local_directory = '/tmp'\n",
    "\n",
    "# Directory where to store dask and htcondor logs\n",
    "logs_directory = ''\n",
    "\n",
    "# Save the intensity of the bits and the flipping direction\n",
    "save_bits_int = False\n",
    "\n",
    "\n",
    "# Add a note if needed\n",
    "notes = 'processing experiment with bubbles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a running time tag to the pipeline run name\n",
    "experiment_fpath = Path(experiment_fpath)\n",
    "date_tag = time.strftime(\"%y%m%d_%H_%M_%S\")\n",
    "pipeline_run_name = date_tag + '_' + experiment_fpath.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing experiment with bubbles\n"
     ]
    }
   ],
   "source": [
    "print(f\"{notes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the pipeline run\n",
    "\n",
    "running_pipeline = Pipeline(\n",
    "                        pipeline_run_name= pipeline_run_name,\n",
    "                        experiment_fpath= experiment_fpath,\n",
    "                        run_type= run_type,\n",
    "                        parsing_type= parsing_type,\n",
    "                        processing_engine= processing_engine,\n",
    "                        cores= cores,\n",
    "                        memory= memory,\n",
    "                        disk= disk,\n",
    "                        local_directory= local_directory,\n",
    "                        chunk_size= chunk_size,\n",
    "                        raw_data_folder_storage_path= raw_data_folder_storage_path,\n",
    "                        parsed_image_tag= parsed_image_tag,\n",
    "                        preprocessed_image_tag= preprocessed_image_tag,\n",
    "                        dataset_folder_storage_path= dataset_folder_storage_path,\n",
    "                        results_folder_storage_path= results_folder_storage_path,\n",
    "                        save_intermediate_steps= save_intermediate_steps,\n",
    "                        dataset_path= dataset_path,\n",
    "                        same_dot_radius_duplicate_dots= same_dot_radius_duplicate_dots,\n",
    "                        stitching_selected= stitching_selected,\n",
    "                        hamming_distance= hamming_distance,\n",
    "                        logs_directory= logs_directory,\n",
    "                        save_bits_int= save_bits_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - INFO - raw_data already exist\n",
      "distributed.worker - INFO - original_robofish_logs already exist\n",
      "distributed.worker - INFO - extra_processing_data already exist\n",
      "distributed.worker - INFO - extra_files already exist\n",
      "distributed.worker - INFO - pipeline_config already exist\n",
      "distributed.worker - INFO - output_figures already exist\n",
      "distributed.worker - INFO - probes already exist\n",
      "distributed.worker - INFO - logs already exist\n",
      "distributed.worker - INFO - results already exist\n",
      "distributed.worker - INFO - microscope_tiles_coords already exist\n",
      "distributed.worker - INFO - Codebook_Atto425 has None as codebook\n",
      "distributed.worker - INFO - Codebook_Cy3 has None as codebook\n",
      "distributed.worker - INFO - Codebook_Cy7 has None as codebook\n",
      "distributed.worker - INFO - Codebook_DAPI has None as codebook\n",
      "distributed.worker - INFO - Codebook_FITC has None as codebook\n",
      "distributed.worker - INFO - Codebook_TxRed has None as codebook\n",
      "distributed.worker - INFO - Codebook_Europium has None as codebook\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/utils.py\", line 655, in log_errors\n",
      "    yield\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 4217, in retire_workers\n",
      "    await self.replicate(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 3970, in replicate\n",
      "    assert count > 0\n",
      "AssertionError\n",
      "distributed.core - ERROR - Exception while handling op retire_workers\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 497, in handle_comm\n",
      "    result = await result\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 4217, in retire_workers\n",
      "    await self.replicate(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 3970, in replicate\n",
      "    assert count > 0\n",
      "AssertionError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/utils.py\", line 655, in log_errors\n",
      "    yield\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/deploy/adaptive.py\", line 187, in scale_down\n",
      "    await self.scheduler.retire_workers(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 787, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 657, in send_recv\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 497, in handle_comm\n",
      "    result = await result\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 4217, in retire_workers\n",
      "    await self.replicate(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 3970, in replicate\n",
      "    assert count > 0\n",
      "AssertionError\n",
      "tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <zmq.eventloop.ioloop.ZMQIOLoop object at 0x7fc0be5165e0>>, <Task finished name='Task-2944390' coro=<AdaptiveCore.adapt() done, defined at /home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/deploy/adaptive_core.py:179> exception=AssertionError()>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/deploy/adaptive_core.py\", line 203, in adapt\n",
      "    await self.scale_down(**recommendations)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/deploy/adaptive.py\", line 187, in scale_down\n",
      "    await self.scheduler.retire_workers(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 787, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 657, in send_recv\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 497, in handle_comm\n",
      "    result = await result\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 4217, in retire_workers\n",
      "    await self.replicate(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 3970, in replicate\n",
      "    assert count > 0\n",
      "AssertionError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/utils.py\", line 655, in log_errors\n",
      "    yield\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 4217, in retire_workers\n",
      "    await self.replicate(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 3970, in replicate\n",
      "    assert count > 0\n",
      "AssertionError\n",
      "distributed.core - ERROR - Exception while handling op retire_workers\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 497, in handle_comm\n",
      "    result = await result\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 4217, in retire_workers\n",
      "    await self.replicate(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 3970, in replicate\n",
      "    assert count > 0\n",
      "AssertionError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/utils.py\", line 655, in log_errors\n",
      "    yield\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/deploy/adaptive.py\", line 187, in scale_down\n",
      "    await self.scheduler.retire_workers(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 787, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 657, in send_recv\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 497, in handle_comm\n",
      "    result = await result\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 4217, in retire_workers\n",
      "    await self.replicate(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 3970, in replicate\n",
      "    assert count > 0\n",
      "AssertionError\n",
      "tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <zmq.eventloop.ioloop.ZMQIOLoop object at 0x7fc0be5165e0>>, <Task finished name='Task-5798880' coro=<AdaptiveCore.adapt() done, defined at /home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/deploy/adaptive_core.py:179> exception=AssertionError()>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/deploy/adaptive_core.py\", line 203, in adapt\n",
      "    await self.scale_down(**recommendations)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/deploy/adaptive.py\", line 187, in scale_down\n",
      "    await self.scheduler.retire_workers(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 787, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 657, in send_recv\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/core.py\", line 497, in handle_comm\n",
      "    result = await result\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 4217, in retire_workers\n",
      "    await self.replicate(\n",
      "  File \"/home/simone/mini/envs/test_d/lib/python3.8/site-packages/distributed/scheduler.py\", line 3970, in replicate\n",
      "    assert count > 0\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "# Full pipeline run\n",
    "\n",
    "running_pipeline.run_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Signal completion of the analysis\n",
    "end_processing_file(path_destination=experiment_fpath.parent,\n",
    "                    completion_pattern='processing_completed.txt')\n",
    "\n",
    "print('Processing completed')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "test_d"
  },
  "kernelspec": {
   "display_name": "test_d",
   "language": "python",
   "name": "test_d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
