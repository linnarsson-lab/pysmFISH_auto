{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec874d78-7b97-4c72-89d9-7d680ebb959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463a4d4b-460f-48a8-b8e6-9b4036051041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e0e37b9-46b2-4b82-aee5-b02111c4f4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/mini/envs/test_d_seg/lib/python3.8/site-packages/dask_jobqueue/core.py:19: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile\n",
      "/home/simone/mini/envs/test_d_seg/lib/python3.8/site-packages/dask_jobqueue/core.py:19: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile\n",
      "/home/simone/mini/envs/test_d_seg/lib/python3.8/site-packages/dask_jobqueue/htcondor.py:6: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import parse_bytes\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "from pysmFISH.pipeline import Pipeline\n",
    "from pysmFISH import stitching\n",
    "from pysmFISH import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65c3a99-8cae-4ee7-87c9-4b4fca7c4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_fpath = Path('/datb/sl/fish_rawdata/AMEXP20211026_EEL_MTG')\n",
    "date_tag = time.strftime(\"%y%m%d_%H_%M_%S\")\n",
    "pipeline_run_name = date_tag + '_' + experiment_fpath.stem\n",
    "run_type = 're-run'\n",
    "parsing_type = 'no_parsing'\n",
    "processing_engine = 'unmanaged_cluster',\n",
    "workers_addresses_list = ['monod10','monod33']\n",
    "scheduler_port = 21429\n",
    "fresh_tissue_segmentation_engine = 'stardist'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbfce343-984b-49f6-916d-ac157ca78576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 105 µs, sys: 69 µs, total: 174 µs\n",
      "Wall time: 190 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "running_pipeline = Pipeline(\n",
    "        pipeline_run_name= pipeline_run_name,\n",
    "        experiment_fpath= experiment_fpath,\n",
    "        run_type= run_type,\n",
    "        parsing_type= parsing_type,\n",
    "        processing_engine= processing_engine,\n",
    "        chunk_size=20,\n",
    "        same_dot_radius_duplicate_dots = 5,\n",
    "        save_bits_int=False,\n",
    "        scheduler_port=scheduler_port,\n",
    "        fresh_tissue_segmentation_engine=fresh_tissue_segmentation_engine,\n",
    "        workers_addresses_list = workers_addresses_list,\n",
    "        dataset_path='/datb/sl/fish_rawdata/AMEXP20211026_EEL_MTG/220118_21_06_37_AMEXP20211026_EEL_MTG_img_data_dataset.parquet'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29068408-ace7-49da-8fd1-003f852aff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO---check why error in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1973a642-ff2a-4e3c-a4e7-fefcba69dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_pipeline.processing_engine = 'unmanaged_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b8466a-a5d0-4560-9e57-d6fbde92a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_pipeline.processing_env_config['processing_engine'] = running_pipeline.processing_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a1c4ff-7a0a-45c3-9343-2c422017232b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unmanaged_cluster'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_pipeline.processing_env_config['processing_engine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157d777e-04a0-47be-8b97-da0716d8e479",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - INFO - extra_processing_data already exist\n",
      "distributed.worker - INFO - pipeline_config already exist\n",
      "distributed.worker - INFO - output_figures already exist\n",
      "distributed.worker - INFO - probes already exist\n",
      "distributed.worker - INFO - logs already exist\n",
      "distributed.worker - INFO - results already exist\n",
      "distributed.worker - INFO - microscope_tiles_coords already exist\n",
      "distributed.worker - INFO - notebooks already exist\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - -----------------------------------------------\n",
      "distributed.deploy.ssh - INFO - /home/simone/mini/envs/test_d_seg/lib/python3.8/site-packages/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "distributed.deploy.ssh - INFO - Perhaps you already have a cluster running?\n",
      "distributed.deploy.ssh - INFO - Hosting the HTTP server on port 3072 instead\n",
      "distributed.deploy.ssh - INFO - warnings.warn(\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - -----------------------------------------------\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - Clear task state\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO -   Scheduler at:  tcp://193.10.16.58:21429\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:43728'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:44510'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:45288'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:37103'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:46582'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:43874'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:37416'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:33504'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:46112'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:34806'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:38772'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:37968'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:37174'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:41994'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:33821'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:34410'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:38770'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:43720'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:39136'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:38213'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:41433'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:36978'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:40244'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:44584'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:46773'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:42500'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:45228'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:42440'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:45328'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:42908'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:40128'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:40184'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:39396'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:46314'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:34330'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:40293'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:45948'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:46367'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:35987'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:37793'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:40614'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:35429'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:41335'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:44206'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:46533'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:44020'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:34701'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:41793'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:40530'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:35619'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:41650'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.10:45117'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:34316'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:40202'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:44458'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:33310'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:35410'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:42781'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:34575'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:33191'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:36825'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:37472'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:45806'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:40617'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:35010'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:41511'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:43038'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:46682'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:39866'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:43074'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:34038'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:35253'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:41955'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:37631'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:44940'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:38024'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:44002'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:44110'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:39365'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.0.33:39482'\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:   tcp://192.168.0.10:34386\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:   tcp://192.168.0.33:43711\n",
      "distributed.worker - INFO - cluster properties: htcondor_cluster_setup {'processing_engine': 'unmanaged_cluster', 'cores': 20, 'memory': '200GB', 'disk': '0.1GB', 'local_directory': '/tmp', 'logs_directory': '/datb/sl/fish_rawdata/AMEXP20211026_EEL_MTG/logs', 'adaptive': True, 'maximum_jobs': 15, 'scheduler_port': 21429, 'dashboard_port': 8787, 'scheduler_address': 'localhost', 'workers_addresses_list': ['monod10', 'monod33'], 'nprocs': 40, 'nthreads': 1} \n",
      "distributed.worker - INFO - Started unmanaged cluster\n",
      "distributed.worker - INFO - selected functions for eel-human-adult-brain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.02 s, sys: 3.43 s, total: 8.45 s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "running_pipeline.run_setup()\n",
    "running_pipeline.run_cluster_activation()\n",
    "running_pipeline.run_parsing()\n",
    "running_pipeline.run_required_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8ad9f-7482-404b-95bd-f58d4e0e0f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593a5c50-4ab1-4a93-adf7-c3b1c2fc4c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 44.6s\n",
      "Peak located at edge\n",
      "[########################################] | 100% Completed |  1min 43.0s\n",
      "[########################################] | 100% Completed |  1min 41.0s\n",
      "[########################################] | 100% Completed |  1min 46.7s\n",
      "[########################################] | 100% Completed |  1min 48.7s\n",
      "[########################################] | 100% Completed |  1min 48.5s\n",
      "[########################################] | 100% Completed |  1min 53.0s\n",
      "No peaks found. Widening the search to find the scaling factor. If this happends in the last cycles \n",
      "0 0 5000\n",
      "1 5000 10000\n",
      "2 10000 15000\n",
      "3 15000 20000\n",
      "4 20000 25000\n",
      "5 25000 30000\n",
      "6 30000 35000\n",
      "7 35000 40000\n",
      "Calculating unique genes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1138065/735513601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunning_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing_fresh_tissue_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tmp_code/pysmFISH_auto/pysmFISH/pipeline.py\u001b[0m in \u001b[0;36mprocessing_fresh_tissue_step\u001b[0;34m(self, parsing, tag_ref_beads, tag_nuclei)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         )\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m         segmentation.register_assign(\n\u001b[0m\u001b[1;32m   1388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_fpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m             \u001b[0msegmented_object_dict_recalculated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp_code/pysmFISH_auto/pysmFISH/segmentation.py\u001b[0m in \u001b[0;36mregister_assign\u001b[0;34m(experiment_path, segmented_object_dict_recalculated, dataset_experiment, dataset_nuclei, experiment_metadata, nuclei_metadata, pipeline_run_name, segmentation_output_path, mask_expansion_radius, hamming_distance)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0mcell_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".zarr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m     assigned_data_df, unique_genes, point_cell_id = CA.asignment_chunked(\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mcell_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_genes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     )\n",
      "\u001b[0;32m~/tmp_code/pysmFISH_auto/pysmFISH/cell_assignment.py\u001b[0m in \u001b[0;36masignment_chunked\u001b[0;34m(self, mask, points, genes, offset, chunk_size_x, unique_genes, workers)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_genes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calculating unique genes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0munique_genes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m#Make chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/mini/envs/test_d_seg/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mini/envs/test_d_seg/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "running_pipeline.processing_fresh_tissue_step(parsing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06c7b1-54df-4dbc-8458-d40c048ef38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_pipeline.processing_env_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3922b6-0f50-42d0-ae4e-4342554a7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stardist.models import StarDist2D\n",
    "model = StarDist2D.from_pretrained(\"2D_versatile_fluo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090e551-e2ba-4688-b8cb-d5a36a94dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = model.export_TF('testmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40178f-a836-4fb2-91a7-1f4eb2e6514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5d410-deda-49b5-9eed-542186c41283",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = model.export_TF(fname = 'model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecc7f4-4477-494e-b6ff-306650b07b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea9769-3efa-4278-967a-787e3ddfa813",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = StarDist2D(None, 'model', basedir='model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958c14b-4aac-4c04-b28b-c1a2840c6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pysmFISH import data_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cda9bf-f2a9-4829-994f-31c28d3bd50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_parquet('/datb/sl/fish_rawdata/AMEXP20211026_EEL_MTG/fresh_tissue/220520_08_44_35_20211026_123326_069__ChannelCy3_Nuclei_Seq0002_img_data_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d6fbd-5c89-4f29-b3f1-602b2769a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_parquet('/datb/sl/fish_rawdata/AMEXP20211026_EEL_MTG/fresh_tissue/results/20211026_123326_069__ChannelEuropium_Cy3_Seq0003_counts_beads_fresh_tissue_decoded_fov_98.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b65ca-e950-410d-8eb3-b7a812268b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = data_models.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f684ea3-41ca-45ed-a1dd-e0c46a5cf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a27edf-293e-4761-9f3b-b34840a47a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1324b8-bba7-4f5f-8a4e-dadf5b04a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.load_dataset('/datb/sl/fish_rawdata/AMEXP20211026_EEL_MTG/fresh_tissue/220520_08_44_35_20211026_123326_069__ChannelCy3_Nuclei_Seq0002_img_data_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31702cf5-55b8-4f7d-8766-e095ecf5efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.collect_metadata(ds.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca9a0e-8586-4687-adb4-35157e6375e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb503b-413e-486e-bcf9-7ae8f9abed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pickle.load(open('/datb/sl/fish_rawdata/AMEXP20211026_EEL_MTG/fresh_tissue/segmentation/segmented_objects_dict_recalculated_ids.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24aeb1-4b7d-481a-91f7-87e59c026387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysmFISH.segmentation import create_high_mag_beads_RNA_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ecaf3-22bd-4553-a426-6d0e0907b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_beads, all_RNA =create_high_mag_beads_RNA_source(running_pipeline.experiment_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76224eef-19d4-439b-922b-396a200a42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b1e8f-2c8f-46d7-a2ed-dcdcd71b11ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_d_seg",
   "language": "python",
   "name": "test_d_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
