{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# pysmFISH pipeline running template\n",
    "\n",
    "This jupyter lab notebook is used to run automated data analysis via papermill. The data will be run through the entire pipeline (full run). A copy of the run notebook will be stored in the processed experiment folder inside the notebooks subfolder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-08-23T11:49:47.124Z",
     "iopub.status.busy": "2021-08-23T11:49:47.117Z",
     "iopub.status.idle": "2021-08-23T11:49:57.299Z",
     "shell.execute_reply": "2021-08-23T11:49:57.372Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from pysmFISH.pipeline import Pipeline\n",
    "from pysmFISH.utils import end_processing_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS TAGGED PARAMETERS\n",
    "\n",
    "# REQUIRED ARGUMENTS\n",
    "# -------------------\n",
    "\n",
    "# Path to the experiment folder\n",
    "experiment_fpath = '' \n",
    "\n",
    "# Define if it is a 'new' or 're-run' (default: new)\n",
    "run_type = 'new'\n",
    "\n",
    "# Define the parsing type. Can be: \n",
    "# original/no_parsing/reparsing_from_processing_folder/reparsing_from_storage\n",
    "# (default: original)\n",
    "parsing_type = 'original'\n",
    "\n",
    "# OPTIONAL KWARGS\n",
    "# ----------------\n",
    "\n",
    "# Path to the cold storage hard drive (default: /fish/rawdata)\n",
    "raw_data_folder_storage_path = '/fish/rawdata'\n",
    "\n",
    "# Tag to identify the zarr file with parsed images (default: img_data)\n",
    "parsed_image_tag = 'img_data'\n",
    "\n",
    "# Tag to identify the zarr file with preprocessed images (default: preprocessed_img_data)\n",
    "preprocessed_image_tag = 'preprocessed_img_data'\n",
    "\n",
    "# Path to the location where the dataset are stored (default: /fish/fish_datasets)\n",
    "dataset_folder_storage_path = '/fish/fish_datasets'\n",
    "\n",
    "# Path to the location where the dataset are stored (default: /fish/fish_results)\n",
    "results_folder_storage_path = '/fish/fish_results'\n",
    "\n",
    "# Determine if the processed images will be saved (default: True)\n",
    "save_intermediate_steps = True\n",
    "\n",
    "# Path to an existing dataset that will be used in the processing\n",
    "dataset_path = ''\n",
    "\n",
    "# Number of FOV to process in parallel\n",
    "chunk_size = 50\n",
    "\n",
    "# Searching distance that define two dots as identical (default: 10)\n",
    "same_dot_radius_duplicate_dots = 10\n",
    "\n",
    "# Define the stitched counts on which the overlapping dotes will be removed \n",
    "# (default: microscope_stitched) \n",
    "stitching_selected = 'microscope_stitched'\n",
    "\n",
    "# Value to select the barcodes that are passing the \n",
    "# screening (< hamming_distance). (default: 3)\n",
    "hamming_distance = 3\n",
    "\n",
    "# Define the name of the system that will run the processing. Can be local/htcondor\n",
    "# (default htcondor). If engine == local the parameters that define the cluster\n",
    "# will be ignored\n",
    "processing_engine = 'htcondor'\n",
    "\n",
    "# Number of cores to use in htcondor (default 20)\n",
    "cores = 20\n",
    "\n",
    "# Total memory for all the cores in htcondor (default 200GB)\n",
    "memory = '200GB'\n",
    "\n",
    "# Size of the spillover disk for dask in htcondor (default 0.1GB)\n",
    "disk = '0.1GB'\n",
    "\n",
    "# Directory where to spill over on the node in htcondor (default /tmp)\n",
    "local_directory = '/tmp'\n",
    "\n",
    "# Directory where to store dask and htcondor logs\n",
    "logs_directory = ''\n",
    "\n",
    "# Save the intensity of the bits and the flipping direction\n",
    "save_bits_int = False\n",
    "\n",
    "\n",
    "# Add a note if needed\n",
    "notes = 'no notes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Add a running time tag to the pipeline run name\n",
    "experiment_fpath = Path(experiment_fpath)\n",
    "date_tag = time.strftime(\"%y%m%d_%H_%M_%S\")\n",
    "pipeline_run_name = date_tag + '_' + experiment_fpath.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{notes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set up the pipeline run\n",
    "\n",
    "running_pipeline = Pipeline(\n",
    "                        pipeline_run_name= pipeline_run_name,\n",
    "                        experiment_fpath= experiment_fpath,\n",
    "                        run_type= run_type,\n",
    "                        parsing_type= parsing_type,\n",
    "                        processing_engine= processing_engine,\n",
    "                        cores= cores,\n",
    "                        memory= memory,\n",
    "                        disk= disk,\n",
    "                        local_directory= local_directory,\n",
    "                        chunk_size= chunk_size,\n",
    "                        raw_data_folder_storage_path= raw_data_folder_storage_path,\n",
    "                        parsed_image_tag= parsed_image_tag,\n",
    "                        preprocessed_image_tag= preprocessed_image_tag,\n",
    "                        dataset_folder_storage_path= dataset_folder_storage_path,\n",
    "                        results_folder_storage_path= results_folder_storage_path,\n",
    "                        save_intermediate_steps= save_intermediate_steps,\n",
    "                        dataset_path= dataset_path,\n",
    "                        same_dot_radius_duplicate_dots= same_dot_radius_duplicate_dots,\n",
    "                        stitching_selected= stitching_selected,\n",
    "                        hamming_distance= hamming_distance,\n",
    "                        logs_directory= logs_directory,\n",
    "                        save_bits_int= save_bits_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Full pipeline run\n",
    "\n",
    "running_pipeline.run_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Signal completion of the analysis\n",
    "end_processing_file(path_destination=experiment_fpath.parent,\n",
    "                    completion_pattern='processing_completed.txt')\n",
    "\n",
    "print('Processing completed')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "test_d"
  },
  "kernelspec": {
   "display_name": "test_d",
   "language": "python",
   "name": "test_d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
